AST Function Index
Root: /Users/vincentzed/Documents/Github/open_source/sglang/python/sglang/srt
Excluded directories:

File: models/arcee.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', reduce_results: bool = True)
    return: None
    class: ArceeMLP
  - name: forward
    signature: (self, x, forward_batch = None)
    class: ArceeMLP
  - name: __init__
    signature: (self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, rope_is_neox_style: bool = True, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', bias: bool = False)
    return: None
    class: ArceeAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: ArceeAttention
  - name: __init__
    signature: (self, config: LlamaConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: ArceeDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: ArceeDecoderLayer
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: ArceeModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]], PPProxyTensors]
    class: ArceeModel
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: ArceeModel
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: ArceeForCausalLM
  - name: _init_model
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: ArceeForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: LogitsProcessorOutput
    class: ArceeForCausalLM
  - name: start_layer
    signature: (self)
    class: ArceeForCausalLM
  - name: end_layer
    signature: (self)
    class: ArceeForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: ArceeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: ArceeForCausalLM
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: ArceeForCausalLM

File: models/baichuan.py
  - name: _get_alibi_slopes
    signature: (total_num_heads: int)
    return: torch.Tensor
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BaiChuanMLP
  - name: forward
    signature: (self, x)
    class: BaiChuanMLP
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, position_embedding: str, rope_theta: float = 10000, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, layer_id: int = 0, prefix: str = '')
    class: BaiChuanAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BaiChuanAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, position_embedding: str, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BaiChuanDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: BaiChuanDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BaiChuanModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BaiChuanModel
  - name: __init__
    signature: (self, config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BaiChuanBaseForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BaiChuanBaseForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: BaiChuanBaseForCausalLM
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BaichuanForCausalLM

File: models/bailing_moe.py
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BailingAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BailingAttention
  - name: __init__
    signature: (self, intermediate_size: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, reduce_results: Optional[bool] = True, prefix: str = '')
    return: None
    class: BailingMLP
  - name: forward
    signature: (self, x)
    class: BailingMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BailingMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: BailingMoE
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BailingMoeBlock
  - name: forward
    signature: (self, hidden_states: torch.Tensor, position_ids: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: BailingMoeBlock
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BailingMoeModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: BailingMoeModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None)
    return: None
    class: BailingMoeForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: BailingMoeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: BailingMoeForCausalLM

File: models/bert.py
  - name: __init__
    signature: (self, config: BertConfig)
    class: BertEmbedding
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BertEmbedding
  - name: __init__
    signature: (self, config: BertConfig)
    class: BertPooler
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BertPooler
  - name: __init__
    signature: (self, config: BertConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BertEncoder
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BertEncoder
  - name: __init__
    signature: (self, config: BertConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BertLayer
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    class: BertLayer
  - name: __init__
    signature: (self, hidden_size: int, num_attention_heads: int, layer_norm_eps: float, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BertAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BertAttention
  - name: __init__
    signature: (self, hidden_size: int, num_attention_heads: int, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BertSelfAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: BertSelfAttention
  - name: __init__
    signature: (self, hidden_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BertSelfOutput
  - name: forward
    signature: (self, hidden_states: torch.Tensor, input_tensor: torch.Tensor)
    return: torch.Tensor
    class: BertSelfOutput
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BertIntermediate
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: BertIntermediate
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BertOutput
  - name: forward
    signature: (self, hidden_states: torch.Tensor, input_tensor: torch.Tensor)
    return: torch.Tensor
    class: BertOutput
  - name: __init__
    signature: (self, *, config: BertConfig, quant_config: Optional[QuantizationConfig] = None, use_bert_pooler: bool = False, prefix: str = '')
    class: BertModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False)
    return: torch.Tensor
    class: BertModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    return: Set[str]
    class: BertModel
  - name: __init__
    signature: (self, *, config: BertConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: BertForSequenceClassification
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: BertForSequenceClassification
  - name: weight_filter
    signature: ()
    class: BertForSequenceClassification
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False)
    return: torch.Tensor
    class: BertForSequenceClassification

File: models/chatglm.py
  - name: __init__
    signature: (self, config, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GLMAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GLMAttention
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GLMMLP
  - name: forward
    signature: (self, hidden_states)
    class: GLMMLP
  - name: __init__
    signature: (self, config, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GLMBlock
  - name: forward
    signature: (self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GLMBlock
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GLMTransformer
  - name: forward
    signature: (self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GLMTransformer
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: ChatGLMM
  - name: forward
    signature: (self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: ChatGLMM
  - name: __init__
    signature: (self, config: ChatGLMConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: ChatGLMForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: ChatGLMForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: ChatGLMForCausalLM

File: models/clip.py
  - name: __init__
    signature: (self, config: CLIPVisionConfig)
    class: CLIPVisionEmbeddings
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    return: torch.Tensor
    class: CLIPVisionEmbeddings
  - name: __init__
    signature: (self, config: CLIPTextConfig)
    class: CLIPTextEmbeddings
  - name: forward
    signature: (self, input_ids: Optional[torch.LongTensor] = None, position_ids: Optional[torch.LongTensor] = None, inputs_embeds: Optional[torch.FloatTensor] = None)
    return: torch.Tensor
    class: CLIPTextEmbeddings
  - name: __init__
    signature: (self, config, act_layer: Type[nn.Module] = QuickGELU, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: CLIPMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: CLIPMLP
  - name: __init__
    signature: (self, config: CLIPVisionConfig, act_layer: Type[nn.Module] = QuickGELU, norm_layer: Type[nn.Module] = None, attn_implementation: Optional[str] = 'sdpa', quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: CLIPEncoderLayer
  - name: forward
    signature: (self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor)
    return: torch.Tensor
    class: CLIPEncoderLayer
  - name: __init__
    signature: (self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: CLIPEncoder
  - name: forward
    signature: (self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor = None, causal_attention_mask: torch.Tensor = None, return_all_hidden_states: bool = False)
    return: Union[torch.Tensor, list[torch.Tensor]]
    class: CLIPEncoder
  - name: __init__
    signature: (self, config: CLIPTextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: CLIPTextTransformer
  - name: device
    signature: (self)
    return: torch.device
    class: CLIPTextTransformer
  - name: forward
    signature: (self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.Tensor] = None)
    class: CLIPTextTransformer
  - name: __init__
    signature: (self, config: CLIPTextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: CLIPTextModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, position_ids: torch.Tensor)
    class: CLIPTextModel
  - name: __init__
    signature: (self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: CLIPVisionTransformer
  - name: device
    signature: (self)
    return: torch.device
    class: CLIPVisionTransformer
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    return: torch.Tensor
    class: CLIPVisionTransformer
  - name: __init__
    signature: (self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: CLIPVisionModel
  - name: device
    signature: (self)
    return: torch.device
    class: CLIPVisionModel
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    class: CLIPVisionModel
  - name: __init__
    signature: (self, config: CLIPConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: CLIPModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool = True)
    class: CLIPModel
  - name: pad_input_ids
    signature: (self, input_ids: List[int], image_inputs: MultimodalInputs)
    class: CLIPModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: CLIPModel
  - name: monkey_patch_weight_loader
    signature: ()
  - name: prepare_weights
    signature: (self, model_name_or_path: str, revision: Optional[str], fall_back_to_pt: bool)
    return: Tuple[str, List[str], bool]

File: models/commandr.py
  - name: layer_norm_func
    signature: (hidden_states, weight, variance_epsilon)
  - name: __init__
    signature: (self, param_shape = None, eps = 1e-05)
    class: LayerNorm
  - name: forward
    signature: (self, hidden_states, residuals = None)
    class: LayerNorm
  - name: weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor)
    class: LayerNorm
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: CohereMLP
  - name: forward
    signature: (self, x)
    class: CohereMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: CohereAttention
  - name: _apply_qk_norm
    signature: (self, q, k)
    class: CohereAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: CohereAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: CohereDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: CohereDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: CohereModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: CohereModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: CohereForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: CohereForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: CohereForCausalLM

File: models/dbrx.py
  - name: __init__
    signature: (self, config: DbrxConfig, params_dtype: Optional[torch.dtype] = None, prefix: str = '')
    class: DbrxRouter
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: DbrxRouter
  - name: __init__
    signature: (self, config: DbrxConfig, quant_config: Optional[QuantizationConfig] = None, params_dtype: Optional[torch.dtype] = None, prefix: str = '')
    class: DbrxExperts
  - name: weight_loader
    signature: (self, param: nn.Parameter, loaded_weight: torch.Tensor, weight_name: str)
    class: DbrxExperts
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: DbrxExperts
  - name: __init__
    signature: (self, config: DbrxConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: DbrxAttention
  - name: forward
    signature: (self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: DbrxAttention
  - name: __init__
    signature: (self, config: DbrxConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: DbrxFusedNormAttention
  - name: forward
    signature: (self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: DbrxFusedNormAttention
  - name: __init__
    signature: (self, config: DbrxConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: DbrxBlock
  - name: forward
    signature: (self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: DbrxBlock
  - name: __init__
    signature: (self, config: DbrxConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: DbrxModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: DbrxModel
  - name: __init__
    signature: (self, config: DbrxConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: DbrxForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: DbrxForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: DbrxForCausalLM

File: models/deepseek.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, reduce_results: bool = True, prefix: str = '')
    return: None
    class: DeepseekMLP
  - name: forward
    signature: (self, x)
    class: DeepseekMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: DeepseekMoE
  - name: pack_params
    signature: (self)
    class: DeepseekMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: DeepseekMoE
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeepseekAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: DeepseekAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeepseekDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: torch.Tensor
    class: DeepseekDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeepseekModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: DeepseekModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeepseekForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: DeepseekForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: DeepseekForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: DeepseekForCausalLM

File: models/deepseek_janus_pro.py
  - name: named_apply
    signature: (fn: Callable, module: nn.Module, name = '', depth_first: bool = True, include_root: bool = False)
    return: nn.Module
  - name: VQ_16
    signature: (**kwargs)
  - name: _ntuple
    signature: (n)
  - name: parse
    signature: (x)
  - name: _trunc_normal_
    signature: (tensor, mean, std, a, b)
  - name: norm_cdf
    signature: (x)
  - name: trunc_normal_tf_
    signature: (tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0)
    doc: Fills the input Tensor with values drawn from a truncated
  - name: nchw_to
    signature: (x: torch.Tensor, fmt: Format)
  - name: resample_patch_embed
    signature: (patch_embed, new_size: List[int], interpolation: str = 'bicubic', antialias: bool = True, verbose: bool = False)
    doc: Resample the weights of the patch embedding kernel to target resolution.
  - name: resize
    signature: (x_np, _new_size)
  - name: get_resize_mat
    signature: (_old_size, _new_size)
  - name: resample_kernel
    signature: (kernel)
  - name: __init__
    signature: (self, img_size: Optional[int] = 224, patch_size: int = 16, in_chans: int = 3, embed_dim: int = 768, norm_layer: Optional[Callable] = None, flatten: bool = True, output_fmt: Optional[str] = None, bias: bool = True, strict_img_size: bool = True, dynamic_img_pad: bool = False)
    class: PatchEmbed
  - name: _init_img_size
    signature: (self, img_size: Union[int, Tuple[int, int]])
    class: PatchEmbed
  - name: set_input_size
    signature: (self, img_size: Optional[Union[int, Tuple[int, int]]] = None, patch_size: Optional[Union[int, Tuple[int, int]]] = None)
    class: PatchEmbed
  - name: feat_ratio
    signature: (self, as_scalar = True)
    return: Union[Tuple[int, int], int]
    class: PatchEmbed
  - name: dynamic_feat_size
    signature: (self, img_size: Tuple[int, int])
    return: Tuple[int, int]
    class: PatchEmbed
    doc: Get grid (feature) size for given image size taking account of dynamic padding.
  - name: forward
    signature: (self, x)
    class: PatchEmbed
  - name: __init__
    signature: (self, in_features, hidden_features = None, out_features = None, act_layer = nn.GELU, norm_layer = None, bias = True, drop = 0.0, use_conv = False)
    class: Mlp
  - name: forward
    signature: (self, x)
    class: Mlp
  - name: drop_path
    signature: (x, drop_prob: float = 0.0, training: bool = False, scale_by_keep: bool = True)
    doc: Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
  - name: __init__
    signature: (self, drop_prob: float = 0.0, scale_by_keep: bool = True)
    class: DropPath
  - name: forward
    signature: (self, x)
    class: DropPath
  - name: extra_repr
    signature: (self)
    class: DropPath
  - name: __init__
    signature: (self, dim: int, num_heads: int, mlp_ratio: float = 4.0, qkv_bias: bool = False, qk_norm: bool = False, proj_drop: float = 0.0, attn_drop: float = 0.0, init_values: Optional[float] = None, drop_path: float = 0.0, act_layer: nn.Module = nn.GELU, norm_layer: nn.Module = nn.LayerNorm, mlp_layer: nn.Module = Mlp)
    return: None
    class: VisionTransformerBlock
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: VisionTransformerBlock
  - name: __init__
    signature: (self, prob: float = 0.5, num_prefix_tokens: int = 1, ordered: bool = False, return_indices: bool = False)
    class: PatchDropout
  - name: forward
    signature: (self, x)
    return: Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]
    class: PatchDropout
  - name: resample_abs_pos_embed
    signature: (posemb: torch.Tensor, new_size: List[int], old_size: Optional[List[int]] = None, num_prefix_tokens: int = 1, interpolation: str = 'bicubic', antialias: bool = True, verbose: bool = False)
  - name: init_weights
    signature: (self)
  - name: init_weights_vit_timm
    signature: (module: nn.Module, name: str = '')
    return: None
    doc: ViT weight initialization, original timm impl (for reproducibility)
  - name: __init__
    signature: (self, img_size: Union[int, Tuple[int, int]] = 224, patch_size: Union[int, Tuple[int, int]] = 16, in_chans: int = 3, num_classes: int = 1000, global_pool: Literal['', 'avg', 'token', 'map'] = 'token', embed_dim: int = 768, depth: int = 12, num_heads: int = 12, mlp_ratio: float = 4.0, qkv_bias: bool = True, qk_norm: bool = False, init_values: Optional[float] = None, class_token: bool = True, no_embed_class: bool = False, reg_tokens: int = 0, pre_norm: bool = False, fc_norm: Optional[bool] = None, dynamic_img_size: bool = False, dynamic_img_pad: bool = False, drop_rate: float = 0.0, pos_drop_rate: float = 0.0, patch_drop_rate: float = 0.0, proj_drop_rate: float = 0.0, attn_drop_rate: float = 0.0, drop_path_rate: float = 0.0, weight_init: Literal['skip', 'jax', 'jax_nlhb', 'moco', ''] = '', embed_layer: Callable = PatchEmbed, _norm_layer: Optional[LayerType] = None, _act_layer: Optional[LayerType] = None, block_fn: Type[nn.Module] = VisionTransformerBlock, mlp_layer: Type[nn.Module] = Mlp, ignore_head: bool = False)
    return: None
    class: VisionTransformer
    doc: Args:
  - name: init_weights
    signature: (self, mode: Literal['jax', 'jax_nlhb', 'moco', ''] = '')
    return: None
    class: VisionTransformer
  - name: no_weight_decay
    signature: (self)
    return: Set
    class: VisionTransformer
  - name: group_matcher
    signature: (self, coarse: bool = False)
    return: Dict
    class: VisionTransformer
  - name: get_classifier
    signature: (self)
    return: nn.Module
    class: VisionTransformer
  - name: reset_classifier
    signature: (self, num_classes: int, global_pool = None)
    return: None
    class: VisionTransformer
  - name: _pos_embed
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: VisionTransformer
  - name: _intermediate_layers
    signature: (self, x: torch.Tensor, n: Union[int, Sequence] = 1)
    return: List[torch.Tensor]
    class: VisionTransformer
  - name: forward_features
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: VisionTransformer
  - name: forward_head
    signature: (self, x: torch.Tensor, pre_logits: bool = False)
    return: torch.Tensor
    class: VisionTransformer
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: VisionTransformer
  - name: model_name_to_cls
    signature: (cls_name)
  - name: __init__
    signature: (self, params)
    class: vision_head
  - name: forward
    signature: (self, x)
    class: vision_head
  - name: create_siglip_vit
    signature: (model_name: str = 'siglip_so400m_patch14_384', image_size: int = 384, select_layer: int = -1, ckpt_path: str = '', **kwargs)
  - name: __init__
    signature: (self, mean, std, inplace = False)
    class: Normalize
  - name: forward
    signature: (self, tensor: Tensor)
    return: Tensor
    class: Normalize
    doc: Args:
  - name: __repr__
    signature: (self)
    return: str
    class: Normalize
  - name: __init__
    signature: (self, model_name: str = 'siglip_large_patch16_384', image_size: Union[Tuple[int, int], int] = 336, select_feature: str = 'patch', select_layer: int = -2, select_layers: list = None, ckpt_path: str = '', pixel_mean: Optional[List[float]] = None, pixel_std: Optional[List[float]] = None, **kwargs)
    class: CLIPVisionTower
  - name: device
    signature: (self)
    return: torch.device
    class: CLIPVisionTower
  - name: dtype
    signature: (self)
    class: CLIPVisionTower
  - name: build_vision_tower
    signature: (self, vision_tower_params)
    class: CLIPVisionTower
  - name: feature_select
    signature: (self, image_forward_outs)
    class: CLIPVisionTower
  - name: forward
    signature: (self, images)
    class: CLIPVisionTower
    doc: Args:
  - name: __init__
    signature: (self, cfg)
    class: MlpProjector
  - name: forward
    signature: (self, x_or_tuple: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor])
    class: MlpProjector
    doc: Args:
  - name: __init__
    signature: (self, dim: int, init_values: float = 1e-05, inplace: bool = False)
    return: None
    class: LayerScale
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: LayerScale
  - name: use_fused_attn
    signature: (experimental: bool = False)
    return: bool
  - name: __init__
    signature: (self, in_features: int, out_features: int = None, embed_dim: int = None, num_heads: int = 8, feat_size: Optional[int] = None, mlp_ratio: float = 4.0, qkv_bias: bool = True, qk_norm: bool = False, latent_len: int = 1, latent_dim: int = None, pos_embed: str = '', pool_type: str = 'token', norm_layer: Optional[nn.Module] = None, drop: float = 0.0)
    class: AttentionPoolLatent
  - name: init_weights
    signature: (self)
    class: AttentionPoolLatent
  - name: forward
    signature: (self, x)
    class: AttentionPoolLatent
  - name: __init__
    signature: (self, in_channels = 3, ch = 128, ch_mult = (1, 1, 2, 2, 4), num_res_blocks = 2, norm_type = 'group', dropout = 0.0, resamp_with_conv = True, z_channels = 256)
    class: Encoder
  - name: forward
    signature: (self, x)
    class: Encoder
  - name: __init__
    signature: (self, z_channels = 256, ch = 128, ch_mult = (1, 1, 2, 2, 4), num_res_blocks = 2, norm_type = 'group', dropout = 0.0, resamp_with_conv = True, out_channels = 3)
    class: Decoder
  - name: last_layer
    signature: (self)
    class: Decoder
  - name: forward
    signature: (self, z)
    class: Decoder
  - name: __init__
    signature: (self, n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage)
    class: VectorQuantizer
  - name: forward
    signature: (self, z)
    class: VectorQuantizer
  - name: get_codebook_entry
    signature: (self, indices, shape = None, channel_first = True)
    class: VectorQuantizer
  - name: __init__
    signature: (self, in_channels, out_channels = None, conv_shortcut = False, dropout = 0.0, norm_type = 'group')
    class: ResnetBlock
  - name: forward
    signature: (self, x)
    class: ResnetBlock
  - name: __init__
    signature: (self, in_channels, norm_type = 'group')
    class: AttnBlock
  - name: forward
    signature: (self, x)
    class: AttnBlock
  - name: nonlinearity
    signature: (x)
  - name: Normalize
    signature: (in_channels, norm_type = 'group')
  - name: __init__
    signature: (self, in_channels, with_conv)
    class: Upsample
  - name: forward
    signature: (self, x)
    class: Upsample
  - name: __init__
    signature: (self, in_channels, with_conv)
    class: Downsample
  - name: forward
    signature: (self, x)
    class: Downsample
  - name: compute_entropy_loss
    signature: (affinity, loss_type = 'softmax', temperature = 0.01)
  - name: __init__
    signature: (self, config: ModelArgs)
    class: VQModel
  - name: encode
    signature: (self, x)
    class: VQModel
  - name: decode
    signature: (self, quant)
    class: VQModel
  - name: decode_code
    signature: (self, code_b, shape = None, channel_first = True)
    class: VQModel
  - name: forward
    signature: (self, input)
    class: VQModel
  - name: __init__
    signature: (self, config: MultiModalityConfig, quant_config: Optional[QuantizationConfig] = None)
    class: MultiModalityCausalLM
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: MultiModalityCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: MultiModalityCausalLM
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool = False)
    return: torch.Tensor
    class: MultiModalityCausalLM
  - name: prepare_gen_img_embeds
    signature: (self, image_ids: torch.LongTensor)
    class: MultiModalityCausalLM
  - name: pad_input_ids
    signature: (self, input_ids: List[int], image_inputs: MultimodalInputs)
    class: MultiModalityCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MultiModalityCausalLM

File: models/deepseek_nextn.py
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeepseekModelNextN
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: DeepseekModelNextN
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeepseekV3ForCausalLMNextN
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: DeepseekV3ForCausalLMNextN
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: DeepseekV3ForCausalLMNextN

File: models/deepseek_v2.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, reduce_results: bool = True, prefix: str = '', tp_rank: Optional[int] = None, tp_size: Optional[int] = None)
    return: None
    class: DeepseekV2MLP
  - name: forward
    signature: (self, x, forward_batch = None, should_allreduce_fusion: bool = False, use_reduce_scatter: bool = False)
    class: DeepseekV2MLP
  - name: __init__
    signature: (self, config, prefix: str = '', is_nextn: bool = False)
    class: MoEGate
  - name: forward
    signature: (self, hidden_states)
    class: MoEGate
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None, is_nextn: bool = False)
    class: DeepseekV2MoE
  - name: get_moe_weights
    signature: (self)
    class: DeepseekV2MoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch] = None, should_allreduce_fusion: bool = False, use_reduce_scatter: bool = False)
    return: torch.Tensor
    class: DeepseekV2MoE
  - name: forward_normal_dual_stream
    signature: (self, hidden_states: torch.Tensor, should_allreduce_fusion: bool = False, use_reduce_scatter: bool = False)
    return: torch.Tensor
    class: DeepseekV2MoE
  - name: forward_normal
    signature: (self, hidden_states: torch.Tensor, should_allreduce_fusion: bool = False, use_reduce_scatter: bool = False)
    return: torch.Tensor
    class: DeepseekV2MoE
  - name: forward_cpu
    signature: (self, hidden_states: torch.Tensor, should_allreduce_fusion: bool = False)
    return: torch.Tensor
    class: DeepseekV2MoE
  - name: forward_deepep
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: DeepseekV2MoE
  - name: _forward_shared_experts
    signature: (self, hidden_states)
    class: DeepseekV2MoE
  - name: op_gate
    signature: (self, state)
    class: DeepseekV2MoE
  - name: op_shared_experts
    signature: (self, state)
    class: DeepseekV2MoE
  - name: op_select_experts
    signature: (self, state)
    class: DeepseekV2MoE
  - name: op_dispatch_a
    signature: (self, state)
    class: DeepseekV2MoE
  - name: op_dispatch_b
    signature: (self, state)
    class: DeepseekV2MoE
  - name: op_experts
    signature: (self, state)
    class: DeepseekV2MoE
  - name: op_combine_a
    signature: (self, state)
    class: DeepseekV2MoE
  - name: op_combine_b
    signature: (self, state)
    class: DeepseekV2MoE
  - name: op_output
    signature: (self, state)
    class: DeepseekV2MoE
  - name: yarn_get_mscale
    signature: (scale: float = 1, mscale: float = 1)
    return: float
  - name: __init__
    signature: (self, config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, reduce_results: bool = True, layer_id: int = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: DeepseekV2AttentionMLA
  - name: dispatch_attn_forward_method
    signature: (self, forward_batch: ForwardBatch)
    return: AttnForwardMethod
    class: DeepseekV2AttentionMLA
  - name: _dispatch_mla_subtype
    signature: ()
    class: DeepseekV2AttentionMLA
  - name: op_prepare
    signature: (self, state)
    class: DeepseekV2AttentionMLA
  - name: op_core
    signature: (self, state)
    class: DeepseekV2AttentionMLA
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
    class: DeepseekV2AttentionMLA
  - name: forward_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
    class: DeepseekV2AttentionMLA
  - name: forward_core
    signature: (self, intermediate_state)
    class: DeepseekV2AttentionMLA
  - name: forward_normal_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
    class: DeepseekV2AttentionMLA
  - name: forward_normal_core
    signature: (self, q, k, v, forward_batch)
    class: DeepseekV2AttentionMLA
  - name: _fuse_rope_for_trtllm_mla
    signature: (self, forward_batch: ForwardBatch)
    return: bool
    class: DeepseekV2AttentionMLA
    doc: Check if we should skip rope and do fused rope+quantize for TRTLLM MLA decode in fp8_e4m3 path.
  - name: forward_absorb_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
    class: DeepseekV2AttentionMLA
  - name: forward_absorb_core
    signature: (self, q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator)
    class: DeepseekV2AttentionMLA
  - name: forward_absorb_fused_mla_rope_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
    class: DeepseekV2AttentionMLA
  - name: forward_absorb_fused_mla_rope_cpu_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
    class: DeepseekV2AttentionMLA
  - name: forward_absorb_fused_mla_rope_core
    signature: (self, q_input, key_cache_buf, val_cache_buf, attn_output, kv_indptr, kv_indices, k_pe_output, cos_sin_cache, positions, attn_logits, num_kv_split, sm_scale, enable_rope_fusion, k_input, forward_batch, zero_allocator)
    class: DeepseekV2AttentionMLA
  - name: forward_absorb_fused_mla_rope_cpu_core
    signature: (self, q_input, k_input, v_input, forward_batch, zero_allocator)
    class: DeepseekV2AttentionMLA
  - name: _chunked_prefix_attn_mha
    signature: (self, q: torch.Tensor, accum_output: torch.Tensor, accum_lse: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: DeepseekV2AttentionMLA
  - name: forward_normal_chunked_kv_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
    class: DeepseekV2AttentionMLA
  - name: forward_normal_chunked_kv_core
    signature: (self, q, k, v, forward_batch)
    class: DeepseekV2AttentionMLA
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, is_nextn: bool = False, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: DeepseekV2DecoderLayer
  - name: _is_layer_sparse
    signature: (self, layer_id: int, is_nextn: bool)
    return: bool
    class: DeepseekV2DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)
    return: torch.Tensor
    class: DeepseekV2DecoderLayer
  - name: op_comm_prepare_attn
    signature: (self, state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator, tbo_subbatch_index: Optional[int] = None)
    class: DeepseekV2DecoderLayer
  - name: op_comm_prepare_mlp
    signature: (self, state)
    class: DeepseekV2DecoderLayer
  - name: op_mlp
    signature: (self, state)
    class: DeepseekV2DecoderLayer
  - name: op_comm_postprocess_layer
    signature: (self, state)
    class: DeepseekV2DecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeepseekV2Model
  - name: get_input_embeddings
    signature: (self)
    return: torch.Tensor
    class: DeepseekV2Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, PPProxyTensors]
    class: DeepseekV2Model
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeepseekV2ForCausalLM
  - name: routed_experts_weights_of_layer
    signature: (self)
    class: DeepseekV2ForCausalLM
  - name: determine_num_fused_shared_experts
    signature: (self, architecture: str = 'DeepseekV3ForCausalLM')
    class: DeepseekV2ForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: DeepseekV2ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: DeepseekV2ForCausalLM
  - name: start_layer
    signature: (self)
    class: DeepseekV2ForCausalLM
  - name: end_layer
    signature: (self)
    class: DeepseekV2ForCausalLM
  - name: post_load_weights
    signature: (self, is_nextn = False, weight_names = None)
    class: DeepseekV2ForCausalLM
  - name: _weight_requant_ue8m0
    signature: (self, is_nextn = False)
    class: DeepseekV2ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn = False)
    class: DeepseekV2ForCausalLM
  - name: get_embed_and_head
    signature: (self)
    class: DeepseekV2ForCausalLM
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: DeepseekV2ForCausalLM
  - name: get_model_config_for_expert_location
    signature: (cls, config)
    class: DeepseekV2ForCausalLM

File: models/deepseek_vl2.py
  - name: __init__
    signature: (self, config: DeepseekVL2MlpProjectorConfig, quant_config: Optional[QuantizationConfig] = None)
    class: DeepseekVL2MlpProjector
  - name: forward
    signature: (self, x)
    class: DeepseekVL2MlpProjector
  - name: __init__
    signature: (self, config: DeepseekVL2Config, quant_config: Optional[QuantizationConfig] = None)
    class: DeepseekVL2ForCausalLM
  - name: _init_vision_module
    signature: (self, vision_config, quant_config: Optional[QuantizationConfig])
    return: nn.Module
    class: DeepseekVL2ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, **kwargs: object)
    class: DeepseekVL2ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: DeepseekVL2ForCausalLM
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: DeepseekVL2ForCausalLM
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    class: DeepseekVL2ForCausalLM

File: models/ernie4.py
  - name: __init__
    signature: (self, config, prefix: str = '')
    class: MoEGate
  - name: forward
    signature: (self, hidden_states)
    class: MoEGate
  - name: __init__
    signature: (self, config: Ernie4_5_MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Ernie4Moe
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Ernie4Moe
  - name: forward_normal
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Ernie4Moe
  - name: __init__
    signature: (self, config, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', is_mtp: bool = False)
    class: Ernie4DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Ernie4DecoderLayer
  - name: __init__
    signature: (self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Ernie4Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]
    class: Ernie4Model
  - name: __init__
    signature: (self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Ernie4_5_ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Ernie4_5_ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Ernie4_5_ForCausalLM
  - name: get_embed_and_head
    signature: (self)
    class: Ernie4_5_ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Ernie4_5_MoeForCausalLM

File: models/ernie4_eagle.py
  - name: __init__
    signature: (self, config: Ernie4_5_MoeConfig, layer_id: int, prefix: str, quant_config: Optional[QuantizationConfig] = None)
    return: None
    class: Ernie4ModelMTP
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Ernie4ModelMTP
  - name: __init__
    signature: (self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', mtp_layer_id: int = 0)
    return: None
    class: Ernie4_5_MoeForCausalLMMTP
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Ernie4_5_MoeForCausalLMMTP
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Ernie4_5_MoeForCausalLMMTP
  - name: get_embed_and_head
    signature: (self)
    class: Ernie4_5_MoeForCausalLMMTP
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: Ernie4_5_MoeForCausalLMMTP

File: models/exaone.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: ExaoneGatedMLP
  - name: forward
    signature: (self, x)
    class: ExaoneGatedMLP
  - name: __init__
    signature: (self, config, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 500000, rope_scaling: Optional[Dict[str, Any]] = None, rope_is_neox_style: bool = True, max_position_embeddings: int = 4096, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: ExaoneAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: ExaoneAttention
  - name: __init__
    signature: (self, config, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: ExaoneDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: ExaoneDecoderLayer
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: ExaoneModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: ExaoneModel
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: ExaoneForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: LogitsProcessorOutput
    class: ExaoneForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: ExaoneForCausalLM

File: models/gemma.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GemmaMLP
  - name: forward
    signature: (self, x)
    class: GemmaMLP
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, layer_id: int = 0, max_position_embeddings: int = 8192, rope_theta: float = 10000, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GemmaAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GemmaAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GemmaDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: GemmaDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GemmaModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: GemmaModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GemmaForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: GemmaForCausalLM
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor = None)
    class: GemmaForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: GemmaForCausalLM

File: models/gemma2.py
  - name: get_attention_sliding_window_size
    signature: (config)
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, hidden_activation: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma2MLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Gemma2MLP
  - name: __init__
    signature: (self, layer_id: int, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma2Attention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Gemma2Attention
  - name: __init__
    signature: (self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma2DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Gemma2DecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma2Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Gemma2Model
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma2ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Gemma2ForCausalLM
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor = None)
    class: Gemma2ForCausalLM
  - name: get_attention_sliding_window_size
    signature: (self)
    class: Gemma2ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Gemma2ForCausalLM

File: models/gemma2_reward.py
  - name: __init__
    signature: (self, config: Gemma2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma2ForSequenceClassification
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = True)
    return: EmbeddingPoolerOutput
    class: Gemma2ForSequenceClassification
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Gemma2ForSequenceClassification

File: models/gemma3_causal.py
  - name: get_attention_sliding_window_size
    signature: (config)
  - name: extract_layer_index
    signature: (prefix: str)
    return: int
    doc: Extract the layer index from a prefix string.
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_activation: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3MLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Gemma3MLP
  - name: __init__
    signature: (self, layer_id: int, config: Gemma3TextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3Attention
  - name: naive_attn_with_masks
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor, **kwargs)
    return: torch.Tensor
    class: Gemma3Attention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, position_embeddings: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch, **kwargs)
    return: torch.Tensor
    class: Gemma3Attention
  - name: __init__
    signature: (self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, position_embeddings_global: torch.Tensor, position_embeddings_local: torch.Tensor, forward_batch: ForwardBatch, **kwargs)
    return: tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]
    class: Gemma3DecoderLayer
  - name: __init__
    signature: (self, config: Gemma3TextConfig, device = None)
    class: Gemma3RotaryEmbedding
  - name: _dynamic_frequency_update
    signature: (self, position_ids, device)
    class: Gemma3RotaryEmbedding
    doc: dynamic RoPE layers should recompute `inv_freq` in the following situations:
  - name: forward
    signature: (self, x, position_ids)
    class: Gemma3RotaryEmbedding
  - name: __init__
    signature: (self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0)
    class: Gemma3TextScaledWordEmbedding
  - name: forward
    signature: (self, input_ids: torch.Tensor)
    class: Gemma3TextScaledWordEmbedding
  - name: __init__
    signature: (self, config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3TextModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, **kwargs)
    return: torch.Tensor
    class: Gemma3TextModel
  - name: __init__
    signature: (self, config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3ForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Gemma3ForCausalLM
  - name: get_attention_sliding_window_size
    signature: (self)
    class: Gemma3ForCausalLM
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: Gemma3ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, **kwargs)
    return: LogitsProcessor
    class: Gemma3ForCausalLM
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor = None)
    class: Gemma3ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Gemma3ForCausalLM

File: models/gemma3_mm.py
  - name: __init__
    signature: (self, config: Gemma3Config)
    class: Gemma3MultiModalProjector
  - name: forward
    signature: (self, vision_outputs: torch.Tensor)
    return: torch.Tensor
    class: Gemma3MultiModalProjector
  - name: __init__
    signature: (self, config: Gemma3Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3ForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], image_inputs: MultimodalInputs)
    return: List[int]
    class: Gemma3ForConditionalGeneration
    doc: Pad input IDs with image tokens.
  - name: prepare_attn_masks
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, mask_dtype: torch.dtype, **kwargs)
    return: Dict
    class: Gemma3ForConditionalGeneration
    doc: Prepare attention masks for multimodal inputs.
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Gemma3ForConditionalGeneration
  - name: get_attention_sliding_window_size
    signature: (self)
    class: Gemma3ForConditionalGeneration
    doc: This value is used to initialize attention backends in `ForwardBatch`.
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    class: Gemma3ForConditionalGeneration
    doc: Projects the last hidden state from the vision model into language model space.
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, **kwargs: object)
    return: LogitsProcessor
    class: Gemma3ForConditionalGeneration
    doc: labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
  - name: tie_weights
    signature: (self)
    class: Gemma3ForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Gemma3ForConditionalGeneration

File: models/gemma3n_audio.py
  - name: __init__
    signature: (self, num_channels: int, feature_dims: Sequence[int], eps: float = 0.001)
    class: Gemma3nCumulativeGroupNorm
  - name: forward
    signature: (self, x: torch.Tensor, mask: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: Gemma3nCumulativeGroupNorm
    doc: Applies cumulative group norm, optionally using a mask.
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioRelativePositionEmbedding
  - name: _get_timing_signal_1d_pos
    signature: (self, position: torch.Tensor, dtype: torch.dtype)
    return: torch.Tensor
    class: Gemma3nAudioRelativePositionEmbedding
  - name: _relative_shift
    signature: (self, term_bd_before_shift: torch.Tensor, batch_size: int, num_heads: int, num_query_blocks: int, query_block_size: int, key_context_size: int, max_span_plus_1: int)
    return: torch.Tensor
    class: Gemma3nAudioRelativePositionEmbedding
    doc: Performs the relative shift.
  - name: forward
    signature: (self, queries: torch.Tensor, keys: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAudioRelativePositionEmbedding
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioAttention
  - name: _pad_dim1
    signature: (self, x: torch.Tensor, dim10_val: int, dim11_val: int)
    return: torch.Tensor
    class: Gemma3nAudioAttention
  - name: _convert_to_block
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAudioAttention
    doc: Turns a sequence to non overlapping blocks.
  - name: _extract_block_context
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAudioAttention
    doc: Extracts temporal context for every block.
  - name: forward
    signature: (self, x: torch.Tensor, mask: torch.BoolTensor)
    return: torch.Tensor
    class: Gemma3nAudioAttention
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, idx: int, input_freq_dim: int, manual_padding: Tuple[int, int, int, int] = (0, 0, 0, 0), quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioSSCPConvBlock
  - name: forward
    signature: (self, audio_encodings: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAudioSSCPConvBlock
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioSubSampleConvProjection
  - name: forward
    signature: (self, audio_encodings: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAudioSubSampleConvProjection
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioConformerAttention
  - name: forward
    signature: (self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor)
    return: torch.Tensor
    class: Gemma3nAudioConformerAttention
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioConformerFeedForward
  - name: forward
    signature: (self, audio_encodings: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAudioConformerFeedForward
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioConformerLightConv1d
  - name: forward
    signature: (self, audio_encodings: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAudioConformerLightConv1d
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioConformerBlock
  - name: forward
    signature: (self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor)
    return: torch.Tensor
    class: Gemma3nAudioConformerBlock
  - name: __init__
    signature: (self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAudioEncoder
  - name: forward
    signature: (self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor)
    return: Tuple[torch.Tensor, torch.BoolTensor]
    class: Gemma3nAudioEncoder
    doc: Encodes a batch of MELs.

File: models/gemma3n_causal.py
  - name: get_attention_sliding_window_size
    signature: (config)
  - name: __init__
    signature: (self, dim: int, eps: float = 1e-06, with_scale: bool = True)
    return: None
    class: Gemma3nRMSNorm
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nRMSNorm
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_activation: str, activation_sparsity: float = 0.0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3nTextMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nTextMLP
  - name: _gaussian_topk
    signature: (self, inputs: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nTextMLP
  - name: __init__
    signature: (self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nLaurelBlock
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nLaurelBlock
  - name: __init__
    signature: (self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nAltUp
  - name: compute_router_modalities
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAltUp
  - name: predict
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAltUp
    doc: Predicts the output of a layer using a trainable map.
  - name: correct
    signature: (self, predictions: torch.Tensor, activated: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAltUp
    doc: Corrects the predictions relative to the activated inputs.
  - name: scale_corrected_output
    signature: (self, corrected: torch.Tensor)
    return: torch.Tensor
    class: Gemma3nAltUp
    doc: Scales the provided 3D tensor.
  - name: forward
    signature: (self, hidden_states: torch.Tensor, activated: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Gemma3nAltUp
    doc: Predicts, correct, and optionally scales the output of a layer using trainable maps.
  - name: __init__
    signature: (self, layer_id: int, config: Gemma3nTextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3nAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, positions: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch, **kwargs)
    return: torch.Tensor
    class: Gemma3nAttention
  - name: __init__
    signature: (self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3nDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, per_layer_input: torch.Tensor, forward_batch: ForwardBatch, **kwargs)
    return: torch.Tensor
    class: Gemma3nDecoderLayer
  - name: __init__
    signature: (self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3nTextModel
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Gemma3nTextModel
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: Gemma3nTextModel
  - name: get_per_layer_inputs
    signature: (self, input_ids: torch.LongTensor)
    return: torch.Tensor
    class: Gemma3nTextModel
  - name: project_per_layer_inputs
    signature: (self, inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: Gemma3nTextModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, per_layer_inputs: Optional[torch.Tensor] = None, **kwargs)
    return: torch.Tensor
    class: Gemma3nTextModel
  - name: __init__
    signature: (self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3nForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Gemma3nForCausalLM
  - name: get_attention_sliding_window_size
    signature: (self)
    class: Gemma3nForCausalLM
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: Gemma3nForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, per_layer_inputs: Optional[torch.Tensor] = None, **kwargs)
    return: LogitsProcessor
    class: Gemma3nForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Gemma3nForCausalLM

File: models/gemma3n_mm.py
  - name: __init__
    signature: (self, multimodal_config: Union[Gemma3nAudioConfig, Gemma3nVisionConfig], text_config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Gemma3nMultimodalEmbedder
  - name: forward
    signature: (self, input_ids: Optional[torch.LongTensor] = None, inputs_embeds: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: Gemma3nMultimodalEmbedder
    doc: Embeds token ids or soft tokens for multimodal content into language model space.
  - name: __init__
    signature: (self, config: Gemma3nConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Gemma3nForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    return: List[int]
    class: Gemma3nForConditionalGeneration
    doc: Pad input IDs with image and audio tokens.
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Gemma3nForConditionalGeneration
  - name: get_attention_sliding_window_size
    signature: (self)
    class: Gemma3nForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    class: Gemma3nForConditionalGeneration
    doc: Projects the last hidden state from the vision model into language model space.
  - name: get_audio_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Gemma3nForConditionalGeneration
    doc: Projects the last hidden state from the audio encoder into language model space.
  - name: get_per_layer_inputs
    signature: (self, input_ids: torch.LongTensor)
    return: Optional[torch.Tensor]
    class: Gemma3nForConditionalGeneration
  - name: project_per_layer_inputs
    signature: (self, inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: Gemma3nForConditionalGeneration
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, **kwargs: object)
    return: LogitsProcessor
    class: Gemma3nForConditionalGeneration
    doc: Forward pass for multimodal Gemma3n.
  - name: tie_weights
    signature: (self)
    class: Gemma3nForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Gemma3nForConditionalGeneration
  - name: should_apply_lora
    signature: (self, module_name: str)
    return: bool
    class: Gemma3nForConditionalGeneration
  - name: get_hidden_dim
    signature: (self, module_name)
    class: Gemma3nForConditionalGeneration

File: models/glm4.py
  - name: __init__
    signature: (self, config, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Glm4Attention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Glm4Attention
  - name: __init__
    signature: (self, config, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Glm4DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Glm4DecoderLayer
  - name: __init__
    signature: (self, config: Glm4Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4Model
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Glm4Model
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: Glm4Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]
    class: Glm4Model
  - name: __init__
    signature: (self, config: Glm4Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Glm4ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Glm4ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Glm4ForCausalLM

File: models/glm4_moe.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, reduce_results: bool = True, prefix: str = '', tp_rank: Optional[int] = None, tp_size: Optional[int] = None)
    return: None
    class: Glm4MoeMLP
  - name: forward
    signature: (self, x, forward_batch = None, should_allreduce_fusion = False)
    class: Glm4MoeMLP
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, partial_rotary_factor: float = 0.5, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, head_dim: Optional[int] = None, rms_norm_eps: float = 1e-05, attention_bias: bool = True, quant_config: Optional[QuantizationConfig] = None, use_qk_norm: bool = False, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Glm4MoeAttention
  - name: _apply_qk_norm
    signature: (self, q: torch.Tensor, k: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Glm4MoeAttention
  - name: op_prepare
    signature: (self, state)
    class: Glm4MoeAttention
  - name: op_core
    signature: (self, state)
    class: Glm4MoeAttention
  - name: forward_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    class: Glm4MoeAttention
  - name: forward_core
    signature: (self, intermediate_state)
    class: Glm4MoeAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Glm4MoeAttention
  - name: __init__
    signature: (self, config, prefix: str = '', is_nextn: bool = False)
    class: Glm4MoeGate
  - name: forward
    signature: (self, hidden_states)
    class: Glm4MoeGate
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None, is_nextn: bool = False)
    class: Glm4MoeSparseMoeBlock
  - name: forward_normal_dual_stream
    signature: (self, hidden_states: torch.Tensor, should_allreduce_fusion: bool = False, use_reduce_scatter: bool = False)
    return: torch.Tensor
    class: Glm4MoeSparseMoeBlock
  - name: forward_normal
    signature: (self, hidden_states: torch.Tensor, should_allreduce_fusion: bool = False, use_reduce_scatter: bool = False)
    return: torch.Tensor
    class: Glm4MoeSparseMoeBlock
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, is_nextn: bool = False, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Glm4MoeDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)
    return: torch.Tensor
    class: Glm4MoeDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4MoeModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4MoeForCausalLM
  - name: determine_num_fused_shared_experts
    signature: (self, architecture: str = 'Glm4MoeForCausalLM')
    class: Glm4MoeForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Glm4MoeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn = False)
    class: Glm4MoeForCausalLM

File: models/glm4_moe_nextn.py
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4MoeModelNextN
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Glm4MoeModelNextN
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4MoeForCausalLMNextN
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Glm4MoeForCausalLMNextN
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Glm4MoeForCausalLMNextN

File: models/glm4v.py
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Glm4vRMSNorm
  - name: __init__
    signature: (self, in_features: int, hidden_features: int, bias: bool = False, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Glm4vVisionMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    class: Glm4vVisionMLP
  - name: __init__
    signature: (self, config: Glm4vVisionConfig, norm_layer: Optional[nn.Module] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4vVisionBlock
  - name: __init__
    signature: (self, patch_size: int = 14, temporal_patch_size: int = 2, in_channels: int = 3, hidden_size: int = 1536)
    return: None
    class: Glm4vVisionPatchEmbed
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Glm4vVisionPatchEmbed
  - name: __init__
    signature: (self, d_model: int, context_dim: int, quant_config: Optional[QuantizationConfig] = None, bias: bool = False, prefix: str = '')
    return: None
    class: Glm4vPatchMerger
  - name: forward
    signature: (self, x: torch.Tensor)
    class: Glm4vPatchMerger
  - name: __init__
    signature: (self, config: Glm4vVisionConfig)
    class: Glm4vVisionEmbeddings
  - name: forward
    signature: (self, embeddings, lengths, image_shapes, h_coords, w_coords)
    return: torch.Tensor
    class: Glm4vVisionEmbeddings
  - name: __init__
    signature: (self, dim: int, theta: float = 10000.0)
    return: None
    class: Glm4vVisionRotaryEmbedding
  - name: update_freqs_cache
    signature: (self, seqlen: int)
    return: None
    class: Glm4vVisionRotaryEmbedding
  - name: forward
    signature: (self, seqlen: int)
    return: torch.Tensor
    class: Glm4vVisionRotaryEmbedding
  - name: __init__
    signature: (self, vision_config: Glm4vVisionConfig, norm_eps: float = 1e-06, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4vVisionModel
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: Glm4vVisionModel
  - name: device
    signature: (self)
    return: torch.device
    class: Glm4vVisionModel
  - name: rot_pos_emb
    signature: (self, grid_thw: torch.Tensor)
    return: torch.Tensor
    class: Glm4vVisionModel
  - name: forward
    signature: (self, x: torch.Tensor, grid_thw: torch.Tensor)
    return: torch.Tensor
    class: Glm4vVisionModel
  - name: __init__
    signature: (self, config: Glm4vConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4vForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Glm4vForConditionalGeneration
  - name: get_video_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Glm4vForConditionalGeneration
  - name: _update_hf_config
    signature: (self)
    class: Glm4vForConditionalGeneration
    doc: update hf config to ensure vision attention num_attention_heads is divisible by tp_size
  - name: _pad_vit_attn_dummy_heads
    signature: (self, name: str, loaded_weight: torch.Tensor)
    class: Glm4vForConditionalGeneration
    doc: pad attn qkv weights for dummy heads
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Glm4vForConditionalGeneration

File: models/glm4v_moe.py
  - name: __init__
    signature: (self, config: Glm4vMoeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Glm4vMoeForConditionalGeneration
  - name: determine_num_fused_shared_experts
    signature: (self, architecture: str = 'Glm4MoeForCausalLM')
    class: Glm4vMoeForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn = False)
    class: Glm4vMoeForConditionalGeneration

File: models/gpt2.py
  - name: __init__
    signature: (self, layer_id: int, config: GPT2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPT2Attention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GPT2Attention
  - name: __init__
    signature: (self, intermediate_size: int, config: GPT2Config, act_layer: Type[nn.Module] = NewGELU, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPT2MLP
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: GPT2MLP
  - name: __init__
    signature: (self, layer_id: int, config: GPT2Config, act_layer: Type[nn.Module] = NewGELU, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPT2Block
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GPT2Block
  - name: __init__
    signature: (self, config: GPT2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPT2Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GPT2Model
  - name: __init__
    signature: (self, config: GPT2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPT2LMHeadModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GPT2LMHeadModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: GPT2LMHeadModel

File: models/gpt_bigcode.py
  - name: __init__
    signature: (self, layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPTBigCodeAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GPTBigCodeAttention
  - name: __init__
    signature: (self, intermediate_size: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPTBigMLP
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: GPTBigMLP
  - name: __init__
    signature: (self, layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPTBigCodeBlock
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GPTBigCodeBlock
  - name: __init__
    signature: (self, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPTBigCodeModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GPTBigCodeModel
  - name: __init__
    signature: (self, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GPTBigCodeForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GPTBigCodeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: GPTBigCodeForCausalLM

File: models/gpt_oss.py
  - name: __init__
    signature: (self, **kwargs)
    class: GptOssConfig
  - name: get_attention_sliding_window_size
    signature: (config)
  - name: __init__
    signature: (self, layer_id: int, config: GptOssConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GptOssSparseMoeBlock
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch] = None, should_allreduce_fusion: bool = False)
    return: torch.Tensor
    class: GptOssSparseMoeBlock
  - name: get_moe_weights
    signature: (self)
    class: GptOssSparseMoeBlock
  - name: forward_normal
    signature: (self, hidden_states: torch.Tensor, should_allreduce_fusion: bool = False)
    return: torch.Tensor
    class: GptOssSparseMoeBlock
  - name: _enable_fused_set_kv_buffer
    signature: ()
  - name: _create_fused_set_kv_buffer_arg
    signature: (value: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch)
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, head_dim: Optional[int] = None, rms_norm_eps: float = 1e-06, attention_bias: bool = False, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', sliding_window_size: int = -1, layer_type: str = '', params_dtype: torch.dtype = torch.bfloat16)
    return: None
    class: GptOssAttention
  - name: forward_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    class: GptOssAttention
  - name: forward_core
    signature: (self, intermediate_state)
    class: GptOssAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GptOssAttention
  - name: __init__
    signature: (self, config: GptOssConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', sliding_window_size: int | None = None)
    return: None
    class: GptOssDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: GptOssDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', decoder_layer_type: type[nn.Module] = GptOssDecoderLayer)
    return: None
    class: GptOssModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, PPProxyTensors]
    class: GptOssModel
  - name: __init__
    signature: (self, config: GptOssConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GptOssForCausalLM
  - name: routed_experts_weights_of_layer
    signature: (self)
    class: GptOssForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: GptOssForCausalLM
  - name: start_layer
    signature: (self)
    class: GptOssForCausalLM
  - name: end_layer
    signature: (self)
    class: GptOssForCausalLM
  - name: _get_default_weight_mapping
    signature: (self)
    class: GptOssForCausalLM
    doc: Generate default weight name mapping for GptOss safetensors.
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn: bool = False, weight_name_mapping: dict = None)
    class: GptOssForCausalLM
  - name: _load_weights_mxfp4
    signature: (self, weights, is_nextn, weight_name_mapping)
    class: GptOssForCausalLM
  - name: _load_mxfp4_experts_weights
    signature: (self, weights)
    class: GptOssForCausalLM
  - name: _load_normal_weights
    signature: (self, weights, is_nextn: bool, weight_name_mapping: dict, other_loaded_param_names = [])
    class: GptOssForCausalLM
  - name: get_embed_and_head
    signature: (self)
    class: GptOssForCausalLM
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: GptOssForCausalLM
  - name: set_eagle3_layers_to_capture
    signature: (self, layer_ids: Optional[List[int]] = None)
    class: GptOssForCausalLM
  - name: get_model_config_for_expert_location
    signature: (cls, config)
    class: GptOssForCausalLM
  - name: get_attention_sliding_window_size
    signature: (self)
    class: GptOssForCausalLM
  - name: _canonicalize_weights
    signature: (config, weights_in: Iterable[Tuple[str, torch.Tensor]])
  - name: _dequant_mlp_weight
    signature: (debug_name, w_blocks, w_scales)
  - name: __init__
    signature: (self, fn)
    class: _WeightCreator
  - name: maybe_materialize
    signature: (obj)
    class: _WeightCreator

File: models/granite.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GraniteMLP
  - name: forward
    signature: (self, x)
    class: GraniteMLP
  - name: __init__
    signature: (self, config: GraniteConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, rope_is_neox_style: bool = True, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GraniteAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GraniteAttention
  - name: __init__
    signature: (self, config: GraniteConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GraniteDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: GraniteDecoderLayer
  - name: __init__
    signature: (self, config: GraniteConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GraniteModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: GraniteModel
  - name: __init__
    signature: (self, config: GraniteConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GraniteForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False)
    return: LogitsProcessorOutput
    class: GraniteForCausalLM
  - name: get_module_name_from_weight_name
    signature: (self, name)
    class: GraniteForCausalLM
  - name: get_num_params
    signature: (self)
    class: GraniteForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: GraniteForCausalLM
  - name: get_weights_by_name
    signature: (self, name: str, truncate_size: int = 100, tp_size: int = 1)
    return: Optional[torch.Tensor]
    class: GraniteForCausalLM
    doc: Get the weights of the parameter by its name. Similar to `get_parameter` in Hugging Face.

File: models/granitemoe.py
  - name: __init__
    signature: (self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, tp_size: Optional[int] = None, prefix: str = '')
    class: GraniteMoeMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: GraniteMoeMoE
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, max_position: int = 4096 * 32, layer_id: int = 0, rope_theta: float = 10000, quant_config: Optional[QuantizationConfig] = None, attention_multiplier: Optional[float] = None, prefix: str = '')
    return: None
    class: GraniteMoeAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GraniteMoeAttention
  - name: __init__
    signature: (self, config: GraniteConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: GraniteMoeDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: GraniteMoeDecoderLayer
  - name: __init__
    signature: (self, config: GraniteConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GraniteMoeModel
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: GraniteMoeModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: GraniteMoeModel
  - name: __init__
    signature: (self, config: GraniteConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: GraniteMoeForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False)
    return: LogitsProcessorOutput
    class: GraniteMoeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[tuple[str, torch.Tensor]])
    return: set[str]
    class: GraniteMoeForCausalLM

File: models/grok.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', reduce_results = True, use_presharded_weights: bool = False, split_gate_up: bool = False)
    return: None
    class: Grok1MLP
  - name: forward
    signature: (self, x)
    class: Grok1MLP
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, tp_size: Optional[int] = None, reduce_results: bool = True, use_presharded_weights: bool = False, inplace: bool = True, no_combine: bool = False, prefix: str = '')
    class: Grok1MoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Grok1MoE
  - name: _yarn_linear_ramp_mask
    signature: (low: float, high: float, dim: int, dtype: torch.dtype)
    return: torch.Tensor
  - name: get_rope_scaling
    signature: (config)
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype, *, extra_method: str = 'yarn_log', extrapolation_factor: float = 1, attn_factor: float = 1, beta_fast: int = 32, beta_slow: int = 1)
    return: None
    class: ScalingRotaryEmbedding
  - name: _compute_inv_freq
    signature: (self, scaling_factor: float)
    return: torch.Tensor
    class: ScalingRotaryEmbedding
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: ScalingRotaryEmbedding
  - name: __init__
    signature: (self, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, max_position: int = 4096 * 32, rope_theta: float = 10000, quant_config: Optional[QuantizationConfig] = None, reduce_results: bool = True, alt_stream: Optional[torch.cuda.Stream] = None, load_presharded_attn: bool = False, prefix: str = '')
    return: None
    class: Grok1Attention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Grok1Attention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, load_presharded_moe: bool = False, load_presharded_attn: bool = False, load_presharded_mlp: bool = False, alt_stream: Optional[torch.cuda.Stream] = None, skip_moe: bool = False, prefix: str = '')
    return: None
    class: Grok1DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor] = None, deferred_norm: Optional[RMSNorm] = None)
    return: Tuple[torch.Tensor, torch.Tensor, RMSNorm]
    class: Grok1DecoderLayer
  - name: moe_with_rmoe
    signature: (self, x)
    class: Grok1DecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, load_presharded_moe: bool = False, load_presharded_embedding: bool = False, load_presharded_attn: bool = False, load_presharded_mlp: bool = False, replicate_embedding: bool = False, prefix: str = '')
    return: None
    class: Grok1Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Grok1Model
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Grok1ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Grok1ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]], ignore_parent_name: bool = False, check_hit_names: bool = True, model_config: PretrainedConfig | None = None)
    return: dict[str, torch.Tensor]
    class: Grok1ForCausalLM
  - name: load_weight_wrapper
    signature: (name: str, loaded_weight: torch.Tensor, *args, **kwargs)
    class: Grok1ForCausalLM
  - name: get_num_params_analytical
    signature: (self)
    class: Grok1ForCausalLM
  - name: get_num_params_torch
    signature: (self)
    class: Grok1ForCausalLM
  - name: _prepare_presharded_weights
    signature: (self, model_name_or_path: str, revision: Optional[str], fall_back_to_pt: bool)
    return: Tuple[str, list[str], bool]

File: models/hunyuan.py
  - name: _is_moe
    signature: (config: PretrainedConfig)
    return: bool
  - name: _get_cla_factor
    signature: (config: PretrainedConfig)
    return: int
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, bias: bool = False, prefix: str = '', reduce_results: bool = True)
    return: None
    class: HunYuanMLP
  - name: forward
    signature: (self, x)
    class: HunYuanMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, layer_id: int = -1)
    class: HunYuanSparseMoeBlock
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: HunYuanSparseMoeBlock
  - name: get_head_dim
    signature: (config)
  - name: check_head_dim
    signature: (config)
  - name: __init__
    signature: (self, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, bias: bool = False, prefix: str = '', attention_type: str = 'self', layer_id: int = -1)
    return: None
    class: HunYuanAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, kv_states: Optional[Tuple[torch.Tensor]] = None)
    return: torch.Tensor
    class: HunYuanAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', layer_id: int = -1)
    return: None
    class: HunYuanDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], kv_states: Optional[Tuple[torch.Tensor]] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: HunYuanDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: HunYuanModel
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: HunYuanModel
  - name: forward
    signature: (self, input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: HunYuanModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None)
    return: None
    class: HunYuanMoEV1ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: HunYuanMoEV1ForCausalLM
  - name: _split_qkv_weight
    signature: (self, qkv: torch.Tensor)
    class: HunYuanMoEV1ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: HunYuanMoEV1ForCausalLM
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: HunYuanMoEV1ForCausalLM

File: models/idefics2.py
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Idefics2VisionMLP
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Idefics2VisionMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Idefics2EncoderLayer
  - name: forward
    signature: (self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
    return: torch.Tensor
    class: Idefics2EncoderLayer
    doc: Args:
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Idefics2Encoder
  - name: forward
    signature: (self, inputs_embeds: torch.Tensor, cu_seqlens: torch.Tensor)
    return: torch.Tensor
    class: Idefics2Encoder
    doc: Args:
  - name: __init__
    signature: (self, config: PretrainedConfig)
    class: Idefics2VisionEmbeddings
  - name: get_position_ids
    signature: (self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor] = None)
    class: Idefics2VisionEmbeddings
  - name: forward
    signature: (self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor] = None)
    return: torch.Tensor
    class: Idefics2VisionEmbeddings
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, require_post_norm: bool = True, prefix: str = '')
    return: None
    class: Idefics2VisionTransformer
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Idefics2VisionTransformer
  - name: compute_cu_seqlens
    signature: (self, tgt_sizes: Optional[torch.Tensor] = None, input_embeds: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: Idefics2VisionTransformer
  - name: forward
    signature: (self, pixel_values, patch_attention_mask: Optional[torch.BoolTensor] = None, tgt_sizes: Optional[torch.IntTensor] = None)
    return: torch.Tensor
    class: Idefics2VisionTransformer

File: models/internlm2.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: InternLM2MLP
  - name: forward
    signature: (self, x)
    class: InternLM2MLP
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: InternLM2Attention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: InternLM2Attention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: InternLMDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: InternLMDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: InternLM2Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: InternLM2Model
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: InternLM2ForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: InternLM2ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: InternLM2ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: InternLM2ForCausalLM

File: models/internlm2_reward.py
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: InternLM2ForRewardModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = True)
    return: EmbeddingPoolerOutput
    class: InternLM2ForRewardModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: InternLM2ForRewardModel

File: models/interns1.py
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, use_flash_attn = True)
    return: None
    class: InternS1ForConditionalGeneration
  - name: pixel_shuffle
    signature: (self, x, scale_factor = 0.5)
    class: InternS1ForConditionalGeneration
  - name: extract_feature
    signature: (self, pixel_values)
    class: InternS1ForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    class: InternS1ForConditionalGeneration
    doc: Projects the last hidden state from the vision model into language model space.
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: InternS1ForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: InternS1ForConditionalGeneration
  - name: _mapping_interns1_name
    signature: (self, name)
    class: InternS1ForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: InternS1ForConditionalGeneration

File: models/internvl.py
  - name: __init__
    signature: (self, config, quant_config: QuantizationConfig = None)
    class: InternAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
    return: torch.Tensor
    class: InternAttention
  - name: __init__
    signature: (self, config: PretrainedConfig)
    class: InternVisionEmbeddings
  - name: _get_pos_embed
    signature: (self, pos_embed, H, W)
    class: InternVisionEmbeddings
  - name: forward
    signature: (self, pixel_values: torch.FloatTensor)
    return: torch.Tensor
    class: InternVisionEmbeddings
  - name: __init__
    signature: (self, hidden_size, eps = 1e-06)
    class: InternRMSNorm
  - name: forward
    signature: (self, hidden_states)
    class: InternRMSNorm
  - name: __init__
    signature: (self, config: PretrainedConfig)
    class: InternMLP
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: InternMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, drop_path_rate: float, quant_config: QuantizationConfig = None)
    class: InternVisionEncoderLayer
  - name: forward
    signature: (self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
    return: Tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[Tuple[torch.FloatTensor]]]
    class: InternVisionEncoderLayer
    doc: Args:
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None)
    class: InternVisionEncoder
  - name: forward
    signature: (self, inputs_embeds, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None)
    return: Union[Tuple, BaseModelOutput]
    class: InternVisionEncoder
    doc: Args:
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None)
    class: InternVisionModel
  - name: resize_pos_embeddings
    signature: (self, old_size, new_size, patch_size)
    class: InternVisionModel
  - name: get_input_embeddings
    signature: (self)
    class: InternVisionModel
  - name: forward
    signature: (self, pixel_values: Optional[torch.FloatTensor] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, pixel_embeds: Optional[torch.FloatTensor] = None)
    return: Union[Tuple, BaseModelOutputWithPooling]
    class: InternVisionModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, use_flash_attn = True)
    return: None
    class: InternVLChatModel
  - name: pixel_shuffle
    signature: (self, x, scale_factor = 0.5)
    class: InternVLChatModel
  - name: extract_feature
    signature: (self, pixel_values)
    class: InternVLChatModel
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    class: InternVLChatModel
    doc: Projects the last hidden state from the vision model into language model space.
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: InternVLChatModel
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: InternVLChatModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: InternVLChatModel

File: models/kimi_vl.py
  - name: __init__
    signature: (self, config: KimiVLConfig)
    class: KimiVLMultiModalProjector
  - name: forward
    signature: (self, image_features: torch.Tensor)
    return: torch.Tensor
    class: KimiVLMultiModalProjector
  - name: __init__
    signature: (self, config: KimiVLConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', **kwargs)
    return: None
    class: KimiVLForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: KimiVLForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: KimiVLForConditionalGeneration
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool = False)
    class: KimiVLForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: KimiVLForConditionalGeneration
  - name: get_spec_layer_idx_from_weight_name
    signature: (config: DeepseekV2Config, weight_name: str)
    return: Optional[int]

File: models/kimi_vl_moonvit.py
  - name: multihead_attention
    signature: (q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, q_cu_seqlens: Optional[torch.Tensor] = None, k_cu_seqlens: Optional[torch.Tensor] = None)
    doc: Multi-head attention using flash attention 2.
  - name: sdpa_attention
    signature: (q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, q_cu_seqlens: Optional[torch.Tensor] = None, k_cu_seqlens: Optional[torch.Tensor] = None)
    return: torch.Tensor
    doc: Multi-head attention using torch scaled dot product attention.
  - name: _apply_rope_input_validation
    signature: (x, freqs_cis)
  - name: apply_rope
    signature: (xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor)
    return: tuple[torch.Tensor, torch.Tensor]
    doc: Args: (The leading dimensions of all inputs should be the same)
  - name: __init__
    signature: (self, height: int, width: int, dim: int, interpolation_mode: str = 'bicubic')
    return: None
    class: Learnable2DInterpPosEmb
  - name: reset_parameters
    signature: (self)
    class: Learnable2DInterpPosEmb
  - name: forward
    signature: (self, x: torch.Tensor, grid_hws: torch.Tensor)
    return: torch.Tensor
    class: Learnable2DInterpPosEmb
  - name: __init__
    signature: (self, out_dim: int, in_dim: int = 3, patch_size: Union[int, Tuple[int, int]] = (14, 14), pos_emb_height: int = 14, pos_emb_width: int = 14)
    class: MoonVisionPatchEmbed
  - name: forward
    signature: (self, x: torch.Tensor, grid_hw: torch.Tensor)
    return: torch.Tensor
    class: MoonVisionPatchEmbed
    doc: Args:
  - name: __init__
    signature: (self, dim: int, max_height: int, max_width: int, theta_base = 10000, device = 'cuda')
    class: Rope2DPosEmb
  - name: extra_repr
    signature: (self)
    class: Rope2DPosEmb
  - name: precomputed_freqs_cis
    signature: (self)
    return: torch.Tensor
    class: Rope2DPosEmb
    doc: Calculate the cis(freqs) for each position in the 2D grid.
  - name: get_freqs_cis_by_seqlens
    signature: (self, grid_hws: torch.Tensor)
    return: torch.Tensor
    class: Rope2DPosEmb
    doc: Args:
  - name: get_freqs_cis_by_idx
    signature: (self, pos_idx: torch.Tensor, pos_idx_mask: torch.Tensor)
    return: torch.Tensor
    class: Rope2DPosEmb
    doc: Args:
  - name: __init__
    signature: (self, dims: list[int], activation, bias = True)
    class: MLP2
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: MLP2
  - name: __init__
    signature: (self, num_heads: int, hidden_dim: int, mlp_dim: int, *, attn_implementation: str = 'flash_attention_2', activation = F.gelu, attn_bias: bool = False)
    class: MoonVitEncoderLayer
  - name: attention_qkvpacked
    signature: (self, x: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Optional[torch.Tensor] = None)
    class: MoonVitEncoderLayer
    doc: Args:
  - name: forward
    signature: (self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Union[torch.Tensor, None] = None)
    return: torch.Tensor
    class: MoonVitEncoderLayer
    doc: Args:
  - name: __init__
    signature: (self, hidden_dim: int, num_layers: int, block_cfg: dict)
    return: None
    class: MoonVitEncoder
  - name: forward
    signature: (self, hidden_states: torch.Tensor, grid_hw: torch.Tensor)
    return: torch.Tensor
    class: MoonVitEncoder
  - name: patch_merger
    signature: (x: torch.Tensor, grid_hw: torch.Tensor, merge_kernel_size: list[int, int] = (2, 2))
    return: List[torch.Tensor]
  - name: __init__
    signature: (self, in_channels: int, merge_kernel_size: list[int, int], hidden_act: str = 'gelu', ln_eps: float = 1e-05, out_dim: int = 4096)
    class: MoonVitVLProjector
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: MoonVitVLProjector
  - name: __init__
    signature: (self, config: MoonViTConfig, *inputs, **kwargs)
    class: MoonVitPretrainedModel
  - name: forward
    signature: (self, pixel_values: torch.Tensor, grid_hw: torch.Tensor)
    return: torch.Tensor
    class: MoonVitPretrainedModel
    doc: Args:

File: models/llama.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', reduce_results: bool = True)
    return: None
    class: LlamaMLP
  - name: forward
    signature: (self, x, forward_batch = None, use_reduce_scatter: bool = False)
    class: LlamaMLP
  - name: __init__
    signature: (self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, rope_is_neox_style: bool = True, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', bias: bool = False)
    return: None
    class: LlamaAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: LlamaAttention
  - name: __init__
    signature: (self, config: LlamaConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: LlamaDecoderLayer
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]], PPProxyTensors]
    class: LlamaModel
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: LlamaModel
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaForCausalLM
  - name: _init_model
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: LlamaForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: LogitsProcessorOutput
    class: LlamaForCausalLM
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor = None)
    return: Optional[LogitsProcessorOutput]
    class: LlamaForCausalLM
  - name: start_layer
    signature: (self)
    class: LlamaForCausalLM
  - name: end_layer
    signature: (self)
    class: LlamaForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: LlamaForCausalLM
  - name: get_module_name_from_weight_name
    signature: (self, name)
    class: LlamaForCausalLM
  - name: get_num_params
    signature: (self)
    class: LlamaForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlamaForCausalLM
  - name: get_weights_by_name
    signature: (self, name: str, truncate_size: int = 100, tp_size: int = 1)
    return: Optional[torch.Tensor]
    class: LlamaForCausalLM
    doc: Get the weights of the parameter by its name. Similar to `get_parameter` in Hugging Face.
  - name: get_embed_and_head
    signature: (self)
    class: LlamaForCausalLM
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: LlamaForCausalLM
  - name: get_embed
    signature: (self)
    class: LlamaForCausalLM
  - name: set_embed
    signature: (self, embed)
    class: LlamaForCausalLM
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: LlamaForCausalLM
  - name: set_eagle3_layers_to_capture
    signature: (self, layer_ids: Optional[List[int]] = None)
    class: LlamaForCausalLM

File: models/llama4.py
  - name: custom_routing_function
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Llama4MoE
  - name: __init__
    signature: (self, config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Llama4MoE
  - name: forward
    signature: (self, hidden_states, forward_batch: ForwardBatch, use_reduce_scatter: bool = False)
    class: Llama4MoE
  - name: _forward_core
    signature: (self, hidden_states, forward_mode: ForwardMode)
    class: Llama4MoE
  - name: _forward_core_normal
    signature: (self, hidden_states)
    class: Llama4MoE
  - name: _forward_core_shared_routed_overlap
    signature: (self, hidden_states)
    class: Llama4MoE
  - name: _get_or_create_alt_stream
    signature: (device_module)
  - name: __init__
    signature: (self, config: Llama4TextConfig, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, bias: bool = False, bias_o_proj: bool = False, prefix: str = '')
    return: None
    class: Llama4Attention
  - name: _get_attn_scale
    signature: (self, positions: torch.Tensor)
    return: torch.Tensor
    class: Llama4Attention
  - name: _mul_attn_scale
    signature: (self, positions, q)
    class: Llama4Attention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Llama4Attention
  - name: __init__
    signature: (self, config: Llama4TextConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Llama4DecoderLayer
  - name: _is_moe_layer
    signature: (self, layer_id: int)
    return: bool
    class: Llama4DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Llama4DecoderLayer
  - name: __init__
    signature: (self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Llama4Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]
    class: Llama4Model
  - name: __init__
    signature: (self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Llama4ForCausalLM
  - name: get_input_embeddings
    signature: (self)
    class: Llama4ForCausalLM
  - name: _init_model
    signature: (self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Llama4ForCausalLM

File: models/llama_classification.py
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaForClassification
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = True)
    return: EmbeddingPoolerOutput
    class: LlamaForClassification
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlamaForClassification

File: models/llama_eagle.py
  - name: __init__
    signature: (self, config: LlamaConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaDecoderLayer
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: LlamaModel
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaForCausalLMEagle
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlamaForCausalLMEagle

File: models/llama_eagle3.py
  - name: __init__
    signature: (self, config: LlamaConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, embeds: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: LlamaDecoderLayer
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: LlamaModel
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaForCausalLMEagle3
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    return: None
    class: LlamaForCausalLMEagle3
  - name: get_hot_token_id
    signature: (self)
    class: LlamaForCausalLMEagle3

File: models/llama_embedding.py
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config = None, prefix: str = '')
    return: None
    class: LlamaEmbeddingModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = True)
    return: EmbeddingPoolerOutput
    class: LlamaEmbeddingModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlamaEmbeddingModel

File: models/llama_reward.py
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaForSequenceClassification
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = True)
    return: EmbeddingPoolerOutput
    class: LlamaForSequenceClassification
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlamaForSequenceClassification
  - name: __init__
    signature: (self, hidden_size, num_label)
    class: LlamaForSequenceClassificationWithNormal_Weights.Weights
  - name: forward
    signature: (self, x)
    class: LlamaForSequenceClassificationWithNormal_Weights.Weights
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaForSequenceClassificationWithNormal_Weights
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = True)
    return: EmbeddingPoolerOutput
    class: LlamaForSequenceClassificationWithNormal_Weights
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlamaForSequenceClassificationWithNormal_Weights

File: models/llava.py
  - name: pad_input_ids
    signature: (self, input_ids: List[int], image_inputs: MultimodalInputs)
    class: LlavaBaseForCausalLM
  - name: encode_images
    signature: (self, pixel_values: Union[torch.Tensor, List[torch.Tensor]])
    return: torch.Tensor
    class: LlavaBaseForCausalLM
    doc: encode images by vision tower and multimodal projector
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: LlavaBaseForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlavaBaseForCausalLM
  - name: num_patches_per_side
    signature: (self)
    class: LlavaBaseForCausalLM
  - name: __init__
    signature: (self, config: LlavaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlavaLlamaForCausalLM
  - name: __init__
    signature: (self, config: LlavaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlavaQwenForCausalLM
  - name: __init__
    signature: (self, config: LlavaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlavaMistralForCausalLM
  - name: dtype
    signature: (self)
    class: LlavaForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], image_inputs: MultimodalInputs)
    class: LlavaForConditionalGeneration
  - name: _get_sgl_model_cls
    signature: (self, config, auto_model_type: Type[AutoModel] = AutoModel)
    class: LlavaForConditionalGeneration
    doc: Get the SGLang model implementation class according to config.
  - name: _config_cls_name_to_arch_name_mapping
    signature: (self, auto_model_type: Type[AutoModel])
    return: Dict[str, str]
    class: LlavaForConditionalGeneration
  - name: __init__
    signature: (self, config: LlavaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlavaForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: LlavaForConditionalGeneration
    doc: Extract features from image inputs.
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool = False)
    class: LlavaForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlavaForConditionalGeneration
    doc: Load weights for LlavaForConditionalGeneration.

File: models/llavavid.py
  - name: __init__
    signature: (self, config: LlavaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlavaVidForCausalLM
  - name: pad_input_ids
    signature: (self, input_ids: List[int], image_inputs: MultimodalInputs)
    class: LlavaVidForCausalLM
  - name: encode_images
    signature: (self, pixel_values: torch.Tensor)
    return: torch.Tensor
    class: LlavaVidForCausalLM
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: LlavaVidForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LlavaVidForCausalLM
  - name: num_patches_per_side
    signature: (self)
    class: LlavaVidForCausalLM

File: models/longcat_flash.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, reduce_results: bool = False, prefix: str = '')
    return: None
    class: LongcatFlashMLP
  - name: forward
    signature: (self, x)
    class: LongcatFlashMLP
  - name: __init__
    signature: (self, config, zero_expert_num = 0, rounter_params_dtype = torch.float32, prefix: str = '')
    class: LongcatFlashRouter
  - name: forward
    signature: (self, hidden_states)
    class: LongcatFlashRouter
  - name: __init__
    signature: (self, config: LongcatFlashConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: LongcatFlashMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: LongcatFlashMoE
  - name: get_moe_weights
    signature: (self)
    class: LongcatFlashMoE
  - name: __init__
    signature: (self, config: LongcatFlashConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: LongcatFlashDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)
    return: torch.Tensor
    class: LongcatFlashDecoderLayer
  - name: forward_mlp
    signature: (self, hidden_states, positions, residual, forward_batch, zero_allocator)
    class: LongcatFlashDecoderLayer
  - name: __init__
    signature: (self, config: LongcatFlashConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LongcatFlashModel
  - name: get_input_embeddings
    signature: (self)
    return: torch.Tensor
    class: LongcatFlashModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: LongcatFlashModel
  - name: __init__
    signature: (self, config: LongcatFlashConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LongcatFlashForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: LongcatFlashForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: LongcatFlashForCausalLM
  - name: post_load_weights
    signature: (self, weight_names = None)
    class: LongcatFlashForCausalLM
  - name: _weight_requant_ue8m0
    signature: (self)
    class: LongcatFlashForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LongcatFlashForCausalLM
  - name: get_embed_and_head
    signature: (self)
    class: LongcatFlashForCausalLM
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: LongcatFlashForCausalLM
  - name: get_model_config_for_expert_location
    signature: (cls, config)
    class: LongcatFlashForCausalLM

File: models/longcat_flash_nextn.py
  - name: __init__
    signature: (self, config: LongcatFlashConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: LongcatFlashDenseDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)
    return: torch.Tensor
    class: LongcatFlashDenseDecoderLayer
  - name: __init__
    signature: (self, config: LongcatFlashConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LongcatFlashModelNextN
  - name: get_input_embeddings
    signature: (self)
    return: torch.Tensor
    class: LongcatFlashModelNextN
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: LongcatFlashModelNextN
  - name: __init__
    signature: (self, config: LongcatFlashConfig, quant_config: Optional[QuantizationConfig] = None)
    return: None
    class: LongcatFlashForCausalLMNextN
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: LongcatFlashForCausalLMNextN
  - name: post_load_weights
    signature: (self)
    class: LongcatFlashForCausalLMNextN
  - name: _weight_requant_ue8m0
    signature: (self)
    class: LongcatFlashForCausalLMNextN
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: LongcatFlashForCausalLMNextN

File: models/mimo.py
  - name: __init__
    signature: (self, config: MiMoConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiMoModel
  - name: __init__
    signature: (self, config: MiMoConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiMoForCausalLM
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: MiMoForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False)
    return: torch.Tensor
    class: MiMoForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MiMoForCausalLM
  - name: get_embed_and_head
    signature: (self)
    class: MiMoForCausalLM
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: MiMoForCausalLM
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: MiMoForCausalLM

File: models/mimo_mtp.py
  - name: __init__
    signature: (self, config: PretrainedConfig, prefix: str, quant_config: Optional[QuantizationConfig] = None)
    return: None
    class: MiMoMultiTokenPredictorLayer
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: MiMoMultiTokenPredictorLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiMoMTP
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: MiMoMTP
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MiMoMTP
  - name: map_model_name_to_mtp_param_name
    signature: (self, name: str)
    return: str
    class: MiMoMTP
  - name: get_embed_and_head
    signature: (self)
    class: MiMoMTP
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: MiMoMTP

File: models/minicpm.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPMMLP
  - name: forward
    signature: (self, x)
    class: MiniCPMMLP
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPMAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: MiniCPMAttention
  - name: __init__
    signature: (self, config, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPMDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: MiniCPMDecoderLayer
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPMModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: MiniCPMModel
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPMForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: MiniCPMForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MiniCPMForCausalLM

File: models/minicpm3.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPM3MLP
  - name: forward
    signature: (self, x)
    class: MiniCPM3MLP
  - name: input_to_float8
    signature: (x, dtype = torch.float8_e4m3fn)
  - name: __init__
    signature: (self, config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, layer_id = None, prefix: str = '')
    return: None
    class: MiniCPM3AttentionMLA
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: MiniCPM3AttentionMLA
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPM3DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: MiniCPM3DecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPM3Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: MiniCPM3Model
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPM3ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: MiniCPM3ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MiniCPM3ForCausalLM

File: models/minicpmo.py
  - name: apply_spk_emb
    signature: (input_ids: torch.Tensor = None, spk_emb: torch.Tensor = None, input_embeds: torch.Tensor = None, spk_emb_token_id: int = 0, num_spk_embs: int = 1)
    doc: Replace consecutive `num_spk_embs` speaker embedding placeholders in input_embeds with pre-prepared speaker embeddings. This is an in-place replacement, no new tensor is created, so no value is returned.
  - name: make_streaming_chunk_mask_generation
    signature: (inputs_embeds: torch.Tensor, past_seen_tokens: int, streaming_tts_text_mask: torch.Tensor, streaming_reserved_length: int = 300, streaming_audio_chunk_size: int = 50, streaming_text_chunk_size: int = 10, num_spk_emb: int = 1, use_spk_emb: bool = True)
    return: torch.Tensor
    doc: In streaming audio generation, determine which `text` positions the TTS model can attend to when generating each chunk of `audio` tokens.
  - name: __init__
    signature: (self, dim: int, intermediate_dim: int, kernel: int, dilation: int, layer_scale_init_value: float = 1e-06)
    class: ConvNeXtBlock
  - name: forward
    signature: (self, x: torch.Tensor, cond = None)
    return: torch.Tensor
    class: ConvNeXtBlock
  - name: __init__
    signature: (self, idim: int, odim: int, n_layer = 12, bn_dim = 64, hidden = 256, kernel = 7, dilation = 2, up = False)
    class: DVAEDecoder
  - name: forward
    signature: (self, x: torch.Tensor, conditioning = None)
    return: torch.Tensor
    class: DVAEDecoder
  - name: __init__
    signature: (self, dim: int, levels: List[int], G: int, R: int, eps = 1e-05, transpose = True)
    class: GFSQ
  - name: _embed
    signature: (self, x: torch.Tensor)
    class: GFSQ
  - name: __call__
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: GFSQ
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: GFSQ
  - name: __init__
    signature: (self)
    class: DVAE
  - name: forward
    signature: (self, inp: torch.Tensor, mode: Literal['encode', 'decode'] = 'decode')
    return: torch.Tensor
    class: DVAE
  - name: __init__
    signature: (self, penalty: float, max_input_ids: int, past_window: int)
    class: CustomRepetitionPenaltyLogitsProcessorRepeat
  - name: __call__
    signature: (self, input_ids: torch.LongTensor, scores: torch.FloatTensor)
    return: torch.FloatTensor
    class: CustomRepetitionPenaltyLogitsProcessorRepeat
  - name: __init__
    signature: (self, config: PretrainedConfig)
    class: ConditionalChatTTS
  - name: merge_inputs_embeds
    signature: (self, input_ids: torch.Tensor, lm_spk_emb_last_hidden_states: Optional[torch.Tensor] = None)
    class: ConditionalChatTTS
    doc: Merge `input_ids` and `lm_spk_emb_last_hidden_states` to `inputs_embeds`.
  - name: prefill_text
    signature: (self, input_ids: torch.Tensor, position_ids: torch.LongTensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], lm_spk_emb_last_hidden_states: Optional[torch.Tensor] = None)
    class: ConditionalChatTTS
    doc: Prefill a chunk of new text tokens in streaming setting.
  - name: prefill_audio_ids
    signature: (self, input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], streaming_tts_text_mask = None, add_audio_bos: bool = True)
    class: ConditionalChatTTS
    doc: Prefill a chunk of audio ids to the model. Used in sliding-window long audio generation.
  - name: generate
    signature: (self, input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], temperature: torch.Tensor, eos_token: Union[int, torch.Tensor], streaming_tts_text_mask = None, force_no_stop = False, min_new_token = 10, max_new_token = 50, logits_warpers: List[LogitsWarper] = [], logits_processors: List[CustomRepetitionPenaltyLogitsProcessorRepeat] = [], show_tqdm = False)
    class: ConditionalChatTTS
    doc: Generate audio codes in streaming setting or non-streaming setting.
  - name: decode_to_mel_specs
    signature: (self, result_list: List[torch.Tensor])
    class: ConditionalChatTTS
    doc: Decode discrete audio codes to mel spectrograms.
  - name: __init__
    signature: (self, config: WhisperConfig, layer_idx: int = None)
    class: MiniCPMWhisperEncoderLayer
  - name: forward
    signature: (self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, layer_head_mask: torch.Tensor, output_attentions: bool = False, past_key_values: Optional[EncoderDecoderCache] = None, use_cache: Optional[bool] = False)
    return: torch.Tensor
    class: MiniCPMWhisperEncoderLayer
    doc: Args:
  - name: __init__
    signature: (self, config: WhisperConfig)
    class: MiniCPMWhisperEncoder
  - name: forward
    signature: (self, input_features, attention_mask = None, head_mask = None, output_attentions = None, output_hidden_states = None, return_dict = None, past_key_values: Optional[EncoderDecoderCache] = None, use_cache: Optional[bool] = None)
    class: MiniCPMWhisperEncoder
    doc: Forward pass of the Whisper encoder.
  - name: __init__
    signature: (self, in_dim, out_dim)
    class: MultiModalProjector
  - name: forward
    signature: (self, audio_features)
    class: MultiModalProjector
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None)
    return: None
    class: MiniCPMO
  - name: init_tts_module
    signature: (self)
    class: MiniCPMO
  - name: init_audio_module
    signature: (self)
    class: MiniCPMO
  - name: init_llm
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: nn.Module
    class: MiniCPMO
  - name: init_vision_module
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str = '')
    class: MiniCPMO
  - name: init_resampler
    signature: (self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: nn.Module
    class: MiniCPMO
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_input: MultimodalInputs)
    class: MiniCPMO
  - name: _get_feat_extract_output_lengths
    signature: (self, input_lengths: torch.LongTensor)
    class: MiniCPMO
    doc: Computes the output length of the convolutional layers and the output length of the audio encoder
  - name: get_audio_embedding_streaming
    signature: (self, items: List[MultimodalDataItem])
    class: MiniCPMO
    doc: Extract audio embeddings in a streaming manner using cached key-value pairs.
  - name: subsequent_chunk_mask
    signature: (self, size: int, chunk_size: int, num_left_chunks: int = -1, device: torch.device = torch.device('cpu'), num_lookhead: int = 0)
    return: torch.Tensor
    class: MiniCPMO
    doc: Create mask for subsequent steps (size, size) with chunk size,
  - name: get_audio_embedding
    signature: (self, items: List[MultimodalDataItem], chunk_length = -1)
    class: MiniCPMO
    doc: Extract full audio embeddings with optional chunk-based attention.
  - name: get_audio_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: MiniCPMO
  - name: get_omni_embedding
    signature: (self, items: List[MultimodalDataItem], chunk_length = -1, stream_input = False)
    class: MiniCPMO
    doc: Args:
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: MiniCPMO
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, **kwargs: Any)
    return: torch.Tensor
    class: MiniCPMO
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MiniCPMO

File: models/minicpmv.py
  - name: get_1d_sincos_pos_embed_from_grid
    signature: (embed_dim: int, pos: np.ndarray, version: Tuple[int, int] = (2, 0))
    return: torch.Tensor
    doc: embed_dim: output dimension for each position
  - name: get_2d_sincos_pos_embed_from_grid
    signature: (embed_dim: int, grid: np.ndarray, version: Tuple[int, int] = (2, 0))
    return: torch.Tensor
  - name: get_2d_sincos_pos_embed
    signature: (embed_dim: int, grid_size: Union[int, Tuple[int, int]], cls_token: bool = False, version: Tuple[int, int] = (2, 0))
    return: torch.Tensor
    doc: grid_size: int of the grid height and width
  - name: __init__
    signature: (self, num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int] = None, norm_layer: Callable[[int], nn.LayerNorm] = DEFAULT_LN, do_post_projection: bool = True, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: BaseResampler
  - name: _init_weights
    signature: (self, m: nn.Module)
    return: None
    class: BaseResampler
  - name: _repeat
    signature: (self, query, N: int)
    class: BaseResampler
  - name: __init__
    signature: (self, num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int] = None, norm_layer: Callable[[int], nn.LayerNorm] = DEFAULT_LN, max_size: Tuple[int, int] = (70, 70), quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Resampler2_5
  - name: _set_2d_pos_cache
    signature: (self, max_size: Tuple[int, int], device: torch.types.Device = 'cpu')
    return: None
    class: Resampler2_5
  - name: _adjust_pos_cache
    signature: (self, tgt_sizes: torch.Tensor, device: torch.types.Device)
    return: None
    class: Resampler2_5
  - name: forward
    signature: (self, x: torch.Tensor, tgt_sizes: torch.Tensor)
    return: torch.Tensor
    class: Resampler2_5
  - name: get_version_by_config
    signature: (config: PretrainedConfig)
    return: Tuple[int, ...]
  - name: __init__
    signature: (self, *, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: MiniCPMBaseModel
  - name: _get_image_bounds
    signature: (self, input_ids: torch.Tensor, pad_values: List[int], im_start_id: int, im_end_id: int, slice_start_id: Optional[int] = None, slice_end_id: Optional[int] = None)
    return: torch.Tensor
    class: MiniCPMBaseModel
    doc: Returns a tensor indicating the bounds (start and end token ids) of the images
  - name: _parse_and_validate_inputs
    signature: (self, input_ids: torch.Tensor, **kwargs: object)
    return: Optional[MiniCPMVImageInputs]
    class: MiniCPMBaseModel
  - name: get_embedding
    signature: (self, input_ids: torch.Tensor, image_inputs: Optional[MiniCPMVImageInputs])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: MiniCPMBaseModel
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: MiniCPMBaseModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, **kwargs: Any)
    return: torch.Tensor
    class: MiniCPMBaseModel
  - name: init_llm
    signature: (self, config: Qwen2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: nn.Module
    class: MiniCPMBaseModel
  - name: init_vision_module
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str = '')
    return: nn.Module
    class: MiniCPMBaseModel
  - name: init_resampler
    signature: (self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: nn.Module
    class: MiniCPMBaseModel
  - name: get_vision_embedding
    signature: (self, pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor] = None, tgt_sizes: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: MiniCPMBaseModel
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: MiniCPMBaseModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: MiniCPMV2_6
  - name: init_llm
    signature: (self, config: Qwen2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: nn.Module
    class: MiniCPMV2_6
  - name: init_vision_module
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str = '')
    return: nn.Module
    class: MiniCPMV2_6
  - name: init_resampler
    signature: (self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: nn.Module
    class: MiniCPMV2_6
  - name: get_vision_embedding
    signature: (self, pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor] = None, tgt_sizes: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: MiniCPMV2_6
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: MiniCPMV2_6
  - name: pad_input_ids
    signature: (self, input_ids: List[int], image_inputs: MultimodalInputs)
    class: MiniCPMV2_6
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MiniCPMV
  - name: __getattr__
    signature: (self, name)
    class: MiniCPMV
  - name: __call__
    signature: (self, *args, **kwargs)
    class: MiniCPMV
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MiniCPMV

File: models/mistral.py
  - name: __init__
    signature: (self, **kwargs)
    class: Mistral3ForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Mistral3ForConditionalGeneration
    doc: Extract features from image inputs.
  - name: __getattr__
    signature: (self, name)
    class: Mistral3ForConditionalGeneration
  - name: __hasattr__
    signature: (self, name)
    class: Mistral3ForConditionalGeneration
  - name: __call__
    signature: (self, *args, **kwargs)
    class: Mistral3ForConditionalGeneration

File: models/mixtral.py
  - name: __init__
    signature: (self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, tp_size: Optional[int] = None, prefix: str = '')
    class: MixtralMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: MixtralMoE
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, max_position: int = 4096 * 32, rope_theta: float = 10000, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MixtralAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: MixtralAttention
  - name: __init__
    signature: (self, config: MixtralConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MixtralDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: torch.Tensor
    class: MixtralDecoderLayer
  - name: __init__
    signature: (self, config: MixtralConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MixtralModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, PPProxyTensors]
    class: MixtralModel
  - name: __init__
    signature: (self, config: MixtralConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MixtralForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: MixtralForCausalLM
  - name: start_layer
    signature: (self)
    class: MixtralForCausalLM
  - name: end_layer
    signature: (self)
    class: MixtralForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MixtralForCausalLM

File: models/mixtral_quant.py
  - name: __init__
    signature: (self, num_experts: int, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MixtralMLP
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: MixtralMLP
  - name: __init__
    signature: (self, config: MixtralConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: MixtralMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: MixtralMoE
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, max_position: int = 4096 * 32, rope_theta: float = 10000, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MixtralAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: MixtralAttention
  - name: __init__
    signature: (self, config: MixtralConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MixtralDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: torch.Tensor
    class: MixtralDecoderLayer
  - name: __init__
    signature: (self, config: MixtralConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: MixtralModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: MixtralModel
  - name: __init__
    signature: (self, config: MixtralConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: QuantMixtralForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: QuantMixtralForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: QuantMixtralForCausalLM

File: models/mllama.py
  - name: __init__
    signature: (self, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]], bias: bool = False)
    return: None
    class: ColumnParallelConv2dPatch
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: ColumnParallelConv2dPatch
  - name: __init__
    signature: (self, config: config_mllama.MllamaVisionConfig, is_gated: bool = True)
    class: MllamaPrecomputedAspectRatioEmbedding
  - name: forward
    signature: (self, hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor)
    return: torch.Tensor
    class: MllamaPrecomputedAspectRatioEmbedding
  - name: __init__
    signature: (self, config: config_mllama.MllamaVisionConfig)
    class: MllamaPrecomputedPositionEmbedding
  - name: forward
    signature: (self, hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor)
    return: torch.Tensor
    class: MllamaPrecomputedPositionEmbedding
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: MllamaVisionMLP
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: MllamaVisionMLP
  - name: __init__
    signature: (self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig] = None, is_gated: bool = False, prefix: str = '')
    class: MllamaVisionEncoderLayer
  - name: forward
    signature: (self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None)
    class: MllamaVisionEncoderLayer
  - name: __init__
    signature: (self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig] = None, num_layers = 32, is_gated = False, output_hidden_states = None, prefix: str = '')
    class: MllamaVisionEncoder
  - name: forward
    signature: (self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None)
    return: Union[Tuple, BaseModelOutput]
    class: MllamaVisionEncoder
  - name: __init__
    signature: (self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: MllamaVisionModel
  - name: apply_class_embedding
    signature: (self, hidden_state: torch.Tensor)
    return: torch.Tensor
    class: MllamaVisionModel
  - name: forward
    signature: (self, pixel_values: torch.Tensor, aspect_ratio_ids: torch.Tensor, aspect_ratio_mask: torch.Tensor)
    return: torch.Tensor
    class: MllamaVisionModel
  - name: __init__
    signature: (self, hidden_size, eps = 1e-06)
    class: MllamaTextRMSNorm
  - name: forward
    signature: (self, hidden_states)
    class: MllamaTextRMSNorm
  - name: extra_repr
    signature: (self)
    class: MllamaTextRMSNorm
  - name: __init__
    signature: (self, config: Optional[config_mllama.MllamaTextConfig] = None, layer_id: Optional[int] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: MllamaTextCrossAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], cross_attention_states: Optional[torch.Tensor], forward_batch: ForwardBatch)
    return: torch.Tensor
    class: MllamaTextCrossAttention
  - name: __init__
    signature: (self, config: config_mllama.MllamaTextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str = '')
    return: None
    class: MllamaCrossAttentionDecoderLayer
  - name: forward
    signature: (self, hidden_states: torch.Tensor, cross_attention_states: torch.Tensor, cross_attention_mask: torch.Tensor, full_text_row_masked_out_mask: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: MllamaCrossAttentionDecoderLayer
  - name: __init__
    signature: (self, config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str = '')
    class: MllamaTextModel
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool)
    return: torch.Tensor
    class: MllamaTextModel
  - name: __init__
    signature: (self, config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str = '')
    class: MllamaForCausalLM
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool)
    return: torch.Tensor
    class: MllamaForCausalLM
  - name: __init__
    signature: (self, config: config_mllama.MllamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: MllamaForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: MllamaForConditionalGeneration
  - name: _batch_image_inputs
    signature: (self, forward_batch: ForwardBatch)
    class: MllamaForConditionalGeneration
  - name: flat_encoder_result
    signature: (self, cross_attention_states: torch.Tensor, encoder_lens_need: List[int])
    class: MllamaForConditionalGeneration
  - name: get_full_text_row_masked_out_mask
    signature: (self, forward_batch: ForwardBatch)
    class: MllamaForConditionalGeneration
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: Union[Tuple, CausalLMOutputWithPast]
    class: MllamaForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: MllamaForConditionalGeneration

File: models/mllama4.py
  - name: __init__
    signature: (self, input_size: int, intermediate_size: int, output_size: int, bias: bool, output_activation: bool, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', use_data_parallel: bool = False)
    class: Llama4VisionMLP
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Llama4VisionMLP
  - name: pixel_shuffle
    signature: (input_tensor, shuffle_ratio)
  - name: __init__
    signature: (self, config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', use_data_parallel: bool = False)
    class: Llama4VisionPixelShuffleMLP
  - name: forward
    signature: (self, encoded_patches: torch.Tensor)
    return: torch.Tensor
    class: Llama4VisionPixelShuffleMLP
  - name: apply_position_embedding
    signature: (q, k, freqs_ci, shape)
  - name: __init__
    signature: (self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str = '', use_data_parallel: bool = False)
    class: Llama4VisionEncoderLayer
  - name: forward
    signature: (self, hidden_state: torch.Tensor, freqs_ci: torch.Tensor)
    class: Llama4VisionEncoderLayer
  - name: __init__
    signature: (self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str = '', use_data_parallel: bool = False)
    class: Llama4VisionEncoder
  - name: forward
    signature: (self, hidden_states: torch.Tensor, freqs_ci: torch.Tensor)
    return: torch.Tensor
    class: Llama4VisionEncoder
    doc: Args:
  - name: __init__
    signature: (self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', use_data_parallel: bool = False)
    class: Llama4UnfoldConvolution
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Llama4UnfoldConvolution
  - name: __init__
    signature: (self, config)
    class: Llama4VisionRotaryEmbedding
  - name: forward
    signature: (self, hidden_states)
    class: Llama4VisionRotaryEmbedding
  - name: __init__
    signature: (self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Llama4VisionModel
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    return: torch.Tensor
    class: Llama4VisionModel
  - name: __init__
    signature: (self, config: Llama4Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Llama4ForConditionalGeneration
  - name: _has_vision_weights
    signature: (self, config)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Check if the model has vision components by examining the checkpoint.
  - name: _check_vision_weights_in_index
    signature: (self, index_file: str)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Check if the model.safetensors.index.json contains vision weights.
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: Llama4ForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Llama4ForConditionalGeneration
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, **kwargs: object)
    return: torch.Tensor
    class: Llama4ForConditionalGeneration
  - name: permute_qk_weight_for_rotary
    signature: (self, name: str, loaded_weight: torch.Tensor)
    return: Tuple[str, torch.Tensor]
    class: Llama4ForConditionalGeneration
  - name: permute
    signature: (w: torch.Tensor, n_heads: int)
    class: Llama4ForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    return: Set[str]
    class: Llama4ForConditionalGeneration
  - name: _should_skip_weight
    signature: (self, name: str)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Check if we should skip loading this weight.
  - name: _transform_weight_name
    signature: (self, name: str)
    return: str
    class: Llama4ForConditionalGeneration
    doc: Transform weight name by adding language_model prefix if needed.
  - name: _handle_scale_remapping
    signature: (self, name: str, params_dict: dict)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Handle scale parameter remapping. Returns True if handled.
  - name: _handle_stacked_params
    signature: (self, name: str, loaded_weight: torch.Tensor, stacked_params_mapping: list, params_dict: dict, loaded_params: set)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Handle stacked parameter loading. Returns True if handled.
  - name: _handle_expert_weights
    signature: (self, name: str, loaded_weight: torch.Tensor, expert_params_mapping: list, params_dict: dict, num_experts: int, loaded_params: set)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Handle expert weight loading for MoE (Mixture of Experts) layers.
  - name: _handle_other_expert_params
    signature: (self, name: str, loaded_weight: torch.Tensor, expert_params_mapping: list, params_dict: dict, loaded_params: set)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Handle expert parameters that are not gate_up_proj or down_proj weights.
  - name: _transform_expert_name
    signature: (self, name: str, is_weight: bool = False)
    return: Tuple[str, str, List[str]]
    class: Llama4ForConditionalGeneration
    doc: Transform expert parameter name and get shard information.
  - name: _handle_expert_scale_params
    signature: (self, name: str, loaded_weight: torch.Tensor, params_dict: dict, num_experts: int, loaded_params: set)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Handle quantization scale parameters for expert weights.
  - name: _handle_expert_weight_params
    signature: (self, name: str, loaded_weight: torch.Tensor, params_dict: dict, num_experts: int, loaded_params: set)
    return: bool
    class: Llama4ForConditionalGeneration
    doc: Handle actual weight tensors for expert layers (gate_up_proj and down_proj).
  - name: _handle_default_weight
    signature: (self, name: str, loaded_weight: torch.Tensor, params_dict: dict)
    class: Llama4ForConditionalGeneration
    doc: Handle default weight loading.
  - name: set_eagle3_layers_to_capture
    signature: (self, layer_ids: Optional[List[int]] = None)
    class: Llama4ForConditionalGeneration
  - name: get_embed_and_head
    signature: (self)
    class: Llama4ForConditionalGeneration
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: Llama4ForConditionalGeneration
  - name: get_embed
    signature: (self)
    class: Llama4ForConditionalGeneration
  - name: set_embed
    signature: (self, embed)
    class: Llama4ForConditionalGeneration

File: models/nemotron_nas.py
  - name: _ffn_mult_to_intermediate_size
    signature: (ffn_mult: float, n_embd: int)
    return: int
  - name: _find_multiple
    signature: (n: int, k: int)
    return: int
  - name: __init__
    signature: (self, config: LlamaConfig, layer_idx: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: DeciLMDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: DeciLMDecoderLayer
  - name: __init__
    signature: (self, *, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', layer_type: Type[DeciLMDecoderLayer] = DeciLMDecoderLayer)
    class: DeciModel
  - name: get_layer
    signature: (idx: int, prefix: str)
    class: DeciModel
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: DeciModel
  - name: forward
    signature: (self, input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor] = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, PPProxyTensors]
    class: DeciModel
  - name: __init__
    signature: (self, *, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: DeciLMForCausalLM
  - name: _init_model
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: DeciLMForCausalLM
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: DeciLMForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor] = None, get_embedding: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: LogitsProcessorOutput
    class: DeciLMForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    return: None
    class: DeciLMForCausalLM

File: models/olmo.py
  - name: __init__
    signature: (self, config: OlmoConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: OlmoAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: OlmoAttention
  - name: __init__
    signature: (self, config: OlmoConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: OlmoMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: OlmoMLP
  - name: __init__
    signature: (self, config: OlmoConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: OlmoDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]
    class: OlmoDecoderLayer
  - name: __init__
    signature: (self, config: OlmoConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: OlmoModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: OlmoModel
    doc: :param input_ids: A tensor of shape `(batch_size, seq_len)`.
  - name: __init__
    signature: (self, config: OlmoConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: OlmoForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: OlmoForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: OlmoForCausalLM

File: models/olmo2.py
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Olmo2Attention
  - name: _apply_qk_norm
    signature: (self, q: torch.Tensor, k: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Olmo2Attention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Olmo2Attention
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Olmo2MLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Olmo2MLP
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Olmo2DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Olmo2DecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Olmo2Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Olmo2Model
    doc: :param input_ids: A tensor of shape `(batch_size, seq_len)`.
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Olmo2ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Olmo2ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Olmo2ForCausalLM

File: models/olmoe.py
  - name: __init__
    signature: (self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, tp_size: Optional[int] = None, layer_id: int = 0, prefix: str = '')
    class: OlmoeMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: OlmoeMoE
  - name: __init__
    signature: (self, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 4096, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: OlmoeAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: OlmoeAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: OlmoeDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: torch.Tensor
    class: OlmoeDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: OlmoeModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: OlmoeModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: OlmoeForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: OlmoeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: OlmoeForCausalLM

File: models/persimmon.py
  - name: __init__
    signature: (self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig] = None)
    class: PersimmonMLP
  - name: forward
    signature: (self, hidden_states)
    return: torch.Tensor
    class: PersimmonMLP
  - name: __init__
    signature: (self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', layer_id: int = 0)
    class: PersimmonAttention
  - name: _split_heads
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: PersimmonAttention
  - name: _merge_heads
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: PersimmonAttention
  - name: forward
    signature: (self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: PersimmonAttention
  - name: __init__
    signature: (self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', idx: int = 0)
    class: PersimmonDecoderLayer
  - name: forward
    signature: (self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: PersimmonDecoderLayer
  - name: __init__
    signature: (self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: PersimmonModel
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: PersimmonModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: PersimmonModel
  - name: __init__
    signature: (self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: PersimmonForCausalLM
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: PersimmonForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor] = None)
    return: LogitsProcessorOutput
    class: PersimmonForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[tuple[str, torch.Tensor]])
    class: PersimmonForCausalLM

File: models/phi.py
  - name: __init__
    signature: (self, config: PhiConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', layer_id: int = 0)
    class: PhiAttention
  - name: forward
    signature: (self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: PhiAttention
  - name: __init__
    signature: (self, config: PhiConfig, quant_config: Optional[QuantizationConfig] = None)
    class: PhiMLP
  - name: forward
    signature: (self, hidden_states)
    class: PhiMLP
  - name: __init__
    signature: (self, config: PhiConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', idx: int = 0)
    class: PhiLayer
  - name: forward
    signature: (self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: PhiLayer
  - name: __init__
    signature: (self, config: PhiConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: PhiModel
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: PhiModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: PhiModel
  - name: __init__
    signature: (self, config: PhiConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: PhiForCausalLM
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: PhiForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor] = None)
    return: LogitsProcessorOutput
    class: PhiForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[tuple[str, torch.Tensor]])
    class: PhiForCausalLM

File: models/phi3_small.py
  - name: quick_gelu
    signature: (x)
  - name: gegelu
    signature: (input, limit: Optional[float] = None)
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Phi3SmallMLP
  - name: forward
    signature: (self, x)
    class: Phi3SmallMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Phi3SmallSelfAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
    class: Phi3SmallSelfAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Phi3SmallDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Phi3SmallDecoderLayer
  - name: __init__
    signature: (self, config: Phi3Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Phi3SmallModel
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: Phi3SmallModel
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
    return: Union[torch.Tensor]
    class: Phi3SmallModel
  - name: __init__
    signature: (self, config: Phi3Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Phi3SmallForCausalLM
  - name: get_input_embeddings
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: Phi3SmallForCausalLM
  - name: set_input_embeddings
    signature: (self, value)
    class: Phi3SmallForCausalLM
  - name: get_output_embeddings
    signature: (self)
    class: Phi3SmallForCausalLM
  - name: set_output_embeddings
    signature: (self, value)
    class: Phi3SmallForCausalLM
  - name: set_decoder
    signature: (self, decoder)
    class: Phi3SmallForCausalLM
  - name: get_decoder
    signature: (self)
    class: Phi3SmallForCausalLM
  - name: compute_logits
    signature: (self, input_ids: torch.LongTensor, hidden_states: torch.Tensor, sampling_metadata)
    return: Optional[torch.Tensor]
    class: Phi3SmallForCausalLM
  - name: forward
    signature: (self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor] = None, get_embedding: bool = False)
    return: LogitsProcessorOutput
    class: Phi3SmallForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Phi3SmallForCausalLM

File: models/phi4mm.py
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str = '', model_dir: str = '')
    return: None
    class: Phi4MMImageEncoder
  - name: get_img_features
    signature: (self, img_embeds: torch.FloatTensor, attention_mask = None)
    return: torch.FloatTensor
    class: Phi4MMImageEncoder
  - name: forward
    signature: (self, pixel_values: torch.FloatTensor, image_sizes: torch.Tensor, image_attention_mask: torch.Tensor)
    return: list[torch.FloatTensor]
    class: Phi4MMImageEncoder
    doc: process image and return vision embeddings.
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Phi4MMForCausalLM
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Phi4MMForCausalLM
  - name: get_audio_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Phi4MMForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, **kwargs: object)
    return: torch.Tensor
    class: Phi4MMForCausalLM
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: Phi4MMForCausalLM
  - name: should_apply_lora
    signature: (self, module_name: str)
    return: bool
    class: Phi4MMForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Phi4MMForCausalLM
  - name: _should_skip
    signature: (name: str)
    return: bool
    class: Phi4MMForCausalLM

File: models/phi4mm_audio.py
  - name: __init__
    signature: (self, d_model = 512, ext_pw_out_channel = 0, depthwise_seperable_out_channel = 256, depthwise_multiplier = 1, n_head = 4, d_ffn = 2048, ext_pw_kernel_size = 1, kernel_size = 3, dropout_rate = 0.1, causal = False, batch_norm = False, activation = 'relu', chunk_se = 0, chunk_size = 18, conv_activation = 'relu', conv_glu_type = 'sigmoid', bias_in_glu = True, linear_glu_in_convm = False, attention_inner_dim = -1, attention_glu_type = 'swish', activation_checkpointing = '', export = False, use_pt_scaled_dot_product_attention = False, attn_group_sizes: int = 1)
    class: ConformerEncoderLayer
  - name: forward
    signature: (self, x, pos_k, pos_v, mask, relative_attention_bias: Optional[Tensor] = None)
    class: ConformerEncoderLayer
    doc: ConformerEncoder forward.
  - name: __init__
    signature: (self, input_size, chunk_size, left_chunk, attention_dim = 256, attention_heads = 4, input_layer = 'nemo_conv', cnn_out = -1, cnn_layer_norm = False, time_reduction = 4, dropout_rate = 0.0, padding_idx = -1, relative_attention_bias_args = None, positional_dropout_rate = 0.0, nemo_conv_settings = None, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True] = 'none', attention_group_size = 1, encoder_embedding_config = None)
    class: TransformerEncoderBase
  - name: compute_lens_change
    signature: (self, feature_lens)
    class: TransformerEncoderBase
    doc: feature_lens: int
  - name: forward
    signature: (self)
    class: TransformerEncoderBase
    doc: Abstract forward method implementation.
  - name: _chunk_size_selection
    signature: (self, chunk_size = None, left_chunk = None)
    class: TransformerEncoderBase
    doc: If chunk size is a list, we will randomly select a chunk size.
  - name: _get_embed_class
    signature: (self, embed)
    class: TransformerEncoderBase
  - name: _forward_embeddings_core
    signature: (self, input_tensor, masks)
    class: TransformerEncoderBase
  - name: _position_embedding
    signature: (self, input_tensor)
    class: TransformerEncoderBase
  - name: _streaming_mask
    signature: (self, seq_len, batch_size, chunk_size, left_chunk)
    class: TransformerEncoderBase
  - name: forward_embeddings
    signature: (self, xs_pad, masks, chunk_size_nc = None, left_chunk_nc = None)
    class: TransformerEncoderBase
    doc: Forwarding the inputs through the top embedding layers
  - name: get_offset
    signature: (self)
    class: TransformerEncoderBase
    doc: Returns offset used when retaining inputs for decoding.
  - name: __init__
    signature: (self, input_size, chunk_size, left_chunk, num_lang = None, attention_dim = 256, attention_heads = 4, linear_units = 2048, num_blocks = 6, dropout_rate = 0.1, input_layer = 'nemo_conv', causal = True, batch_norm = False, cnn_out = -1, cnn_layer_norm = False, ext_pw_out_channel = 0, ext_pw_kernel_size = 1, depthwise_seperable_out_channel = 256, depthwise_multiplier = 1, chunk_se = 0, kernel_size = 3, activation = 'relu', conv_activation = 'relu', conv_glu_type = 'sigmoid', bias_in_glu = True, linear_glu_in_convm = False, attention_glu_type = 'swish', export = False, extra_layer_output_idx = -1, extra_multi_layer_output_idxs = [], activation_checkpointing = '', relative_attention_bias_args = None, time_reduction = 4, use_pt_scaled_dot_product_attention = False, nemo_conv_settings = None, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True] = 'none', replication_pad_for_subsample_embedding = False, attention_group_size = 1, encoder_embedding_config = None)
    class: ConformerEncoder
  - name: init_relative_attention_bias
    signature: (self, input_tensor)
    class: ConformerEncoder
  - name: calculate_hs_mask
    signature: (self, xs_pad, device, mask)
    class: ConformerEncoder
  - name: forward
    signature: (self, xs_pad, masks)
    class: ConformerEncoder
    doc: Conformer Forward function
  - name: __init__
    signature: (self, window_size: int = 8, num_queries: int = 1, num_blocks: int = 2, attention_dim: int = 512, attention_heads: int = 8, linear_units: int = 2048, dropout_rate: float = 0.0, normalize_before: bool = True)
    class: WindowQformer
  - name: forward
    signature: (self, audio_embed, mask, embed_len = None)
    class: WindowQformer
    doc: forward decoder
  - name: __init__
    signature: (self, config: PretrainedConfig, **kwargs)
    return: None
    class: AudioEmbedding
  - name: set_audio_embeds
    signature: (self, input_embeds: torch.FloatTensor)
    return: None
    class: AudioEmbedding
  - name: set_audio_embed_sizes
    signature: (self, audio_embed_sizes: torch.LongTensor)
    return: None
    class: AudioEmbedding
  - name: get_audio_features
    signature: (self, input_embeds: torch.FloatTensor, audio_attention_mask: torch.Tensor = None, audio_projection_mode: str = 'speech')
    return: torch.FloatTensor
    class: AudioEmbedding
    doc: arguments:
  - name: forward
    signature: (self, audio_features: torch.FloatTensor, audio_attention_mask: torch.Tensor = None, audio_projection_mode: str = 'speech')
    return: torch.FloatTensor
    class: AudioEmbedding
    doc: arguments:

File: models/phi4mm_utils.py
  - name: __init__
    signature: (self, input_size, output_size)
    class: BlockBase
  - name: get_activation
    signature: (name = 'relu')
    doc: Select an activation function by name
  - name: adaptive_enc_mask
    signature: (x_len, chunk_start_idx, left_window = 0, right_window = 0)
    doc: The function is very important for Transformer Transducer Streaming mode
  - name: __init__
    signature: (self)
    return: None
    class: Swish
  - name: forward
    signature: (self, x: Tensor)
    return: Tensor
    class: Swish
    doc: Apply Swish function
  - name: __init__
    signature: (self, dim: int = -1, act_name: str = 'sigmoid')
    return: None
    class: GLU
  - name: forward
    signature: (self, x: Tensor)
    return: Tensor
    class: GLU
    doc: GLU forward
  - name: __init__
    signature: (self, input_dim, output_dim, kernel_size, glu_type = 'sigmoid', bias_in_glu = True, causal = False)
    class: GLUPointWiseConv
  - name: forward
    signature: (self, x)
    class: GLUPointWiseConv
    doc: Args:
  - name: __init__
    signature: (self, input_dim, depthwise_seperable_out_channel, kernel_size, depthwise_multiplier, padding = 0)
    class: DepthWiseSeperableConv1d
  - name: forward
    signature: (self, x)
    class: DepthWiseSeperableConv1d
    doc: Args:
  - name: __init__
    signature: (self, input_dim, ext_pw_out_channel, depthwise_seperable_out_channel, ext_pw_kernel_size, kernel_size, depthwise_multiplier, dropout_rate, causal = False, batch_norm = False, chunk_se = 0, chunk_size = 18, activation = 'relu', glu_type = 'sigmoid', bias_in_glu = True, linear_glu_in_convm = False, export = False)
    class: ConvModule
  - name: _add_ext_pw_layer
    signature: (self)
    class: ConvModule
    doc: This function is an extension of __init__ function
  - name: forward
    signature: (self, x)
    class: ConvModule
    doc: ConvModule Forward.
  - name: __init__
    signature: (self, input_dim, output_dim, glu_type = 'sigmoid', bias_in_glu = True)
    class: GLULinear
  - name: forward
    signature: (self, x)
    class: GLULinear
    doc: GLULinear forward
  - name: __init__
    signature: (self, d_model, d_inner, dropout_rate, activation = 'sigmoid', bias_in_glu = True)
    class: FeedForward
  - name: forward
    signature: (self, x)
    class: FeedForward
    doc: FeedForward forward function.
  - name: _pre_hook
    signature: (state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
    doc: Perform pre-hook in load_state_dict for backward compatibility.
  - name: __init__
    signature: (self, num_heads, num_buckets = -1, max_distance = 1000, symmetric = False)
    class: T5RelativeAttentionLogitBias
  - name: forward
    signature: (self, x)
    class: T5RelativeAttentionLogitBias
  - name: _bucket_relative_position
    signature: (self, relative_position)
    class: T5RelativeAttentionLogitBias
  - name: __init__
    signature: (self, d_model, dropout_rate, max_len = 5000)
    class: AbsolutePositionalEncoding
    doc: Construct an PositionalEncoding object.
  - name: extend_pe
    signature: (self, x)
    class: AbsolutePositionalEncoding
    doc: Reset the positional encodings.
  - name: forward
    signature: (self, x: torch.Tensor)
    class: AbsolutePositionalEncoding
    doc: Add positional encoding.
  - name: __init__
    signature: (self, input_size)
    class: MeanVarianceNormLayer
  - name: forward
    signature: (self, input_: Tensor)
    return: Tensor
    class: MeanVarianceNormLayer
    doc: MeanVarianceNormLayer Forward
  - name: __init__
    signature: (self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: Union[str, int] = 0, dilation: int = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device = None, dtype = None)
    return: None
    class: CausalConv1D
  - name: update_cache
    signature: (self, x, cache = None)
    class: CausalConv1D
  - name: forward
    signature: (self, x, cache = None)
    class: CausalConv1D
  - name: __init__
    signature: (self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: Union[str, int] = 0, dilation: int = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device = None, dtype = None)
    return: None
    class: CausalConv2D
  - name: forward
    signature: (self, x)
    class: CausalConv2D
  - name: __init__
    signature: (self, feat_in, feat_out, subsampling_factor = 4, subsampling = 'dw_striding', conv_channels = 256, subsampling_conv_chunking_factor = 1, activation = nn.ReLU(), is_causal = False)
    class: NemoConvSubsampling
  - name: get_sampling_frames
    signature: (self)
    class: NemoConvSubsampling
  - name: get_streaming_cache_size
    signature: (self)
    class: NemoConvSubsampling
  - name: forward
    signature: (self, x, mask)
    class: NemoConvSubsampling
    doc: Forward method for NeMo subsampling.
  - name: reset_parameters
    signature: (self)
    class: NemoConvSubsampling
  - name: conv_split_by_batch
    signature: (self, x)
    class: NemoConvSubsampling
    doc: Tries to split input by batch, run conv and concat results
  - name: conv_split_by_channel
    signature: (self, x)
    class: NemoConvSubsampling
    doc: For dw convs, tries to split input by time, run conv and concat
  - name: channel_chunked_conv
    signature: (self, conv, chunk_size, x)
    class: NemoConvSubsampling
    doc: Performs channel chunked convolution
  - name: change_subsampling_conv_chunking_factor
    signature: (self, subsampling_conv_chunking_factor: int)
    class: NemoConvSubsampling
  - name: calc_length
    signature: (lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num = 1)
    doc: Calculates the output length of a Tensor passed through a convolution or
  - name: __init__
    signature: (self)
    class: AttModule
  - name: set_export
    signature: (self, mode = True)
    class: AttModule
    doc: set the export mode
  - name: forward
    signature: (self, x: Tensor, memory: Optional[Tensor] = None, pos_emb: Optional[Tensor] = None, att_mask: Optional[Tensor] = None)
    return: tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]
    class: AttModule
    doc: AttModule forward
  - name: memory_dims
    signature: (self, max_len = False)
    class: AttBlock
    doc: memory dimensions
  - name: masked_softmax
    signature: (scores, mask: Optional[Tensor])
  - name: __init__
    signature: (self, n_head, n_feat, dropout_rate, attention_inner_dim = -1, glu_type = 'swish', bias_in_glu = True, use_pt_scaled_dot_product_attention = False, n_value = -1, group_size: int = 1)
    class: MultiHeadedAttention
  - name: forward
    signature: (self, query: Tensor, key: Tensor, value: Tensor, pos_k: Tensor, pos_v: Tensor, mask: Optional[Tensor], relative_attention_bias: Optional[Tensor] = None)
    class: MultiHeadedAttention
    doc: Compute 'Scaled Dot Product Attention'.
  - name: forward
    signature: (self, *args)
    class: MultiSequential
    doc: Forward method implementation.
  - name: get_offset
    signature: (input_layer: str, time_reduction: int)
    doc: Get an offset. We will use the offset for determining #frames of a
  - name: unfold_tensor
    signature: (xs_pad, max_seq_len)
    doc: For a given tensor with shape of (N, T, D), if sequence length T is

File: models/phimoe.py
  - name: __init__
    signature: (self, vocab_size = 32000, hidden_size = 4096, intermediate_size = 14336, num_hidden_layers = 32, num_attention_heads = 32, num_key_value_heads = 8, head_dim = None, hidden_act = 'silu', max_position_embeddings = 4096 * 32, initializer_range = 0.02, rms_norm_eps = 1e-05, use_cache = True, pad_token_id = None, bos_token_id = 1, eos_token_id = 2, tie_word_embeddings = False, rope_theta = 1000000.0, sliding_window = None, attention_dropout = 0.0, num_experts_per_tok = 2, num_local_experts = 16, output_router_logits = False, router_aux_loss_coef = 0.001, router_jitter_noise = 0.0, attention_bias = False, lm_head_bias = False, **kwargs)
    class: PhiMoEConfig
  - name: sparsemixer
    signature: (scores, jitter_eps = 0.01)
  - name: phimoe_routing_function
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool)
  - name: __init__
    signature: (self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: PhiMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch] = None)
    return: torch.Tensor
    class: PhiMoE
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int] = None, max_position: int = 4096 * 32, rope_theta: float = 10000, layer_id: int = 0, attention_bias: bool = False, quant_config: Optional[QuantizationConfig] = None, rope_scaling: Optional[dict] = None, prefix: str = '')
    return: None
    class: PhiMoEAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: PhiMoEAttention
  - name: __init__
    signature: (self, config: PhiMoEConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: PhiMoEDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: PhiMoEDecoderLayer
  - name: __init__
    signature: (self, config: PhiMoEConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: PhiMoEModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor]
    class: PhiMoEModel
  - name: __init__
    signature: (self, config: PhiMoEConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: PhiMoEForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor] = None, get_embedding: bool = False)
    return: LogitsProcessorOutput
    class: PhiMoEForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: PhiMoEForCausalLM

File: models/pixtral.py
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, *, prefix: str = '')
    return: None
    class: PixtralHFMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: PixtralHFMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, *, prefix: str = '')
    return: None
    class: PixtralHFTransformerBlock
  - name: forward
    signature: (self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]])
    return: torch.Tensor
    class: PixtralHFTransformerBlock
  - name: __init__
    signature: (self, config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig] = None, *, num_hidden_layers_override: Optional[int] = None, prefix: str = '')
    return: None
    class: PixtralHFTransformer
  - name: forward
    signature: (self, x: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], return_all_hidden_states: bool = False)
    return: Union[torch.Tensor, List[torch.Tensor]]
    class: PixtralHFTransformer
    doc: Forward pass through transformer layers.
  - name: resolve_visual_encoder_outputs
    signature: (outputs: Union[torch.Tensor, List[torch.Tensor]], feature_sample_layers: Optional[List[int]], post_norm: Optional[nn.Module], num_hidden_layers: int)
    return: torch.Tensor
    doc: Resolve outputs from visual encoder based on feature_sample_layers.
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: PixtralHFVisionModel
  - name: __init__
    signature: (self, config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig] = None, *, num_hidden_layers_override: Optional[int] = None, prefix: str = '')
    return: None
    class: PixtralHFVisionModel
  - name: dtype
    signature: (self)
    class: PixtralHFVisionModel
  - name: device
    signature: (self)
    class: PixtralHFVisionModel
  - name: forward
    signature: (self, pixel_values: torch.Tensor, image_sizes: list[tuple[int, int]], output_hidden_states: bool = False, feature_sample_layers: Optional[list[int]] = None)
    return: Union[torch.Tensor, tuple]
    class: PixtralHFVisionModel
    doc: Args:
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    return: Set[str]
    class: PixtralHFVisionModel
    doc: Load weights from a HuggingFace checkpoint with proper parameter mapping.

File: models/qwen.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str = 'silu', quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: QWenMLP
  - name: forward
    signature: (self, x)
    class: QWenMLP
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, max_position_embeddings: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: QWenAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: QWenAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: QWenBlock
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: QWenBlock
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: QWenModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: QWenModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: QWenLMHeadModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    class: QWenLMHeadModel
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int])
    class: QWenLMHeadModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: QWenLMHeadModel

File: models/qwen2.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2MLP
  - name: forward
    signature: (self, x)
    class: Qwen2MLP
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int] = None, layer_id: int = 0, rope_theta: float = 1000000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 32768, quant_config: Optional[QuantizationConfig] = None, dual_chunk_attention_config: Optional[dict[str, Any]] = None, prefix: str = '')
    return: None
    class: Qwen2Attention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Qwen2Attention
  - name: __init__
    signature: (self, config: Qwen2Config, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Qwen2DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Qwen2DecoderLayer
  - name: __init__
    signature: (self, config: Qwen2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', decoder_layer_type: type[nn.Module] = Qwen2DecoderLayer, alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Qwen2Model
  - name: get_input_embedding
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: Qwen2Model
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Qwen2Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, PPProxyTensors]
    class: Qwen2Model
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: Qwen2Model
  - name: __init__
    signature: (self, config: Qwen2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2ForCausalLM
  - name: get_input_embedding
    signature: (self, input_ids: torch.Tensor)
    return: torch.Tensor
    class: Qwen2ForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Qwen2ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: Qwen2ForCausalLM
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor = None)
    class: Qwen2ForCausalLM
  - name: start_layer
    signature: (self)
    class: Qwen2ForCausalLM
  - name: end_layer
    signature: (self)
    class: Qwen2ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen2ForCausalLM
  - name: get_embed_and_head
    signature: (self)
    class: Qwen2ForCausalLM
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: Qwen2ForCausalLM
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: Qwen2ForCausalLM
  - name: set_eagle3_layers_to_capture
    signature: (self, layer_ids: Optional[List[int]] = None)
    class: Qwen2ForCausalLM

File: models/qwen2_5_vl.py
  - name: __init__
    signature: (self, in_features: int, hidden_features: int = None, bias: bool = True, hidden_act = 'silu', quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Qwen2_5_VLMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Qwen2_5_VLMLP
  - name: __init__
    signature: (self, dim: int, intermediate_dim: int, num_heads: int, hidden_act = 'silu', norm_layer: Type[nn.Module] = None, attn_implementation: Optional[str] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', num_dummy_heads: int = 0)
    return: None
    class: Qwen2_5_VisionBlock
  - name: forward
    signature: (self, x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor)
    return: torch.Tensor
    class: Qwen2_5_VisionBlock
  - name: __init__
    signature: (self, dim: int, context_dim: int, spatial_merge_size: int = 2, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2_5_VisionPatchMerger
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Qwen2_5_VisionPatchMerger
  - name: __init__
    signature: (self, vision_config: Qwen2_5_VLVisionConfig, norm_eps: float = 1e-06, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2_5_VisionTransformer
  - name: get_window_index
    signature: (self, grid_thw)
    class: Qwen2_5_VisionTransformer
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: Qwen2_5_VisionTransformer
  - name: device
    signature: (self)
    return: torch.device
    class: Qwen2_5_VisionTransformer
  - name: rot_pos_emb
    signature: (self, grid_thw: torch.Tensor)
    return: torch.Tensor
    class: Qwen2_5_VisionTransformer
  - name: forward
    signature: (self, x: torch.Tensor, grid_thw: torch.Tensor)
    return: torch.Tensor
    class: Qwen2_5_VisionTransformer
  - name: __init__
    signature: (self, config: Qwen2_5_VLConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2_5_VLForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: Qwen2_5_VLForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Qwen2_5_VLForConditionalGeneration
  - name: get_video_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Qwen2_5_VLForConditionalGeneration
  - name: get_input_embeddings
    signature: (self)
    class: Qwen2_5_VLForConditionalGeneration
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool = False)
    class: Qwen2_5_VLForConditionalGeneration
    doc: Run forward pass for Qwen2_5-VL.
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen2_5_VLForConditionalGeneration

File: models/qwen2_audio.py
  - name: __init__
    signature: (self, config: Qwen2AudioConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2AudioForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: Qwen2AudioForConditionalGeneration
  - name: get_audio_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Qwen2AudioForConditionalGeneration
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, **kwargs: Any)
    return: torch.Tensor
    class: Qwen2AudioForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen2AudioForConditionalGeneration

File: models/qwen2_classification.py
  (no function definitions found)
File: models/qwen2_eagle.py
  - name: __init__
    signature: (self, config: Qwen2Config, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2DecoderLayer
  - name: __init__
    signature: (self, config: Qwen2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2Model
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: Qwen2Model
  - name: __init__
    signature: (self, config: Qwen2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2ForCausalLMEagle
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen2ForCausalLMEagle

File: models/qwen2_moe.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, reduce_results: bool = True, prefix: str = '')
    return: None
    class: Qwen2MoeMLP
  - name: forward
    signature: (self, x, use_reduce_scatter: bool = False)
    class: Qwen2MoeMLP
  - name: __init__
    signature: (self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Qwen2MoeSparseMoeBlock
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch] = None, use_reduce_scatter: bool = False)
    return: torch.Tensor
    class: Qwen2MoeSparseMoeBlock
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, qkv_bias: int = True, quant_config: Optional[QuantizationConfig] = None, dual_chunk_attention_config: Optional[dict[str, Any]] = None, prefix: str = '')
    return: None
    class: Qwen2MoeAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Qwen2MoeAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Qwen2MoeDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Qwen2MoeDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', decoder_layer_type: type[nn.Module] = Qwen2MoeDecoderLayer, alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Qwen2MoeModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[torch.Tensor, PPProxyTensors]
    class: Qwen2MoeModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2MoeForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: Qwen2MoeForCausalLM
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor = None)
    class: Qwen2MoeForCausalLM
  - name: start_layer
    signature: (self)
    class: Qwen2MoeForCausalLM
  - name: end_layer
    signature: (self)
    class: Qwen2MoeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen2MoeForCausalLM
  - name: get_model_config_for_expert_location
    signature: (cls, config)
    class: Qwen2MoeForCausalLM
  - name: set_eagle3_layers_to_capture
    signature: (self, layer_ids: Optional[List[int]] = None)
    class: Qwen2MoeForCausalLM

File: models/qwen2_rm.py
  - name: __init__
    signature: (self, config: Qwen2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2ForRewardModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = True)
    return: EmbeddingPoolerOutput
    class: Qwen2ForRewardModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen2ForRewardModel

File: models/qwen2_vl.py
  - name: __init__
    signature: (self, in_features: int, hidden_features: int = None, act_layer: Type[nn.Module] = QuickGELU, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Qwen2VisionMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Qwen2VisionMLP
  - name: __init__
    signature: (self, dim: int, num_heads: int, mlp_ratio: float, act_layer: Type[nn.Module] = QuickGELU, norm_layer: Type[nn.Module] = None, attn_implementation: Optional[str] = 'sdpa', quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2VisionBlock
  - name: forward
    signature: (self, x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor)
    return: torch.Tensor
    class: Qwen2VisionBlock
  - name: __init__
    signature: (self, patch_size: int = 14, temporal_patch_size: int = 2, in_chans: int = 3, embed_dim: int = 1152)
    return: None
    class: Qwen2VisionPatchEmbed
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Qwen2VisionPatchEmbed
  - name: __init__
    signature: (self, d_model: int, context_dim: int, norm_layer: Type[nn.Module] = None, spatial_merge_size: int = 2, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2VisionPatchMerger
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Qwen2VisionPatchMerger
  - name: __init__
    signature: (self, dim: int, theta: float = 10000.0)
    return: None
    class: Qwen2VisionRotaryEmbedding
  - name: update_freqs_cache
    signature: (self, seqlen: int)
    return: None
    class: Qwen2VisionRotaryEmbedding
  - name: forward
    signature: (self, seqlen: int)
    return: torch.Tensor
    class: Qwen2VisionRotaryEmbedding
  - name: __init__
    signature: (self, vision_config: Qwen2VLVisionConfig, norm_eps: float = 1e-06, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2VisionTransformer
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: Qwen2VisionTransformer
  - name: device
    signature: (self)
    return: torch.device
    class: Qwen2VisionTransformer
  - name: rot_pos_emb
    signature: (self, grid_thw: torch.Tensor)
    return: torch.Tensor
    class: Qwen2VisionTransformer
  - name: forward
    signature: (self, x: torch.Tensor, grid_thw: torch.Tensor)
    return: torch.Tensor
    class: Qwen2VisionTransformer
  - name: __init__
    signature: (self, config: Qwen2VLConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen2VLForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: Qwen2VLForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Qwen2VLForConditionalGeneration
  - name: get_video_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Qwen2VLForConditionalGeneration
  - name: _process_video_input
    signature: (self, video_input: Qwen2VLVideoInputs)
    return: torch.Tensor
    class: Qwen2VLForConditionalGeneration
  - name: get_input_embeddings
    signature: (self)
    class: Qwen2VLForConditionalGeneration
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool = False)
    class: Qwen2VLForConditionalGeneration
    doc: Run forward pass for Qwen2-VL.
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen2VLForConditionalGeneration

File: models/qwen3.py
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 1000000, rope_scaling: Optional[Dict[str, Any]] = None, head_dim: Optional[int] = None, max_position_embeddings: int = 32768, quant_config: Optional[QuantizationConfig] = None, rms_norm_eps: float = None, attention_bias: bool = False, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Qwen3Attention
  - name: _apply_qk_norm
    signature: (self, q: torch.Tensor, k: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Qwen3Attention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Qwen3Attention
  - name: __init__
    signature: (self, config: Qwen3Config, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Qwen3DecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Qwen3DecoderLayer
  - name: __init__
    signature: (self, config: Qwen3Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen3Model
  - name: __init__
    signature: (self, config: Qwen3Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen3ForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Qwen3ForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: Qwen3ForCausalLM
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor = None)
    class: Qwen3ForCausalLM
  - name: start_layer
    signature: (self)
    class: Qwen3ForCausalLM
  - name: end_layer
    signature: (self)
    class: Qwen3ForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen3ForCausalLM
  - name: get_embed_and_head
    signature: (self)
    class: Qwen3ForCausalLM
  - name: set_embed_and_head
    signature: (self, embed, head)
    class: Qwen3ForCausalLM
  - name: load_kv_cache_scales
    signature: (self, quantization_param_path: str)
    return: None
    class: Qwen3ForCausalLM
  - name: set_eagle3_layers_to_capture
    signature: (self, layer_ids: Optional[List[int]] = None)
    class: Qwen3ForCausalLM

File: models/qwen3_classification.py
  - name: __init__
    signature: (self, config: Qwen2Config, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen3ForSequenceClassification
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor] = None, get_embedding: bool = True)
    return: EmbeddingPoolerOutput
    class: Qwen3ForSequenceClassification
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen3ForSequenceClassification

File: models/qwen3_moe.py
  - name: __init__
    signature: (self, layer_id: int, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Qwen3MoeSparseMoeBlock
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch] = None, use_reduce_scatter: bool = False)
    return: torch.Tensor
    class: Qwen3MoeSparseMoeBlock
  - name: get_moe_weights
    signature: (self)
    class: Qwen3MoeSparseMoeBlock
  - name: forward_normal
    signature: (self, hidden_states: torch.Tensor, use_reduce_scatter: bool = False)
    return: torch.Tensor
    class: Qwen3MoeSparseMoeBlock
  - name: forward_deepep
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Qwen3MoeSparseMoeBlock
  - name: op_gate
    signature: (self, state)
    class: Qwen3MoeSparseMoeBlock
  - name: op_select_experts
    signature: (self, state)
    class: Qwen3MoeSparseMoeBlock
  - name: op_dispatch_a
    signature: (self, state)
    class: Qwen3MoeSparseMoeBlock
  - name: op_dispatch_b
    signature: (self, state)
    class: Qwen3MoeSparseMoeBlock
  - name: op_experts
    signature: (self, state)
    class: Qwen3MoeSparseMoeBlock
  - name: op_combine_a
    signature: (self, state)
    class: Qwen3MoeSparseMoeBlock
  - name: op_combine_b
    signature: (self, state)
    class: Qwen3MoeSparseMoeBlock
  - name: op_output
    signature: (self, state)
    class: Qwen3MoeSparseMoeBlock
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, head_dim: Optional[int] = None, rms_norm_eps: float = 1e-06, attention_bias: bool = False, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', dual_chunk_attention_config: Optional[dict[str, Any]] = None, alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Qwen3MoeAttention
  - name: _apply_qk_norm
    signature: (self, q: torch.Tensor, k: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Qwen3MoeAttention
  - name: op_prepare
    signature: (self, state)
    class: Qwen3MoeAttention
  - name: op_core
    signature: (self, state)
    class: Qwen3MoeAttention
  - name: forward_prepare
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    class: Qwen3MoeAttention
  - name: forward_core
    signature: (self, intermediate_state)
    class: Qwen3MoeAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Qwen3MoeAttention
  - name: __init__
    signature: (self, config: Qwen3MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', alt_stream: Optional[torch.cuda.Stream] = None)
    return: None
    class: Qwen3MoeDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Qwen3MoeDecoderLayer
  - name: op_comm_prepare_attn
    signature: (self, state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], tbo_subbatch_index: Optional[int] = None)
    class: Qwen3MoeDecoderLayer
  - name: op_comm_prepare_mlp
    signature: (self, state)
    class: Qwen3MoeDecoderLayer
  - name: op_mlp
    signature: (self, state)
    class: Qwen3MoeDecoderLayer
  - name: op_comm_postprocess_layer
    signature: (self, state)
    class: Qwen3MoeDecoderLayer
  - name: __init__
    signature: (self, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen3MoeModel
  - name: __init__
    signature: (self, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Qwen3MoeForCausalLM
  - name: get_input_embeddings
    signature: (self)
    return: nn.Embedding
    class: Qwen3MoeForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: torch.Tensor
    class: Qwen3MoeForCausalLM
  - name: forward_split_prefill
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor = None)
    class: Qwen3MoeForCausalLM
  - name: start_layer
    signature: (self)
    class: Qwen3MoeForCausalLM
  - name: end_layer
    signature: (self)
    class: Qwen3MoeForCausalLM
  - name: get_embed_and_head
    signature: (self)
    class: Qwen3MoeForCausalLM
  - name: set_eagle3_layers_to_capture
    signature: (self, layer_ids: Optional[List[int]] = None)
    class: Qwen3MoeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Qwen3MoeForCausalLM
  - name: get_model_config_for_expert_location
    signature: (cls, config)
    class: Qwen3MoeForCausalLM

File: models/registry.py
  - name: get_supported_archs
    signature: (self)
    return: AbstractSet[str]
    class: _ModelRegistry
  - name: _raise_for_unsupported
    signature: (self, architectures: List[str])
    class: _ModelRegistry
  - name: _try_load_model_cls
    signature: (self, model_arch: str)
    return: Optional[Type[nn.Module]]
    class: _ModelRegistry
  - name: _normalize_archs
    signature: (self, architectures: Union[str, List[str]])
    return: List[str]
    class: _ModelRegistry
  - name: resolve_model_cls
    signature: (self, architectures: Union[str, List[str]])
    return: Tuple[Type[nn.Module], str]
    class: _ModelRegistry
  - name: import_model_classes
    signature: ()

File: models/roberta.py
  - name: __init__
    signature: (self, config: RobertaConfig)
    class: RobertaClassificationHead
  - name: forward
    signature: (self, features, **kwargs)
    class: RobertaClassificationHead
  - name: __init__
    signature: (self, config: RobertaConfig)
    class: RobertaEmbedding
  - name: forward
    signature: (self, input_ids: torch.Tensor, seq_lens: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: RobertaEmbedding
  - name: __init__
    signature: (self, *, config: RobertaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', add_pooling_layer: bool = False)
    class: XLMRobertaBaseModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False)
    return: torch.Tensor
    class: XLMRobertaBaseModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: XLMRobertaBaseModel
  - name: create_position_ids_from_input_ids
    signature: (input_ids, padding_idx, past_key_values_length = 0)
  - name: __init__
    signature: (self, *, config: RobertaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: XLMRobertaModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False)
    return: torch.Tensor
    class: XLMRobertaModel
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: XLMRobertaModel
  - name: __init__
    signature: (self, *, config: RobertaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: XLMRobertaForSequenceClassification
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = True)
    return: torch.Tensor
    class: XLMRobertaForSequenceClassification
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: XLMRobertaForSequenceClassification
  - name: weight_filter
    signature: ()
    class: XLMRobertaForSequenceClassification

File: models/siglip.py
  - name: __init__
    signature: (self, config: SiglipVisionConfig)
    class: SiglipVisionEmbeddings
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    return: torch.Tensor
    class: SiglipVisionEmbeddings
  - name: __init__
    signature: (self, config, act_layer: Type[nn.Module] = QuickGELU, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: SiglipMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: SiglipMLP
  - name: __init__
    signature: (self, config: SiglipVisionConfig, act_layer: Type[nn.Module] = QuickGELU, norm_layer: Type[nn.Module] = None, attn_implementation: Optional[str] = 'sdpa', quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: SiglipEncoderLayer
  - name: forward
    signature: (self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor)
    return: torch.Tensor
    class: SiglipEncoderLayer
  - name: __init__
    signature: (self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: SiglipEncoder
  - name: forward
    signature: (self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor = None, causal_attention_mask: torch.Tensor = None, return_all_hidden_states: bool = False)
    return: Union[torch.Tensor, list[torch.Tensor]]
    class: SiglipEncoder
  - name: __init__
    signature: (self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: SiglipVisionTransformer
  - name: device
    signature: (self)
    return: torch.device
    class: SiglipVisionTransformer
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    return: torch.Tensor
    class: SiglipVisionTransformer
  - name: __init__
    signature: (self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: SiglipVisionModel
  - name: device
    signature: (self)
    return: torch.device
    class: SiglipVisionModel
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    class: SiglipVisionModel

File: models/stablelm.py
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: StablelmMLP
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: StablelmMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: StablelmAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: StablelmAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: StablelmDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: StablelmDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: StableLMEpochModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: StableLMEpochModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: StableLmForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: StableLmForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: StableLmForCausalLM

File: models/step3_vl.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Step3TextMLP
  - name: forward
    signature: (self, x)
    class: Step3TextMLP
  - name: __init__
    signature: (self, layer_id: int, config: Step3TextConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: Step3TextMoEMLP
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Step3TextMoEMLP
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, share_q_dim: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, rms_norm_eps = None, prefix: str = '')
    return: None
    class: Step3TextAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: Step3TextAttention
  - name: __init__
    signature: (self, config: Step3TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Step3TextDecoderLayer
  - name: moe_mlp_forward
    signature: (self, hidden_states)
    class: Step3TextDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Step3TextDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Step3TextModel
  - name: get_input_embeddings
    signature: (self)
    class: Step3TextModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Step3TextModel
  - name: get_abs_pos
    signature: (abs_pos, tgt_size)
  - name: __init__
    signature: (self, dim: int, intermediate_size: int, bias: bool = True, hidden_act = 'quick_gelu', quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Step3VisionMLP
  - name: forward
    signature: (self, hidden_states)
    return: torch.Tensor
    class: Step3VisionMLP
  - name: __init__
    signature: (self, dim: int, num_heads: int = 16, qkv_backend = 'fa3', quant_config = None, prefix: str = '')
    return: None
    class: Step3VisionAttention
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: Step3VisionAttention
  - name: __init__
    signature: (self, config: Step3VisionEncoderConfig)
    class: Step3VisionEmbeddings
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    return: torch.Tensor
    class: Step3VisionEmbeddings
  - name: __init__
    signature: (self, config, attn_implementation: str = 'sdpa')
    return: None
    class: Step3VisionEncoderLayer
  - name: forward
    signature: (self, hidden_states)
    return: torch.Tensor
    class: Step3VisionEncoderLayer
  - name: __init__
    signature: (self, config: Step3VisionEncoderConfig)
    class: Step3VisionTransformer
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: Step3VisionTransformer
  - name: forward
    signature: (self, pixel_values: torch.Tensor)
    class: Step3VisionTransformer
  - name: __init__
    signature: (self, config: Step3VisionEncoderConfig)
    class: Step3VisionEncoder
  - name: forward
    signature: (self, inputs_embeds)
    return: torch.Tensor
    class: Step3VisionEncoder
  - name: __init__
    signature: (self, config: Step3VLConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: Step3VLForConditionalGeneration
  - name: _get_vision_model_output
    signature: (self, input_tensor: torch.Tensor)
    return: torch.Tensor
    class: Step3VLForConditionalGeneration
  - name: _flatten_embeddings
    signature: (self, embeddings)
    return: torch.Tensor
    class: Step3VLForConditionalGeneration
  - name: _process_image_features
    signature: (self, image_features: torch.Tensor)
    return: torch.Tensor
    class: Step3VLForConditionalGeneration
  - name: get_image_feature
    signature: (self, items: List[MultimodalDataItem])
    return: torch.Tensor
    class: Step3VLForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    class: Step3VLForConditionalGeneration
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: Step3VLForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: Step3VLForConditionalGeneration
  - name: match_expert_and_shard_ids
    signature: (name_path: str, weight_path: str)
    return: bool
    class: Step3VLForConditionalGeneration
  - name: get_model_config_for_expert_location
    signature: (cls, config: Step3VLConfig)
    class: Step3VLForConditionalGeneration

File: models/torch_native_llama.py
  - name: gate_up_proj_weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: int)
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaMLP
  - name: forward
    signature: (self, x)
    class: LlamaMLP
  - name: qkv_proj_weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: str)
  - name: __init__
    signature: (self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, rope_is_neox_style: bool = True, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: LlamaAttention
  - name: __init__
    signature: (self, config: LlamaConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: LlamaDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: LlamaDecoderLayer
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None)
    return: None
    class: LlamaModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: LlamaModel
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None)
    return: None
    class: TorchNativeLlamaForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: LogitsProcessorOutput
    class: TorchNativeLlamaForCausalLM
  - name: get_module_name_from_weight_name
    signature: (self, name)
    class: TorchNativeLlamaForCausalLM
  - name: get_num_params
    signature: (self)
    class: TorchNativeLlamaForCausalLM
  - name: load_weights_to_module
    signature: (self, fqn: str, weights: Iterable[Tuple[str, torch.Tensor]])
    class: TorchNativeLlamaForCausalLM
    doc: Load weights onto submodule pointed by path `fqn`.
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: TorchNativeLlamaForCausalLM
    doc: Load weights onto the full model.

File: models/transformers.py
  - name: maybe_prefix
    signature: (prefix: str, name: str)
    return: str
    doc: Add a prefix to a name if the prefix is non-empty.
  - name: sglang_flash_attention_forward
    signature: (module: torch.nn.Module, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attention_mask: torch.Tensor, forward_batch: ForwardBatch, scaling: float = None, attention_instances: list[RadixAttention] = None, **kwargs)
  - name: forward
    signature: (self, input: torch.Tensor)
    return: torch.Tensor
    class: HFColumnParallelLinear
  - name: forward
    signature: (self, input: torch.Tensor)
    return: torch.Tensor
    class: HFRowParallelLinear
  - name: replace_linear_class
    signature: (linear: nn.Linear, style: Literal['colwise', 'rowwise'], quant_config: QuantizationConfig)
    return: Union[ColumnParallelLinear, RowParallelLinear]
    doc: Replace nn.Linear with one of vLLM's tensor parallel linear classes.
  - name: parent_cls
    signature: (self)
    return: type
    class: HFCompatibleLinear
  - name: forward
    signature: (self, input: torch.Tensor)
    return: torch.Tensor
    class: HFCompatibleLinear
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: TransformersForCausalLM
  - name: log_replacement
    signature: (self, name: str, old_module: nn.Module, new_module: nn.Module)
    class: TransformersForCausalLM
  - name: tensor_parallel
    signature: (self, tp_size: int)
    class: TransformersForCausalLM
    doc: Apply the model's tensor parallelization plan.
  - name: _tensor_parallel
    signature: (module: nn.Module, prefix: str = '')
    class: TransformersForCausalLM
  - name: replace_vocab_embed_class
    signature: (self, module: nn.Module)
    class: TransformersForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None, get_embedding: bool = False)
    return: LogitsProcessorOutput
    class: TransformersForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: TransformersForCausalLM

File: models/vila.py
  - name: __init__
    signature: (self, text_config: Optional[Dict[str, Any]] = None, vision_config: Optional[Dict[str, Any]] = None, *, hidden_size: int = 1536, image_token_id: int = 151649, mm_hidden_size: int = 1152, mm_projector_type: str = 'mlp_downsample_3x3_fix', mm_vision_select_feature: str = 'cls_patch', mm_vision_select_layer: int = -2, video_token_id: int = 151650, **kwargs)
    class: VILAConfig
  - name: forward
    signature: (self, x: Tensor)
    return: Tensor
    class: DownSample3x3BlockFix
    doc: Args:
  - name: __init__
    signature: (self, config: VILAConfig, *args, **kwargs)
    class: MultimodalProjector
  - name: device
    signature: (self)
    return: torch.device
    class: MultimodalProjector
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: MultimodalProjector
  - name: forward
    signature: (self, x: Tensor)
    return: Tensor
    class: MultimodalProjector
    doc: Args:
  - name: __init__
    signature: (self, config: VILAConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: VILAForConditionalGeneration
  - name: dtype
    signature: (self)
    return: torch.dtype
    class: VILAForConditionalGeneration
  - name: forward
    signature: (self, input_ids: Tensor, positions: Tensor, forward_batch: ForwardBatch, get_embedding: bool = False)
    return: LogitsProcessorOutput
    class: VILAForConditionalGeneration
  - name: get_image_feature
    signature: (self, mm_input: List[MultimodalDataItem])
    return: Tensor
    class: VILAForConditionalGeneration
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, Tensor]])
    return: None
    class: VILAForConditionalGeneration
  - name: pad_input_ids
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    return: List[int]
    class: VILAForConditionalGeneration
  - name: _vision_tower_output_to_mm_projector_input
    signature: (self, vision_tower_output: BaseModelOutputWithPooling)
    return: Tensor
    class: VILAForConditionalGeneration

File: models/xverse.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseMLP
  - name: forward
    signature: (self, x)
    class: XverseMLP
  - name: __init__
    signature: (self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, rope_is_neox_style: bool = True, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: XverseAttention
  - name: __init__
    signature: (self, config: LlamaConfig, layer_id: int = 0, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
    class: XverseDecoderLayer
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: XverseModel
  - name: __init__
    signature: (self, config: LlamaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor = None)
    return: torch.Tensor
    class: XverseForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]], name = None, loaded_weight = None)
    class: XverseForCausalLM
  - name: load_weights_per_param
    signature: (name, loaded_weight)
    class: XverseForCausalLM

File: models/xverse_moe.py
  - name: __init__
    signature: (self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig] = None, reduce_results: bool = True, prefix: str = '')
    return: None
    class: XverseMLP
  - name: forward
    signature: (self, x)
    class: XverseMLP
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: XverseMoE
  - name: pack_params
    signature: (self)
    class: XverseMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor)
    return: torch.Tensor
    class: XverseMoE
  - name: __init__
    signature: (self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int = 0, rope_theta: float = 10000, rope_scaling: Optional[Dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseAttention
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: XverseAttention
  - name: __init__
    signature: (self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseDecoderLayer
  - name: forward
    signature: (self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
    return: torch.Tensor
    class: XverseDecoderLayer
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseModel
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: XverseModel
  - name: __init__
    signature: (self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: XverseMoeForCausalLM
  - name: forward
    signature: (self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: XverseMoeForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: XverseMoeForCausalLM

File: models/yivl.py
  - name: __init__
    signature: (self, config: LlavaConfig, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    return: None
    class: YiVLForCausalLM
  - name: load_weights
    signature: (self, weights: Iterable[Tuple[str, torch.Tensor]])
    class: YiVLForCausalLM
  - name: __init__
    signature: (self, config: LlavaConfig)
    class: YiVLMultiModalProjector
  - name: forward
    signature: (self, image_features)
    class: YiVLMultiModalProjector
