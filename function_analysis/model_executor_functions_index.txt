AST Function Index
Root: /Users/vincentzed/Documents/Github/open_source/sglang/python/sglang/srt
Excluded directories:

File: model_executor/cuda_graph_runner.py
  - name: get_is_capture_mode
    signature: ()
  - name: model_capture_mode
    signature: ()
  - name: freeze_gc
    signature: (enable_cudagraph_gc: bool)
    doc: Optimize garbage collection during CUDA graph capture.
  - name: _to_torch
    signature: (model: torch.nn.Module, reverse: bool, num_tokens: int)
  - name: patch_model
    signature: (model: torch.nn.Module, enable_compile: bool, num_tokens: int, tp_group: GroupCoordinator)
    doc: Patch the model to make it compatible with with torch.compile
  - name: set_torch_compile_config
    signature: ()
  - name: get_batch_sizes_to_capture
    signature: (model_runner: ModelRunner)
  - name: get_global_graph_memory_pool
    signature: ()
  - name: set_global_graph_memory_pool
    signature: (val)
  - name: __init__
    signature: (self, model_runner: ModelRunner)
    class: CudaGraphRunner
  - name: _cache_loc_dtype
    signature: (self)
    class: CudaGraphRunner
  - name: can_run
    signature: (self, forward_batch: ForwardBatch)
    class: CudaGraphRunner
  - name: capture
    signature: (self)
    return: None
    class: CudaGraphRunner
  - name: _capture_graph
    signature: (self, graph, pool, stream, run_once_fn)
    class: CudaGraphRunner
  - name: _create_device_graph
    signature: (self)
    class: CudaGraphRunner
  - name: capture_one_batch_size
    signature: (self, bs: int, forward: Callable)
    class: CudaGraphRunner
  - name: run_once
    signature: ()
    class: CudaGraphRunner
  - name: recapture_if_needed
    signature: (self, forward_batch: ForwardBatch)
    class: CudaGraphRunner
  - name: replay_prepare
    signature: (self, forward_batch: ForwardBatch, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    class: CudaGraphRunner
  - name: replay
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[LogitsProcessorOutput, PPProxyTensors]
    class: CudaGraphRunner
  - name: get_spec_info
    signature: (self, num_tokens: int)
    class: CudaGraphRunner

File: model_executor/forward_batch_info.py
  - name: is_prefill
    signature: (self)
    class: ForwardMode
  - name: is_extend
    signature: (self)
    class: ForwardMode
  - name: is_decode
    signature: (self)
    class: ForwardMode
  - name: is_mixed
    signature: (self)
    class: ForwardMode
  - name: is_idle
    signature: (self)
    class: ForwardMode
  - name: is_decode_or_idle
    signature: (self)
    class: ForwardMode
  - name: is_target_verify
    signature: (self)
    class: ForwardMode
  - name: is_draft_extend
    signature: (self)
    class: ForwardMode
  - name: is_extend_or_draft_extend_or_mixed
    signature: (self)
    class: ForwardMode
  - name: is_cuda_graph
    signature: (self)
    class: ForwardMode
  - name: is_dummy_first
    signature: (self)
    class: ForwardMode
  - name: is_split_prefill
    signature: (self)
    class: ForwardMode
  - name: need_capture
    signature: (self)
    class: CaptureHiddenMode
  - name: is_full
    signature: (self)
    class: CaptureHiddenMode
  - name: is_last
    signature: (self)
    class: CaptureHiddenMode
  - name: __lt__
    signature: (self, other)
    class: CaptureHiddenMode
  - name: init_new
    signature: (cls, batch: ModelWorkerBatch, model_runner: ModelRunner)
    class: ForwardBatch
  - name: merge_mm_inputs
    signature: (self)
    return: Optional[MultimodalInputs]
    class: ForwardBatch
    doc: Merge all multimodal inputs in the batch into a single MultiModalInputs object.
  - name: contains_image_inputs
    signature: (self)
    return: bool
    class: ForwardBatch
  - name: contains_audio_inputs
    signature: (self)
    return: bool
    class: ForwardBatch
  - name: contains_video_inputs
    signature: (self)
    return: bool
    class: ForwardBatch
  - name: contains_mm_inputs
    signature: (self)
    return: bool
    class: ForwardBatch
  - name: _compute_mrope_positions
    signature: (self, model_runner: ModelRunner, batch: ModelWorkerBatch)
    class: ForwardBatch
  - name: get_max_chunk_capacity
    signature: (self)
    class: ForwardBatch
  - name: set_prefix_chunk_idx
    signature: (self, idx: int)
    class: ForwardBatch
  - name: set_attn_attend_prefix_cache
    signature: (self, attn_attend_prefix_cache: bool)
    class: ForwardBatch
  - name: prepare_chunked_kv_indices
    signature: (self, device: torch.device)
    class: ForwardBatch
  - name: _pad_tensor_to_size
    signature: (self, tensor: torch.Tensor, size: int, *, value: int = 0)
    class: ForwardBatch
  - name: prepare_mlp_sync_batch
    signature: (self, model_runner: ModelRunner)
    class: ForwardBatch
  - name: post_forward_mlp_sync_batch
    signature: (self, logits_output: LogitsProcessorOutput)
    class: ForwardBatch
  - name: get_prefix_chunk_seq_lens
    signature: (self, prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int)
    class: ForwardBatch
  - name: prepare_chunked_prefix_cache_info
    signature: (self, device: torch.device)
    class: ForwardBatch
  - name: can_run_tbo
    signature: (self)
    class: ForwardBatch
  - name: enable_num_token_non_padded
    signature: (server_args)
  - name: __init__
    signature: (self, tensors)
    class: PPProxyTensors
  - name: __getitem__
    signature: (self, key: Union[str, slice])
    class: PPProxyTensors
  - name: __setitem__
    signature: (self, key: str, value: torch.Tensor)
    class: PPProxyTensors
  - name: __len__
    signature: (self)
    class: PPProxyTensors
  - name: __eq__
    signature: (self, other: object)
    class: PPProxyTensors
  - name: __repr__
    signature: (self)
    return: str
    class: PPProxyTensors
  - name: compute_position
    signature: (attn_backend: str, extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum: int)
  - name: compute_position_triton
    signature: (extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum)
    doc: Compute positions. It is a fused version of `compute_position_torch`.
  - name: compute_position_kernel
    signature: (positions, extend_start_loc, extend_prefix_lens, extend_seq_lens, has_prefix: tl.constexpr)
  - name: compute_position_torch
    signature: (extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor)
  - name: clamp_position
    signature: (seq_lens)
  - name: create_chunked_prefix_cache_kv_indices
    signature: (req_to_token_ptr, req_pool_indices_ptr, chunk_start_idx_ptr, chunk_seq_lens_ptr, chunk_cu_seq_lens_ptr, chunk_kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr)

File: model_executor/model_runner.py
  - name: __init__
    signature: (self, is_rank_zero)
    class: RankZeroFilter
  - name: filter
    signature: (self, record)
    class: RankZeroFilter
  - name: __init__
    signature: (self, model_config: ModelConfig, mem_fraction_static: float, gpu_id: int, tp_rank: int, tp_size: int, moe_ep_rank: int, moe_ep_size: int, pp_rank: int, pp_size: int, nccl_port: int, server_args: ServerArgs, dp_rank: Optional[int] = None, is_draft_worker: bool = False, req_to_token_pool: Optional[ReqToTokenPool] = None, token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator] = None)
    class: ModelRunner
  - name: initialize
    signature: (self, min_per_gpu_memory: float)
    class: ModelRunner
  - name: model_specific_adjustment
    signature: (self)
    class: ModelRunner
  - name: init_torch_distributed
    signature: (self)
    class: ModelRunner
  - name: load_model
    signature: (self)
    class: ModelRunner
  - name: update_expert_location
    signature: (self, new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int])
    class: ModelRunner
  - name: update_weights_from_disk
    signature: (self, model_path: str, load_format: str)
    return: tuple[bool, str]
    class: ModelRunner
    doc: Update engine weights in-place from the disk.
  - name: get_weight_iter
    signature: (config)
    class: ModelRunner
  - name: model_load_weights
    signature: (model, iter)
    class: ModelRunner
  - name: init_weights_update_group
    signature: (self, master_address, master_port, rank_offset, world_size, group_name, backend = 'nccl')
    class: ModelRunner
    doc: Initialize the Torch process group for model parameter updates.
  - name: update_weights_from_distributed
    signature: (self, names, dtypes, shapes, group_name)
    class: ModelRunner
    doc: Update specific parameter in the model weights online
  - name: update_weights_from_tensor
    signature: (self, named_tensors: List[Tuple[str, Union[torch.Tensor, 'LocalSerializedTensor']]], load_format: Optional[str] = None)
    class: ModelRunner
  - name: _update_weights_from_flattened_bucket
    signature: (self, flattened_tensor_bucket_dict)
    class: ModelRunner
    doc: Handle flattened bucket format for weight updates
  - name: get_weights_by_name
    signature: (self, name: str, truncate_size: int = 100)
    return: Optional[torch.Tensor]
    class: ModelRunner
    doc: Get the weights of the parameter by its name. Similar to `get_parameter` in Hugging Face.
  - name: init_lora_manager
    signature: (self)
    class: ModelRunner
  - name: load_lora_adapter
    signature: (self, lora_ref: LoRARef)
    class: ModelRunner
    doc: Load a new lora adapter from disk or huggingface.
  - name: unload_lora_adapter
    signature: (self, lora_ref: LoRARef)
    class: ModelRunner
    doc: Unload a lora adapter that was previously loaded during initialization or dynamic loading.
  - name: profile_max_num_token
    signature: (self, total_gpu_memory: int)
    class: ModelRunner
  - name: set_num_token_hybrid
    signature: (self)
    class: ModelRunner
  - name: init_memory_pool
    signature: (self, total_gpu_memory: int, max_num_reqs: Optional[int] = None, max_total_tokens: Optional[int] = None)
    class: ModelRunner
  - name: init_cublas
    signature: (self)
    class: ModelRunner
    doc: We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.
  - name: init_attention_backend
    signature: (self)
    class: ModelRunner
    doc: Init attention kernel backend.
  - name: _get_attention_backend
    signature: (self)
    class: ModelRunner
    doc: Init attention kernel backend.
  - name: _get_attention_backend_from_str
    signature: (self, backend_str: str)
    class: ModelRunner
  - name: init_double_sparsity_channel_config
    signature: (self, selected_channel)
    class: ModelRunner
  - name: init_device_graphs
    signature: (self)
    class: ModelRunner
    doc: Capture cuda graphs.
  - name: init_threads_binding
    signature: (self)
    class: ModelRunner
  - name: apply_torch_tp
    signature: (self)
    class: ModelRunner
  - name: forward_decode
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors = None)
    return: LogitsProcessorOutput
    class: ModelRunner
  - name: forward_extend
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors = None)
    return: LogitsProcessorOutput
    class: ModelRunner
  - name: forward_idle
    signature: (self, forward_batch: ForwardBatch, pp_proxy_tensors = None)
    return: LogitsProcessorOutput
    class: ModelRunner
  - name: forward_split_prefill
    signature: (self, forward_batch: ForwardBatch, reinit_attn_backend: bool = False, forward_count: int = 1)
    return: LogitsProcessorOutput
    class: ModelRunner
  - name: forward
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None, reinit_attn_backend: bool = False, split_forward_count: int = 1)
    return: Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]
    class: ModelRunner
  - name: _forward_raw
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors], reinit_attn_backend: bool = False, split_forward_count: int = 1)
    return: Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]
    class: ModelRunner
  - name: _preprocess_logits
    signature: (self, logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo)
    class: ModelRunner
  - name: sample
    signature: (self, logits_output: LogitsProcessorOutput, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: ModelRunner
    doc: Sample and compute logprobs and update logits_output.
  - name: model_is_mrope
    signature: (self)
    return: bool
    class: ModelRunner
    doc: Detect if the model has "mrope" rope_scaling type.
  - name: save_remote_model
    signature: (self, url: str)
    class: ModelRunner
  - name: save_sharded_model
    signature: (self, path: str, pattern: Optional[str] = None, max_size: Optional[int] = None)
    class: ModelRunner
  - name: _model_load_weights_direct
    signature: (model, named_tensors: List[Tuple[str, torch.Tensor]])
  - name: _unwrap_tensor
    signature: (tensor, tp_rank, device)
  - name: get
    signature: (self, rank: int)
    class: LocalSerializedTensor

File: model_executor/npu_graph_runner.py
  - name: __init__
    signature: (self, model_runner: ModelRunner)
    class: NPUGraphRunner
  - name: _create_device_graph
    signature: (self)
    class: NPUGraphRunner
  - name: _capture_graph
    signature: (self, graph, pool, stream, run_once_fn)
    class: NPUGraphRunner
  - name: _update_inputs
    signature: (self, seq_lens)
    class: NPUGraphRunner
  - name: _cache_loc_dtype
    signature: (self)
    class: NPUGraphRunner
  - name: replay
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[LogitsProcessorOutput, PPProxyTensors]
    class: NPUGraphRunner
