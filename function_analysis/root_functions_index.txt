AST Function Index
Root: /Users/vincentzed/Documents/Github/open_source/sglang/python/sglang/srt
Excluded directories:

File: _custom_ops.py
  - name: init_custom_ar
    signature: (ipc_tensors: List[torch.Tensor], rank_data: torch.Tensor, rank: int, full_nvlink: bool)
    return: int
  - name: all_reduce
    signature: (fa: int, inp: torch.Tensor, out: torch.Tensor, reg_buffer: int, reg_buffer_sz_bytes: int)
    return: None
  - name: dispose
    signature: (fa: int)
    return: None
  - name: meta_size
    signature: ()
    return: int
  - name: register_buffer
    signature: (fa: int, ipc_tensors: List[int])
    return: None
  - name: get_graph_buffer_ipc_meta
    signature: (fa: int)
    return: Tuple[List[int], List[int]]
  - name: register_graph_buffers
    signature: (fa: int, handles: List[List[int]], offsets: List[List[int]])
    return: None
  - name: init_custom_ar
    signature: (meta: torch.Tensor, rank_data: torch.Tensor, handles: List[str], offsets: List[int], rank: int, full_nvlink: bool)
    return: int
  - name: all_reduce_reg
    signature: (fa: int, inp: torch.Tensor, out: torch.Tensor)
    return: None
  - name: all_reduce_unreg
    signature: (fa: int, inp: torch.Tensor, reg_buffer: torch.Tensor, out: torch.Tensor)
    return: None
  - name: dispose
    signature: (fa: int)
    return: None
  - name: meta_size
    signature: ()
    return: int
  - name: register_buffer
    signature: (fa: int, t: torch.Tensor, handles: List[str], offsets: List[int])
    return: None
  - name: get_graph_buffer_ipc_meta
    signature: (fa: int)
    return: Tuple[torch.Tensor, List[int]]
  - name: register_graph_buffers
    signature: (fa: int, handles: List[str], offsets: List[List[int]])
    return: None
  - name: allocate_meta_buffer
    signature: (size: int)
    return: torch.Tensor
  - name: get_meta_buffer_ipc_handle
    signature: (inp: torch.Tensor)
    return: torch.Tensor
  - name: init_custom_qr
    signature: (rank: int, world_size: int, qr_max_size: Optional[int] = None)
    return: int
  - name: qr_get_handle
    signature: (fa: int)
    return: torch.Tensor
  - name: qr_open_handles
    signature: (fa: int, handles: list[torch.Tensor])
    return: None
  - name: qr_all_reduce
    signature: (fa: int, inp: torch.Tensor, out: torch.Tensor, quant_level: int, cast_bf2half: bool)
    return: None
  - name: qr_destroy
    signature: (fa: int)
    return: None
  - name: qr_max_size
    signature: ()
    return: int
  - name: mscclpp_generate_unique_id
    signature: ()
    return: bytes
  - name: mscclpp_init_context
    signature: (unique_id: bytes, rank: int, world_size: int, scratch: torch.Tensor, put_buffer: torch.Tensor, nranks_per_node: int, rank_to_node: List[int], rank_to_ib: List[int], context_selection: int)
    return: int
  - name: mscclpp_allreduce
    signature: (context: int, inp: torch.Tensor, out: torch.Tensor, nthreads: int, nblocks: int)
    return: None

File: aio_rwlock.py
  - name: __init__
    signature: (self)
    class: RWLock
  - name: reader_lock
    signature: (self)
    class: RWLock
    doc: A context manager for acquiring a shared (reader) lock.
  - name: writer_lock
    signature: (self)
    class: RWLock
    doc: A context manager for acquiring an exclusive (writer) lock.
  - name: acquire_reader
    signature: (self)
    class: RWLock
  - name: release_reader
    signature: (self)
    class: RWLock
  - name: acquire_writer
    signature: (self)
    class: RWLock
  - name: release_writer
    signature: (self)
    class: RWLock
  - name: __init__
    signature: (self, rwlock: RWLock)
    class: _ReaderLock
  - name: __aenter__
    signature: (self)
    class: _ReaderLock
  - name: __aexit__
    signature: (self, exc_type, exc_val, exc_tb)
    class: _ReaderLock
  - name: __init__
    signature: (self, rwlock: RWLock)
    class: _WriterLock
  - name: __aenter__
    signature: (self)
    class: _WriterLock
  - name: __aexit__
    signature: (self, exc_type, exc_val, exc_tb)
    class: _WriterLock

File: bench_utils.py
  - name: __enter__
    signature: (self)
    class: suppress_stdout_stderr
  - name: __exit__
    signature: (self, *_)
    class: suppress_stdout_stderr
  - name: bench_kineto
    signature: (fn, kernel_names, num_tests: int = 30, suppress_kineto_output: bool = False, trace_path: str = None, flush_l2: bool = True, with_multiple_kernels: bool = False)

File: code_completion_parser.py
  - name: register_completion_template
    signature: (template: CompletionTemplate, override: bool = False)
    doc: Register a new completion template.
  - name: completion_template_exists
    signature: (template_name: str)
    return: bool
  - name: is_completion_template_defined
    signature: ()
    return: bool
  - name: generate_completion_prompt_from_request
    signature: (request: CompletionRequest)
    return: str
  - name: generate_completion_prompt
    signature: (prompt: str, suffix: str, template_name: str)
    return: str

File: constants.py
  (no function definitions found)
File: conversation.py
  - name: get_prompt
    signature: (self)
    return: str
    class: Conversation
    doc: Get the prompt for generation.
  - name: set_system_message
    signature: (self, system_message: str)
    class: Conversation
    doc: Set the system message.
  - name: append_message
    signature: (self, role: str, message: str)
    class: Conversation
    doc: Append a new message.
  - name: append_image
    signature: (self, image: str, detail: Literal['auto', 'low', 'high'])
    class: Conversation
    doc: Append a new image.
  - name: append_video
    signature: (self, video: str)
    class: Conversation
    doc: Append a new video.
  - name: append_audio
    signature: (self, audio: str)
    class: Conversation
    doc: Append a new audio.
  - name: update_last_message
    signature: (self, message: str)
    class: Conversation
    doc: Update the last output.
  - name: to_gradio_chatbot
    signature: (self)
    class: Conversation
    doc: Convert the conversation to gradio chatbot format.
  - name: to_openai_api_messages
    signature: (self)
    class: Conversation
    doc: Convert the conversation to OpenAI chat completion format.
  - name: copy
    signature: (self)
    class: Conversation
  - name: dict
    signature: (self)
    class: Conversation
  - name: register_conv_template
    signature: (template: Conversation, override: bool = False)
    doc: Register a new conversation template.
  - name: register_conv_template_matching_function
    signature: (func)
  - name: get_conv_template_by_model_path
    signature: (model_path)
  - name: chat_template_exists
    signature: (template_name: str)
    return: bool
  - name: generate_embedding_convs
    signature: (texts: List[str], images: List[str], template_name: str)
    return: List[Conversation]
  - name: _get_full_multimodal_text_prompt
    signature: (modality_token: str, modality_count: int, text_prompt: str)
    return: str
    doc: Combine multimodal prompts for a multimodal language model.
  - name: generate_chat_conv
    signature: (request: ChatCompletionRequest, template_name: str)
    return: Conversation
  - name: get_model_type
    signature: (model_path: str)
    return: Optional[str]
  - name: match_internvl
    signature: (model_path: str)
  - name: match_deepseek_janus_pro
    signature: (model_path: str)
  - name: match_vicuna
    signature: (model_path: str)
  - name: match_deepseek_vl
    signature: (model_path: str)
  - name: match_qwen_chat_ml
    signature: (model_path: str)
  - name: match_minicpm
    signature: (model_path: str)
  - name: match_phi_4_mm
    signature: (model_path: str)

File: custom_op.py
  - name: __init__
    signature: (self)
    class: CustomOp
  - name: enter_torch_compile
    signature: (self, num_tokens: int)
    class: CustomOp
  - name: leave_torch_compile
    signature: (self)
    class: CustomOp
  - name: forward
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_native
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_cuda
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_npu
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_hip
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_xpu
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_hpu
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_cpu
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: dispatch_forward
    signature: (self)
    class: CustomOp

File: harmony_parser.py
  - name: prefix_hold
    signature: (text: str, tokens: List[str])
    return: Tuple[str, str]
    doc: Holds back the longest suffix of `text` that could be a prefix of any token.
  - name: iter_tokens
    signature: (text: str, start_pos: int = 0)
    return: Iterator[Token]
    doc: Iterate over structural tokens in left-to-right order.
  - name: __init__
    signature: (self)
    class: CanonicalStrategy
  - name: parse
    signature: (self, text: str)
    return: Tuple[List[Event], str]
    class: CanonicalStrategy
  - name: _parse_partial_analysis
    signature: (self, text: str, tokens: List[Token], start_pos: int)
    return: Optional[Tuple[Event, str]]
    class: CanonicalStrategy
    doc: Try to parse partial analysis content for incremental streaming.
  - name: _extract_channel_type
    signature: (self, header_text: str)
    return: Optional[str]
    class: CanonicalStrategy
    doc: Extract channel type from header, ignoring other attributes like to=... or <|constrain|>...
  - name: _parse_block
    signature: (self, text: str, tokens: List[Token], start_pos: int)
    return: Optional[Tuple[Optional[Event], int]]
    class: CanonicalStrategy
    doc: Parse a channel block. Returns (event, next_pos) or None if incomplete.
  - name: _is_commentary_filler_between_blocks
    signature: (self, text: str, tokens: List[Token], pos: int)
    return: bool
    class: CanonicalStrategy
    doc: Check if this is commentary filler text or problematic structural tokens in malformed sequences.
  - name: _is_standalone_structural_token
    signature: (self, content: str)
    return: bool
    class: CanonicalStrategy
    doc: Check if content is just a standalone structural token that should be filtered.
  - name: __init__
    signature: (self)
    class: TextStrategy
  - name: set_buffer_context
    signature: (self, buffer: str)
    class: TextStrategy
  - name: parse
    signature: (self, text: str)
    return: Tuple[List[Event], str]
    class: TextStrategy
  - name: __init__
    signature: (self)
    class: HarmonyParser
  - name: parse
    signature: (self, chunk: str)
    return: List[Event]
    class: HarmonyParser

File: hf_transformers_utils.py
  - name: download_from_hf
    signature: (model_path: str, allow_patterns: Optional[Union[str, list]] = None)
  - name: get_hf_text_config
    signature: (config: PretrainedConfig)
    doc: Get the "sub" config relevant to llm for multi modal models.
  - name: get_config
    signature: (model: str, trust_remote_code: bool, revision: Optional[str] = None, model_override_args: Optional[dict] = None, **kwargs)
  - name: get_generation_config
    signature: (model: str, trust_remote_code: bool, revision: Optional[str] = None, **kwargs)
  - name: get_sparse_attention_config
    signature: (model: str, sparse_attention_config_filename: str = 'sparse_attention_config.json')
    return: Dict[str, Any]
  - name: get_context_length
    signature: (config)
    doc: Get the context length of a model from a huggingface model configs.
  - name: get_tokenizer
    signature: (tokenizer_name: str, *args, tokenizer_mode: str = 'auto', trust_remote_code: bool = False, tokenizer_revision: Optional[str] = None, **kwargs)
    return: Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
    doc: Gets a tokenizer for the given model name via Huggingface.
  - name: get_tokenizer_from_processor
    signature: (processor)
  - name: get_processor
    signature: (tokenizer_name: str, *args, tokenizer_mode: str = 'auto', trust_remote_code: bool = False, tokenizer_revision: Optional[str] = None, use_fast: Optional[bool] = True, **kwargs)
  - name: attach_additional_stop_token_ids
    signature: (tokenizer)
  - name: check_gguf_file
    signature: (model: Union[str, os.PathLike])
    return: bool
    doc: Check if the file is a GGUF model.

File: host_shared_memory.py
  - name: __init__
    signature: (self, base_name: str)
    class: HostSharedMemoryManager
  - name: malloc
    signature: (self, *, shape, dtype)
    class: HostSharedMemoryManager
  - name: _malloc_raw
    signature: (self, *, num_bytes: int)
    return: torch.Tensor
    class: HostSharedMemoryManager
  - name: get_host_shared_memory_manager
    signature: ()
  - name: set_host_shared_memory_manager
    signature: (instance: HostSharedMemoryManager)

File: jinja_template_utils.py
  - name: _is_var_access
    signature: (node: jinja2.nodes.Node, varname: str)
    return: bool
    doc: Check if node is a variable access like {{ varname }}
  - name: _is_attr_access
    signature: (node: jinja2.nodes.Node, varname: str, key: str)
    return: bool
    doc: Check if node is an attribute access like {{ varname['key'] }} or {{ varname.key }}
  - name: _is_var_or_elems_access
    signature: (node: jinja2.nodes.Node, varname: str, key: str = None)
    return: bool
    doc: Check if node accesses varname or varname[key] with filters/tests
  - name: _try_extract_ast
    signature: (chat_template: str)
    doc: Try to parse the Jinja template into an AST
  - name: detect_jinja_template_content_format
    signature: (chat_template: str)
    return: str
    doc: Detect whether a chat template expects 'string' or 'openai' content format.
  - name: process_content_for_template_format
    signature: (msg_dict: dict, content_format: str, image_data: list, video_data: list, audio_data: list, modalities: list)
    return: dict
    doc: Process message content based on detected template format.

File: model_parallel.py
  - name: _shard_tensor
    signature: (full_tensor: torch.Tensor, device_mesh: DeviceMesh, placements: Sequence[dt.Shard])
    return: 'dt.DTensor'
    doc: Locally shards a full tensor based on indicated sharding arrangement, and
  - name: _partition_linear_fn
    signature: (self, name, module, device_mesh)
    class: ColwiseParallelSharded
  - name: _partition_linear_fn
    signature: (self, name, module, device_mesh)
    class: RowwiseParallelMaybeWait
  - name: _prepare_output_fn
    signature: (output_layouts, use_local_output, mod, outputs, device_mesh)
    class: RowwiseParallelMaybeWait
  - name: tensor_parallel
    signature: (module: torch.nn.Module, device_mesh: Optional[DeviceMesh] = None)
    doc: Tensor parallelize the model across the given device mesh.
  - name: tplize
    signature: (mod: torch.nn.Module)
    return: None

File: offloader.py
  - name: wrap_modules
    signature: (self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor] = None, whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator] = None)
    class: BaseOffloader
  - name: post_init
    signature: (self)
    class: BaseOffloader
  - name: get_offloader
    signature: ()
  - name: set_offloader
    signature: (instance: BaseOffloader)
  - name: create_offloader_from_server_args
    signature: (server_args: ServerArgs, dp_rank: int)
  - name: __init__
    signature: (self, cpu_offload_max_bytes: int)
    class: OffloaderV1
  - name: wrap_modules
    signature: (self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor] = None, whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator] = None)
    class: OffloaderV1
  - name: maybe_offload_to_cpu
    signature: (self, module: torch.nn.Module)
    return: torch.nn.Module
    class: OffloaderV1
  - name: forward
    signature: (*args, **kwargs)
    class: OffloaderV1
  - name: __init__
    signature: (self, group_size: int, num_in_group: int, prefetch_step: int, mode: str, dp_rank: int, dp_size: int)
    class: OffloaderV2
  - name: wrap_modules
    signature: (self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor] = None, whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator] = None)
    class: OffloaderV2
  - name: post_init
    signature: (self)
    class: OffloaderV2
  - name: _hook_module_forward_for_offloader
    signature: (index, module, offloaders, prefetch_step)
  - name: _on_forward_end
    signature: ()
  - name: _hook_module_forward_raw
    signature: (module, on_forward_end, get_parameter_and_buffer_dicts)
  - name: forward
    signature: (*args, **kwargs)
  - name: __init__
    signature: (self, mode: str, module: torch.nn.Module, alt_stream: torch.cuda.Stream, whitelist_param_names: List[str])
    class: _ModuleOffloader
  - name: post_init
    signature: (self)
    class: _ModuleOffloader
  - name: start_onload
    signature: (self)
    class: _ModuleOffloader
  - name: offload
    signature: (self)
    class: _ModuleOffloader
  - name: wait_and_get_device_tensors
    signature: (self)
    class: _ModuleOffloader
  - name: _create_device_tensors
    signature: (self)
    class: _ModuleOffloader
  - name: create
    signature: (mode: str, **kwargs)
    return: '_BaseParamOffloader'
    class: _BaseParamOffloader
  - name: __init__
    signature: (self, module, param_name)
    class: _BaseParamOffloader
  - name: _param
    signature: (self)
    class: _BaseParamOffloader
  - name: post_init
    signature: (self)
    class: _BaseParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _BaseParamOffloader
  - name: __init__
    signature: (self, module, param_name)
    class: _MetaParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _MetaParamOffloader
  - name: __init__
    signature: (self, module, param_name)
    class: _CpuParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _CpuParamOffloader
  - name: __init__
    signature: (self, module, param_name)
    class: _ShmCpuParamOffloader
  - name: post_init
    signature: (self)
    class: _ShmCpuParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _ShmCpuParamOffloader
  - name: _move_param_to_cpu
    signature: (param, pin_memory: bool)
  - name: _move_param_to_meta
    signature: (module, param_name)
  - name: _empty_strided_like
    signature: (x: torch.Tensor, device, pin_memory = False)
  - name: __init__
    signature: (self, module, param_name)
    class: _ShardedGpuParamOffloader
  - name: post_init
    signature: (self)
    class: _ShardedGpuParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _ShardedGpuParamOffloader
  - name: _even_chunk
    signature: (x: torch.Tensor, chunks: int)
  - name: _create_shared_buffer_tensors
    signature: (local_tensor: torch.Tensor)
    return: List[torch.Tensor]

File: operations.py
  - name: execute_operations
    signature: (inputs, operations)
  - name: execute_overlapped_operations
    signature: (inputs_arr: Sequence, operations_arr: Sequence, delta_stages: Sequence[int])
    return: Sequence
  - name: __init__
    signature: (self, debug_name: str, stages: List[Stage], inputs: dict)
    class: _StageExecutor
  - name: next
    signature: (self)
    class: _StageExecutor
  - name: output
    signature: (self)
    class: _StageExecutor
  - name: done
    signature: (self)
    class: _StageExecutor
  - name: num_stages
    signature: (self)
    class: _StageExecutor
  - name: _annotate_region
    signature: (debug_name)
  - name: __init__
    signature: (self)
    class: _StateDict
  - name: __setattr__
    signature: (self, key, value)
    class: _StateDict
  - name: __getattr__
    signature: (self, item)
    class: _StateDict
  - name: __delattr__
    signature: (self, item)
    class: _StateDict
  - name: pop
    signature: (self, item)
    class: _StateDict
  - name: update
    signature: (self, values: Dict[str, Any])
    class: _StateDict
  - name: get
    signature: (self, item)
    class: _StateDict
  - name: clear
    signature: (self, expect_keys: Sequence[str])
    class: _StateDict
  - name: _convert_operations_to_stages
    signature: (operations: List[Operation])
    return: List[Stage]
  - name: _chunk_by_separator
    signature: (items: List[Any], is_separator: Callable[[Any], bool])
    return: Generator[List[Any], None, None]
  - name: _decorate_operations
    signature: (operations: List[Operation], debug_name_prefix: str = '')
  - name: _decorate_operation
    signature: (operation: Operation, debug_name_prefix: str)

File: operations_strategy.py
  - name: concat
    signature: (cls, items: List['OperationsStrategy'])
    return: 'OperationsStrategy'
    class: OperationsStrategy
  - name: init_new_tbo
    signature: (layers: torch.nn.ModuleList, forward_mode: ForwardMode)
    return: 'OperationsStrategy'
    class: OperationsStrategy
  - name: _assert_all_same
    signature: (items: List)
  - name: _compute_moe_deepseek_layer_operations_strategy_tbo
    signature: (layer: torch.nn.Module, forward_mode: ForwardMode)
    return: OperationsStrategy
  - name: _compute_moe_deepseek_blog_prefill
    signature: (layer)
  - name: _compute_moe_deepseek_blog_decode
    signature: (layer)
  - name: _compute_moe_qwen3_layer_operations_strategy_tbo
    signature: (layer: torch.nn.Module, forward_mode: ForwardMode)
    return: OperationsStrategy
  - name: _compute_moe_qwen3_prefill
    signature: (layer)
  - name: _compute_moe_qwen3_decode
    signature: (layer)

File: patch_torch.py
  - name: monkey_patch_torch_reductions
    signature: ()
    doc: Monkey patching before Torch https://github.com/pytorch/pytorch/pull/149248 is fixed
  - name: _reduce_tensor_modified
    signature: (*args, **kwargs)
  - name: _rebuild_cuda_tensor_modified
    signature: (*args)
  - name: _device_to_uuid
    signature: (device: int)
    return: str
  - name: _device_from_maybe_uuid
    signature: (device_maybe_uuid: Union[int, str])
    return: int
  - name: _modify_tuple
    signature: (t, index: int, modifier: Callable)
  - name: monkey_patch_torch_compile
    signature: ()

File: poll_based_barrier.py
  - name: __init__
    signature: (self, noop: bool = False)
    class: PollBasedBarrier
  - name: local_arrive
    signature: (self)
    class: PollBasedBarrier
  - name: poll_global_arrived
    signature: (self)
    return: bool
    class: PollBasedBarrier
  - name: _compute_global_arrived
    signature: (self)
    return: bool
    class: PollBasedBarrier

File: reasoning_parser.py
  - name: __init__
    signature: (self, normal_text: Optional[str] = None, reasoning_text: Optional[str] = None)
    class: StreamingParseResult
  - name: __init__
    signature: (self, think_start_token: str, think_end_token: str, force_reasoning: bool = False, stream_reasoning: bool = True)
    class: BaseReasoningFormatDetector
  - name: detect_and_parse
    signature: (self, text: str)
    return: StreamingParseResult
    class: BaseReasoningFormatDetector
    doc: One-time parsing: Detects and parses reasoning sections in the provided text.
  - name: parse_streaming_increment
    signature: (self, new_text: str)
    return: StreamingParseResult
    class: BaseReasoningFormatDetector
    doc: Streaming incremental parsing for reasoning content.
  - name: __init__
    signature: (self, stream_reasoning: bool = True, force_reasoning: bool = True)
    class: DeepSeekR1Detector
  - name: __init__
    signature: (self, stream_reasoning: bool = True, force_reasoning: bool = False)
    class: Qwen3Detector
  - name: __init__
    signature: (self, stream_reasoning: bool = True, force_reasoning: bool = False)
    class: KimiDetector
  - name: __init__
    signature: (self, stream_reasoning: bool = True, force_reasoning: bool = True)
    class: GptOssDetector
  - name: detect_and_parse
    signature: (self, text: str)
    return: StreamingParseResult
    class: GptOssDetector
  - name: parse_streaming_increment
    signature: (self, new_text: str)
    return: StreamingParseResult
    class: GptOssDetector
  - name: __init__
    signature: (self, model_type: Optional[str] = None, stream_reasoning: bool = True, force_reasoning: Optional[bool] = None)
    class: ReasoningParser
  - name: parse_non_stream
    signature: (self, full_text: str)
    return: Tuple[Optional[str], Optional[str]]
    class: ReasoningParser
    doc: Non-streaming call: one-time parsing
  - name: parse_stream_chunk
    signature: (self, chunk_text: str)
    return: Tuple[Optional[str], Optional[str]]
    class: ReasoningParser
    doc: Streaming call: incremental parsing

File: server_args.py
  - name: add_load_format_choices
    signature: (choices)
  - name: add_quantization_method_choices
    signature: (choices)
  - name: add_attention_backend_choices
    signature: (choices)
  - name: add_disagg_transfer_backend_choices
    signature: (choices)
  - name: __post_init__
    signature: (self)
    class: ServerArgs
  - name: add_cli_args
    signature: (parser: argparse.ArgumentParser)
    class: ServerArgs
  - name: from_cli_args
    signature: (cls, args: argparse.Namespace)
    class: ServerArgs
  - name: url
    signature: (self)
    class: ServerArgs
  - name: get_hf_config
    signature: (self)
    class: ServerArgs
  - name: check_server_args
    signature: (self)
    class: ServerArgs
  - name: check_lora_server_args
    signature: (self)
    class: ServerArgs
  - name: validate_disagg_tp_size
    signature: (self, prefill_tp: int, decode_tp: int)
    class: ServerArgs
  - name: model_specific_adjustments
    signature: (self)
    class: ServerArgs
  - name: adjust_mem_fraction_for_vlm
    signature: (self, model_config)
    class: ServerArgs
  - name: prepare_server_args
    signature: (argv: List[str])
    return: ServerArgs
    doc: Prepare the server arguments from the command line arguments.
  - name: init_new
    signature: (server_args, dp_rank: Optional[int] = None)
    return: 'PortArgs'
    class: PortArgs
  - name: __call__
    signature: (self, parser, namespace, values, option_string = None)
    class: LoRAPathAction
  - name: __init__
    signature: (self, option_strings, dest, nargs = 0, **kwargs)
    class: DeprecatedAction
  - name: __call__
    signature: (self, parser, namespace, values, option_string = None)
    class: DeprecatedAction
  - name: print_deprecated_warning
    signature: (message: str)
  - name: auto_choose_speculative_params
    signature: (self: ServerArgs)
    doc: Automatically choose the parameters for speculative decoding.

File: torch_memory_saver_adapter.py
  - name: create
    signature: (enable: bool)
    class: TorchMemorySaverAdapter
  - name: check_validity
    signature: (self, caller_name)
    class: TorchMemorySaverAdapter
  - name: configure_subprocess
    signature: (self)
    class: TorchMemorySaverAdapter
  - name: region
    signature: (self, tag: str)
    class: TorchMemorySaverAdapter
  - name: pause
    signature: (self, tag: str)
    class: TorchMemorySaverAdapter
  - name: resume
    signature: (self, tag: str)
    class: TorchMemorySaverAdapter
  - name: enabled
    signature: (self)
    class: TorchMemorySaverAdapter
  - name: configure_subprocess
    signature: (self)
    class: _TorchMemorySaverAdapterReal
  - name: region
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterReal
  - name: pause
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterReal
  - name: resume
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterReal
  - name: enabled
    signature: (self)
    class: _TorchMemorySaverAdapterReal
  - name: configure_subprocess
    signature: (self)
    class: _TorchMemorySaverAdapterNoop
  - name: region
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterNoop
  - name: pause
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterNoop
  - name: resume
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterNoop
  - name: enabled
    signature: (self)
    class: _TorchMemorySaverAdapterNoop

File: two_batch_overlap.py
  - name: get_token_num_per_seq
    signature: (forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None)
  - name: compute_split_seq_index
    signature: (forward_mode: 'ForwardMode', num_tokens: int, extend_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int])
    return: Optional[int]
  - name: _is_two_chunk_split_enabled
    signature: (extend_lens: Sequence[int])
    return: bool
  - name: _split_extend_seqs
    signature: (arr: Sequence[int])
    return: int
  - name: _split_array_by_cum_less_than_half
    signature: (arr: Sequence[int])
    return: int
  - name: _split_array_by_balanced_sum
    signature: (arr: Sequence[int])
    return: int
  - name: _update_device_and_sum_field_from_cpu_field
    signature: (batch: ForwardBatch, cpu_field: str, device_field: str, sum_field: str = None)
  - name: _compute_mask_offset
    signature: (seq_index: int, spec_info: Optional[EagleVerifyInput])
    return: int
  - name: split_spec_info
    signature: (spec_info: Optional[EagleVerifyInput], start_seq_index: int, end_seq_index: int, start_token_index: int, end_token_index: int)
  - name: compute_split_token_index
    signature: (split_seq_index: int, forward_mode: 'ForwardMode', extend_seq_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int])
    return: int
  - name: compute_split_indices_for_cuda_graph_replay
    signature: (forward_mode: ForwardMode, cuda_graph_num_tokens: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  - name: __init__
    signature: (self)
    class: TboCudaGraphRunnerPlugin
  - name: capture_one_batch_size
    signature: (self, batch: ForwardBatch, num_tokens: int)
    class: TboCudaGraphRunnerPlugin
  - name: replay_prepare
    signature: (self, forward_mode: ForwardMode, bs: int, num_token_non_padded: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: TboCudaGraphRunnerPlugin
  - name: prepare_all_gather
    signature: (self, local_batch: ScheduleBatch)
    class: TboDPAttentionPreparer
  - name: compute_output
    signature: (self, partial_global_info)
    class: TboDPAttentionPreparer
  - name: _compute_local_forward_mode
    signature: (local_batch)
    class: TboDPAttentionPreparer
  - name: _compute_global_forward_mode
    signature: (forward_modes)
    class: TboDPAttentionPreparer
  - name: _is_all_same
    signature: (x)
    class: TboDPAttentionPreparer
  - name: prepare
    signature: (cls, batch: ForwardBatch, is_draft_worker: bool = False)
    class: TboForwardBatchPreparer
  - name: prepare_raw
    signature: (cls, batch: ForwardBatch, tbo_children_num_token_non_padded: torch.Tensor)
    class: TboForwardBatchPreparer
  - name: derive_fields_related_to_seq_len_for_two_chunk
    signature: (cls, batch: ForwardBatch, *, child_a: ForwardBatch, child_b: ForwardBatch, tbo_split_seq_index: int)
    class: TboForwardBatchPreparer
  - name: filter_batch
    signature: (cls, batch: ForwardBatch, *, start_token_index: int, end_token_index: int, start_seq_index: int, end_seq_index: int, output_attn_backend: AttentionBackend, out_num_token_non_padded: torch.Tensor)
    class: TboForwardBatchPreparer
  - name: compute_tbo_children_num_token_non_padded
    signature: (cls, batch: ForwardBatch)
    class: TboForwardBatchPreparer
  - name: compute_tbo_children_num_token_non_padded_raw
    signature: (cls, tbo_split_token_index: int, num_token_non_padded: int)
    class: TboForwardBatchPreparer
  - name: _compute_split_token_index
    signature: (cls, batch: ForwardBatch)
    class: TboForwardBatchPreparer
  - name: _compute_extend_num_tokens
    signature: (input_ids, forward_mode: ForwardMode)
  - name: model_forward_maybe_tbo
    signature: (layers, enable_tbo: bool, positions: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor, input_data_scatter_mode: ScatterMode, residual: Optional[torch.Tensor], zero_allocator: Optional[BumpAllocator] = None)
  - name: _model_forward_tbo
    signature: (inputs, operations_strategy: OperationsStrategy, input_data_scatter_mode: ScatterMode, layer_input_scatter_mode: ScatterMode)
  - name: _model_forward_non_tbo
    signature: (inputs, operations_strategy: OperationsStrategy)
  - name: _model_forward_tbo_split_inputs
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: Optional[BumpAllocator], input_data_scatter_mode: ScatterMode, layer_input_scatter_mode: ScatterMode)
    return: List[Dict]
  - name: _post_transform
    signature: (hidden_states, residual, forward_batch, **kwargs)
  - name: _model_forward_tbo_split_inputs_raw
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: Optional[BumpAllocator])
    return: List[Dict]
  - name: _model_forward_filter_inputs
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, positions: torch.Tensor, output_forward_batch: ForwardBatch, tbo_subbatch_index: int)
    return: Dict
  - name: _model_forward_tbo_merge_outputs
    signature: (output_a, output_b)
  - name: _handle_key
    signature: (name)
  - name: __init__
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: _execute
    signature: (self, name, tbo_subbatch_index: Optional[int] = None, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: dispatch
    signature: (self, **kwargs)
    return: DispatchOutput
    class: MaybeTboDeepEPDispatcher
  - name: dispatch_a
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: dispatch_b
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: combine
    signature: (self, **kwargs)
    return: torch.Tensor
    class: MaybeTboDeepEPDispatcher
  - name: combine_a
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: combine_b
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher

File: utils.py
  - name: is_hip
    signature: ()
    return: bool
  - name: is_cuda
    signature: ()
  - name: is_cuda_alike
    signature: ()
  - name: is_hpu
    signature: ()
    return: bool
  - name: is_xpu
    signature: ()
    return: bool
  - name: is_npu
    signature: ()
    return: bool
  - name: is_host_cpu_x86
    signature: ()
    return: bool
  - name: is_cpu
    signature: ()
    return: bool
  - name: get_cuda_version
    signature: ()
  - name: _check
    signature: (cc_major)
  - name: is_blackwell
    signature: ()
  - name: is_sm100_supported
    signature: (device = None)
    return: bool
  - name: is_sm90_supported
    signature: (device = None)
    return: bool
  - name: get_bool_env_var
    signature: (name: str, default: str = 'false')
    return: bool
  - name: get_int_env_var
    signature: (name: str, default: int = 0)
    return: int
  - name: support_triton
    signature: (backend: str)
    return: bool
  - name: cpu_has_amx_support
    signature: ()
  - name: use_intel_amx_backend
    signature: (layer)
  - name: is_flashinfer_available
    signature: ()
    doc: Check whether flashinfer is available.
  - name: random_uuid
    signature: ()
    return: str
  - name: set_inference_mode
    signature: (mode: bool)
    class: DynamicGradMode
  - name: __init__
    signature: (self, mode = True)
    class: DynamicGradMode
  - name: __new__
    signature: (cls, mode_or_orig_func = True if _ENABLE_TORCH_INFERENCE_MODE else None)
    class: DynamicGradMode
  - name: __enter__
    signature: (self)
    return: None
    class: DynamicGradMode
  - name: __exit__
    signature: (self, exc_type: Any, exc_value: Any, traceback: Any)
    return: None
    class: DynamicGradMode
  - name: clone
    signature: (self)
    return: 'DynamicGradMode'
    class: DynamicGradMode
    doc: Create a copy of this class
  - name: enable_show_time_cost
    signature: ()
  - name: __init__
    signature: (self, name, interval = 0.1, color = 0, indent = 0)
    class: TimeInfo
  - name: check
    signature: (self)
    class: TimeInfo
  - name: pretty_print
    signature: (self)
    class: TimeInfo
  - name: mark_start
    signature: (name, interval = 0.1, color = 0, indent = 0)
  - name: mark_end
    signature: (name)
  - name: calculate_time
    signature: (show = False, min_cost_ms = 0.0)
  - name: wrapper
    signature: (func)
  - name: inner_func
    signature: (*args, **kwargs)
  - name: get_available_gpu_memory
    signature: (device, gpu_id, distributed = False, empty_cache = True, cpu_group = None)
    doc: Get available memory for cuda:gpu_id device.
  - name: is_pin_memory_available
    signature: ()
    return: bool
  - name: __call__
    signature: (self, layer_id: int, prefix: str)
    return: torch.nn.Module
    class: LayerFn
  - name: make_layers
    signature: (num_hidden_layers: int, layer_fn: LayerFn, pp_rank: Optional[int] = None, pp_size: Optional[int] = None, prefix: str = '', return_tuple: bool = False, offloader_kwargs: Dict[str, Any] = {})
    return: Tuple[int, int, torch.nn.ModuleList]
    doc: Make a list of layers with the given layer function
  - name: set_random_seed
    signature: (seed: int)
    return: None
    doc: Set the random seed for all libraries.
  - name: find_process_using_port
    signature: (port: int)
    return: Optional[psutil.Process]
  - name: wait_port_available
    signature: (port: int, port_name: str, timeout_s: int = 30, raise_exception: bool = True)
    return: bool
  - name: is_port_available
    signature: (port)
    doc: Return whether a port is available.
  - name: get_free_port
    signature: ()
  - name: decode_video_base64
    signature: (video_base64)
  - name: load_audio
    signature: (audio_file: str, sr: Optional[int] = None, mono: bool = True)
    return: np.ndarray
  - name: load_image
    signature: (image_file: Union[Image.Image, str, ImageData, bytes])
    return: tuple[Image.Image, tuple[int, int]]
  - name: load_video
    signature: (video_file: Union[str, bytes], use_gpu: bool = True)
  - name: suppress_other_loggers
    signature: ()
  - name: assert_pkg_version
    signature: (pkg: str, min_version: str, message: str)
  - name: kill_process_tree
    signature: (parent_pid, include_parent: bool = True, skip_pid: int = None)
    doc: Kill the process and all its child processes.
  - name: monkey_patch_p2p_access_check
    signature: ()
    doc: Monkey patch the slow p2p access check.
  - name: monkey_patch_vllm_gguf_config
    signature: ()
  - name: get_quant_method_with_embedding_replaced
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional['QuantizeMethodBase']
  - name: set_ulimit
    signature: (target_soft_limit = 65535)
  - name: add_api_key_middleware
    signature: (app, api_key: str)
  - name: authentication
    signature: (request, call_next)
  - name: prepare_model_and_tokenizer
    signature: (model_path: str, tokenizer_path: str)
  - name: configure_logger
    signature: (server_args, prefix: str = '')
  - name: replace_submodule
    signature: (model: nn.Module, module_name: str, new_module: nn.Module)
    return: nn.Module
    doc: Replace a submodule in a model with a new module.
  - name: set_weight_attrs
    signature: (weight: torch.Tensor, weight_attrs: Optional[Dict[str, Any]])
    doc: Set attributes on a weight tensor.
  - name: broadcast_pyobj
    signature: (data: List[Any], rank: int, dist_group: Optional[torch.distributed.ProcessGroup] = None, src: int = 0, force_cpu_device: bool = True)
    doc: Broadcast inputs from src rank to all other ranks with torch.dist backend.
  - name: point_to_point_pyobj
    signature: (data: List[Any], rank: int, group: Optional[torch.distributed.ProcessGroup] = None, src: int = 0, dst: int = 1)
    doc: Send data from src to dst in group using DeviceToDevice communication.
  - name: pytorch_profile
    signature: (name, func, *args, data_size = -1)
    doc: Args:
  - name: get_zmq_socket
    signature: (context: zmq.Context, socket_type: zmq.SocketType, endpoint: str, bind: bool)
  - name: set_send_opt
    signature: ()
  - name: set_recv_opt
    signature: ()
  - name: dump_to_file
    signature: (dirpath, name, value)
  - name: is_triton_3
    signature: ()
  - name: maybe_torch_compile
    signature: (*args, **kwargs)
    doc: torch.compile does not work for triton 2.2.0, which is needed in xlm1's jax.
  - name: decorator
    signature: (func)
  - name: delete_directory
    signature: (dirpath)
  - name: set_prometheus_multiproc_dir
    signature: ()
  - name: add_prometheus_middleware
    signature: (app)
  - name: bind_port
    signature: (port)
    doc: Bind to a specific port, assuming it's available.
  - name: get_amdgpu_memory_capacity
    signature: ()
  - name: get_device_sm
    signature: ()
  - name: get_nvgpu_memory_capacity
    signature: ()
  - name: get_hpu_memory_capacity
    signature: ()
  - name: get_npu_memory_capacity
    signature: ()
  - name: get_device_memory_capacity
    signature: (device: str = None)
  - name: init_custom_process_group
    signature: (backend = None, init_method = None, timeout = None, world_size = -1, rank = -1, store = None, group_name = None, pg_options = None)
  - name: crash_on_warnings
    signature: ()
  - name: print_warning_once
    signature: (msg: str)
    return: None
  - name: print_info_once
    signature: (msg: str)
    return: None
  - name: get_device_name
    signature: (device_id: int = 0)
    return: str
  - name: is_habana_available
    signature: ()
    return: bool
  - name: get_device
    signature: (device_id: Optional[int] = None)
    return: str
  - name: get_device_count
    signature: ()
    return: int
  - name: get_device_core_count
    signature: (device_id: int = 0)
    return: int
  - name: get_device_capability
    signature: (device_id: int = 0)
    return: Tuple[int, int]
  - name: get_npu_compiler_config
    signature: ()
  - name: get_compiler_backend
    signature: ()
    return: str
  - name: supports_custom_op
    signature: ()
    return: bool
  - name: direct_register_custom_op
    signature: (op_name: str, op_func: Callable, mutates_args: List[str], fake_impl: Optional[Callable] = None, target_lib: Optional[Library] = None)
    doc: `torch.library.custom_op` can have significant overhead because it
  - name: set_gpu_proc_affinity
    signature: (tp_size: int, nnodes: int, gpu_id: int)
  - name: disable_request_logging
    signature: ()
    return: bool
  - name: dataclass_to_string_truncated
    signature: (data, max_length = 2048, skip_names: Optional[Set[str]] = None)
  - name: permute_weight
    signature: (x: torch.Tensor)
    return: torch.Tensor
  - name: serialize
    signature: (obj, output_str: bool = False)
    class: MultiprocessingSerializer
    doc: Serialize a Python object using ForkingPickler.
  - name: deserialize
    signature: (data)
    class: MultiprocessingSerializer
    doc: Deserialize a previously serialized object.
  - name: debug_timing
    signature: (func)
  - name: wrapper
    signature: (*args, **kwargs)
  - name: nullable_str
    signature: (val: str)
  - name: pyspy_dump_schedulers
    signature: ()
    doc: py-spy dump on all scheduler in a local node.
  - name: kill_itself_when_parent_died
    signature: ()
  - name: set_uvicorn_logging_configs
    signature: ()
  - name: get_ip
    signature: ()
    return: str
  - name: get_open_port
    signature: ()
    return: int
  - name: is_valid_ipv6_address
    signature: (address: str)
    return: bool
  - name: maybe_wrap_ipv6_address
    signature: (address: str)
    return: str
  - name: format_tcp_address
    signature: (ip: str, port: int)
    return: str
  - name: configure_ipv6
    signature: (dist_init_addr)
  - name: launch_dummy_health_check_server
    signature: (host, port, enable_metrics)
  - name: health
    signature: ()
    doc: Check the health of the http server.
  - name: health_generate
    signature: ()
    doc: Check the health of the http server.
  - name: create_checksum
    signature: (directory: str)
  - name: set_cuda_arch
    signature: ()
  - name: next_power_of_2
    signature: (n: int)
  - name: round_up
    signature: (x: int, y: int)
    return: int
  - name: __enter__
    signature: (self)
    class: EmptyContextManager
  - name: __exit__
    signature: (self, exc_type, exc_value, traceback)
    class: EmptyContextManager
  - name: empty_context
    signature: (*args, **kwargs)
  - name: add_prefix
    signature: (name: str, prefix: str)
    return: str
    doc: Add a weight path prefix to a module name.
  - name: is_remote_url
    signature: (url: Union[str, Path])
    return: bool
    doc: Check if the URL is a remote URL of the format:
  - name: parse_connector_type
    signature: (url: str)
    return: str
    doc: Parse the connector type from the URL of the format:
  - name: retry
    signature: (fn, max_retry: int, initial_delay: float = 2.0, max_delay: float = 60.0, should_retry: Callable[[Any], bool] = lambda e: True)
  - name: flatten_nested_list
    signature: (nested_list)
  - name: is_non_idle_and_non_empty
    signature: (forward_mode, hidden_states)
  - name: fast_topk
    signature: (values, topk, dim)
  - name: bind_or_assign
    signature: (target, source)
  - name: get_local_ip_auto
    signature: ()
    return: str
  - name: get_local_ip_by_nic
    signature: (interface: str)
    return: str
  - name: get_local_ip_by_remote
    signature: ()
    return: str
  - name: is_page_size_one
    signature: (server_args)
  - name: is_no_spec_infer_or_topk_one
    signature: (server_args)
  - name: is_fa3_default_architecture
    signature: (hf_config)
  - name: __init__
    signature: (self, buffer_size: int, dtype, device)
    class: BumpAllocator
  - name: allocate
    signature: (self, size: int)
    class: BumpAllocator
  - name: log_info_on_rank0
    signature: (logger, msg)
  - name: load_json_config
    signature: (data: str)
  - name: dispose_tensor
    signature: (x: torch.Tensor)
  - name: __init__
    signature: (self)
    class: Withable
  - name: value
    signature: (self)
    return: T
    class: Withable
  - name: with_value
    signature: (self, new_value: T)
    class: Withable
  - name: require_mlp_tp_gather
    signature: (server_args)
    doc: Check if the input of MLP is obtained by all-gather rather than all-reduce. This only happens when each MLP TP group contains multiple attention DP groups.
  - name: require_attn_tp_gather
    signature: (server_args)
    doc: Check if the input of attention is scattered.
  - name: require_gathered_buffer
    signature: (server_args)
  - name: require_mlp_sync
    signature: (server_args)
  - name: find_local_repo_dir
    signature: (repo_id: str, revision: Optional[str] = None)
    return: Optional[str]
  - name: read_system_prompt_from_file
    signature: (model_name: str)
    return: str
    doc: Read system prompt from a file in the HuggingFace cache directory.
  - name: bind_or_assign
    signature: (target, source)
  - name: prepack_weight_if_needed
    signature: (weight)
  - name: dim_is_supported
    signature: (weight)
  - name: _process_weight_after_loading
    signature: (module, weight_names, transpose_dims = None)
    return: None
  - name: __init__
    signature: (self, weight_names, transpose_dims = None)
    class: PackWeightMethod
  - name: process_weights_after_loading
    signature: (self, module)
    return: None
    class: PackWeightMethod
  - name: __init__
    signature: (self, creator: Callable)
    class: LazyValue
  - name: value
    signature: (self)
    class: LazyValue
  - name: dynamic_import
    signature: (func_path: str)
  - name: gc_object_counts
    signature: ()
  - name: configure_gc_warning
    signature: (warn_threshold_secs)
  - name: gc_callback
    signature: (phase, info)
  - name: freeze_gc
    signature: (context: str)
  - name: configure_gc_logger
    signature: ()
  - name: gc_callback
    signature: (phase, info)
  - name: align
    signature: (x: int, y: int)
    return: int
  - name: ceil_div
    signature: (x: int, y: int)
    return: int
  - name: parse_lscpu_topology
    signature: ()
  - name: get_physical_cpus_by_numa
    signature: ()
  - name: get_cpu_ids_by_node
    signature: ()
  - name: is_shm_available
    signature: (dtype, world_size, local_size)
  - name: lru_cache_frozenset
    signature: (maxsize = 128)
  - name: _to_hashable
    signature: (o)
  - name: decorator
    signature: (func)
  - name: wrapper
    signature: (*args, **kwargs)
  - name: apply_module_patch
    signature: (target_module, target_function, wrappers)
  - name: parse_module_path
    signature: (module_path, function_name, create_dummy)
  - name: create_dummy_module
    signature: (full_path, parent = None)
    doc: Create and register a placeholder module
  - name: create_placeholder_function
    signature: (func_name)
    doc: Create dummy function that raises when called
  - name: placeholder
    signature: (*args, **kwargs)
  - name: mxfp_supported
    signature: ()
    doc: Returns whether the current platform supports MX types.
  - name: __init__
    signature: (self, initial: int = 0)
    class: ConcurrentCounter
    doc: Initialize the counter with an optional initial value.
  - name: value
    signature: (self)
    return: int
    class: ConcurrentCounter
    doc: Return the current value of the counter.
  - name: __repr__
    signature: (self)
    return: str
    class: ConcurrentCounter
    doc: Return an informative string representation of the counter.
  - name: increment
    signature: (self, n: int = 1, notify_all: bool = True)
    class: ConcurrentCounter
    doc: Atomically increment the counter by a given amount and notify all waiters.
  - name: decrement
    signature: (self, n: int = 1, notify_all: bool = True)
    class: ConcurrentCounter
    doc: Atomically decrement the counter by a given amount and notify all waiters.
  - name: wait_for
    signature: (self, condition: Callable[[int], bool])
    class: ConcurrentCounter
    doc: Asynchronously wait until the counter satisfies a given condition.
  - name: wait_for_zero
    signature: (self)
    class: ConcurrentCounter
    doc: Asynchronously wait until the counter reaches zero.
  - name: is_triton_kernels_available
    signature: ()
    return: bool
  - name: check_cuda_result
    signature: (raw_output)

File: warmup.py
  - name: warmup
    signature: (name: str)
    return: callable
  - name: decorator
    signature: (fn: callable)
  - name: execute_warmups
    signature: (disaggregation_mode: str, warmup_names: List[str], tokenizer_manager: TokenizerManager)
  - name: voice_chat
    signature: (disaggregation_mode: str, tokenizer_manager: TokenizerManager)
