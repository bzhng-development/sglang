AST Function Index
Root: /Users/vincentzed/Documents/Github/open_source/sglang/python/sglang/srt
Excluded directories:

File: distributed/__init__.py
  (no function definitions found)
File: distributed/communication_op.py
  - name: tensor_model_parallel_all_reduce
    signature: (input_: torch.Tensor)
    return: torch.Tensor
    doc: All-reduce the input tensor across model parallel group.
  - name: tensor_model_parallel_all_gather
    signature: (input_: torch.Tensor, dim: int = -1)
    return: torch.Tensor
    doc: All-gather the input tensor across model parallel group.
  - name: tensor_model_parallel_gather
    signature: (input_: torch.Tensor, dst: int = 0, dim: int = -1)
    return: Optional[torch.Tensor]
    doc: Gather the input tensor across model parallel group.
  - name: broadcast_tensor_dict
    signature: (tensor_dict: Optional[Dict[Any, Union[torch.Tensor, Any]]] = None, src: int = 0)

File: distributed/device_communicators/cuda_wrapper.py
  - name: find_loaded_library
    signature: (lib_name)
    return: Optional[str]
    doc: According to according to https://man7.org/linux/man-pages/man5/proc_pid_maps.5.html,
  - name: __init__
    signature: (self, so_file: Optional[str] = None)
    class: CudaRTLibrary
  - name: CUDART_CHECK
    signature: (self, result: cudaError_t)
    return: None
    class: CudaRTLibrary
  - name: cudaGetErrorString
    signature: (self, error: cudaError_t)
    return: str
    class: CudaRTLibrary
  - name: cudaSetDevice
    signature: (self, device: int)
    return: None
    class: CudaRTLibrary
  - name: cudaDeviceSynchronize
    signature: (self)
    return: None
    class: CudaRTLibrary
  - name: cudaDeviceReset
    signature: (self)
    return: None
    class: CudaRTLibrary
  - name: cudaMalloc
    signature: (self, size: int)
    return: ctypes.c_void_p
    class: CudaRTLibrary
  - name: cudaFree
    signature: (self, devPtr: ctypes.c_void_p)
    return: None
    class: CudaRTLibrary
  - name: cudaMemset
    signature: (self, devPtr: ctypes.c_void_p, value: int, count: int)
    return: None
    class: CudaRTLibrary
  - name: cudaMemcpy
    signature: (self, dst: ctypes.c_void_p, src: ctypes.c_void_p, count: int)
    return: None
    class: CudaRTLibrary
  - name: cudaIpcGetMemHandle
    signature: (self, devPtr: ctypes.c_void_p)
    return: cudaIpcMemHandle_t
    class: CudaRTLibrary
  - name: cudaIpcOpenMemHandle
    signature: (self, handle: cudaIpcMemHandle_t)
    return: ctypes.c_void_p
    class: CudaRTLibrary

File: distributed/device_communicators/custom_all_reduce.py
  - name: _can_p2p
    signature: (rank: int, world_size: int)
    return: bool
  - name: __init__
    signature: (self, group: ProcessGroup, device: Union[int, str, torch.device], max_size = _MAX_CAR_SIZE)
    return: None
    class: CustomAllreduce
    doc: Args:
  - name: create_shared_buffer
    signature: (size_in_bytes: int, group: Optional[ProcessGroup] = None)
    return: List[int]
    class: CustomAllreduce
    doc: Creates a shared buffer and returns a list of pointers
  - name: free_shared_buffer
    signature: (pointers: List[int], group: Optional[ProcessGroup] = None)
    return: None
    class: CustomAllreduce
  - name: capture
    signature: (self)
    class: CustomAllreduce
    doc: The main responsibility of this context manager is the
  - name: _get_ipc_meta
    signature: (self, inp: torch.Tensor)
    class: CustomAllreduce
  - name: _gather_ipc_meta
    signature: (self, shard_data)
    class: CustomAllreduce
  - name: register_buffer
    signature: (self, inp: torch.Tensor)
    class: CustomAllreduce
  - name: register_graph_buffers
    signature: (self)
    class: CustomAllreduce
  - name: should_custom_ar
    signature: (self, inp: torch.Tensor)
    class: CustomAllreduce
  - name: all_reduce_reg
    signature: (self, inp: torch.Tensor, out: torch.Tensor = None)
    class: CustomAllreduce
  - name: all_reduce_unreg
    signature: (self, inp: torch.Tensor, out: torch.Tensor = None)
    class: CustomAllreduce
  - name: all_reduce
    signature: (self, inp: torch.Tensor, *, out: torch.Tensor = None, registered: bool = False)
    class: CustomAllreduce
    doc: Performs an out-of-place all reduce.
  - name: custom_all_reduce
    signature: (self, input: torch.Tensor)
    return: Optional[torch.Tensor]
    class: CustomAllreduce
    doc: The main allreduce API that provides support for cuda graph.
  - name: close
    signature: (self)
    class: CustomAllreduce
  - name: __del__
    signature: (self)
    class: CustomAllreduce

File: distributed/device_communicators/custom_all_reduce_utils.py
  - name: update_environment_variables
    signature: (envs: Dict[str, str])
  - name: producer
    signature: (batch_src: Sequence[int], producer_queue, consumer_queue, result_queue, cuda_visible_devices: Optional[str] = None)
  - name: consumer
    signature: (batch_tgt: Sequence[int], producer_queue, consumer_queue, result_queue, cuda_visible_devices: Optional[str] = None)
  - name: can_actually_p2p
    signature: (batch_src: Sequence[int], batch_tgt: Sequence[int])
    return: Sequence[bool]
    doc: Usually, checking if P2P access is enabled can be done by
  - name: gpu_p2p_access_check
    signature: (src: int, tgt: int)
    return: bool
    doc: Check if GPU src can access GPU tgt.
  - name: with_nvml_context
    signature: (fn: Callable[_P, _R])
    return: Callable[_P, _R]
  - name: wrapper
    signature: (*args: _P.args, **kwargs: _P.kwargs)
    return: _R
  - name: is_full_nvlink
    signature: (physical_device_ids: List[int], world_size: int)
    return: bool
  - name: is_weak_contiguous
    signature: (inp: torch.Tensor)

File: distributed/device_communicators/hpu_communicator.py
  - name: __init__
    signature: (self, group: ProcessGroup)
    class: HpuCommunicator
  - name: all_reduce
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: HpuCommunicator
  - name: all_gather
    signature: (self, x: torch.Tensor, dim: int = -1)
    return: torch.Tensor
    class: HpuCommunicator

File: distributed/device_communicators/npu_communicator.py
  - name: __init__
    signature: (self, group: ProcessGroup)
    class: NpuCommunicator
  - name: all_reduce
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: NpuCommunicator
  - name: all_gather
    signature: (self, x: torch.Tensor, dim: int = -1)
    return: torch.Tensor
    class: NpuCommunicator

File: distributed/device_communicators/pymscclpp.py
  - name: mscclpp_is_weak_contiguous
    signature: (inp: torch.Tensor)
  - name: mscclpp_convert_to_bytes
    signature: (size_str)
    doc: Converts a human-readable size string (e.g., "1MB", "2.5kb", "3 GB")
  - name: mscclpp_bench_time
    signature: (func, test_niter: int = 10, warmup_niter: int = 2)
  - name: __init__
    signature: (self, group: ProcessGroup, device: Union[int, str, torch.device], max_bytes = _MAX_BYTES)
    return: None
    class: PyMscclppCommunicator
    doc: Args:
  - name: pre_tune_config
    signature: (self, dtype = torch.bfloat16)
    return: bool
    class: PyMscclppCommunicator
  - name: should_mscclpp_allreduce
    signature: (self, inp: torch.Tensor, op: ReduceOp = ReduceOp.SUM)
    return: bool
    class: PyMscclppCommunicator
  - name: all_reduce
    signature: (self, tensor: torch.Tensor, op: ReduceOp = ReduceOp.SUM)
    class: PyMscclppCommunicator
  - name: change_state
    signature: (self, enable: Optional[bool] = None)
    class: PyMscclppCommunicator

File: distributed/device_communicators/pynccl.py
  - name: __init__
    signature: (self, group: Union[ProcessGroup, StatelessProcessGroup], device: Union[int, str, torch.device], library_path: Optional[str] = None)
    class: PyNcclCommunicator
    doc: Args:
  - name: all_reduce
    signature: (self, tensor: torch.Tensor, op: ReduceOp = ReduceOp.SUM, stream = None)
    class: PyNcclCommunicator
  - name: all_gather
    signature: (self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, stream = None, sizes: Optional[list[int]] = None)
    class: PyNcclCommunicator
  - name: reduce_scatter
    signature: (self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, op: ReduceOp = ReduceOp.SUM, stream = None, sizes: Optional[list[int]] = None)
    class: PyNcclCommunicator
  - name: send
    signature: (self, tensor: torch.Tensor, dst: int, stream = None)
    class: PyNcclCommunicator
  - name: recv
    signature: (self, tensor: torch.Tensor, src: int, stream = None)
    class: PyNcclCommunicator
  - name: broadcast
    signature: (self, tensor: torch.Tensor, src: int, stream = None)
    class: PyNcclCommunicator
  - name: register_comm_window_raw
    signature: (self, ptr: int, size: int)
    class: PyNcclCommunicator
  - name: deregister_comm_window
    signature: (self, window)
    class: PyNcclCommunicator
  - name: group_start
    signature: (self)
    class: PyNcclCommunicator
  - name: group_end
    signature: (self)
    class: PyNcclCommunicator
  - name: change_state
    signature: (self, enable: Optional[bool] = None, stream: Optional[torch.cuda.Stream] = None)
    class: PyNcclCommunicator
    doc: A context manager to change the state of the communicator.

File: distributed/device_communicators/pynccl_allocator.py
  - name: is_symmetric_memory_enabled
    signature: ()
  - name: set_graph_pool_id
    signature: (graph_pool_id)
  - name: get_nccl_mem_pool
    signature: ()
  - name: __init__
    signature: (self, group_coordinator: GroupCoordinator)
    class: use_symmetric_memory
  - name: __enter__
    signature: (self)
    class: use_symmetric_memory
  - name: tag
    signature: (self, tensor: torch.Tensor)
    class: use_symmetric_memory
  - name: __exit__
    signature: (self, exc_type, exc_val, exc_tb)
    class: use_symmetric_memory

File: distributed/device_communicators/pynccl_wrapper.py
  - name: find_nccl_library
    signature: ()
    return: str
    doc: We either use the library file specified by the `SGLANG_NCCL_SO_PATH`
  - name: from_torch
    signature: (cls, dtype: torch.dtype)
    return: int
    class: ncclDataTypeEnum
  - name: from_torch
    signature: (cls, op: ReduceOp)
    return: int
    class: ncclRedOpTypeEnum
  - name: __init__
    signature: (self, so_file: Optional[str] = None)
    class: NCCLLibrary
  - name: ncclGetErrorString
    signature: (self, result: ncclResult_t)
    return: str
    class: NCCLLibrary
  - name: NCCL_CHECK
    signature: (self, result: ncclResult_t)
    return: None
    class: NCCLLibrary
  - name: ncclGetRawVersion
    signature: (self)
    return: int
    class: NCCLLibrary
  - name: ncclGetVersion
    signature: (self)
    return: str
    class: NCCLLibrary
  - name: ncclGetUniqueId
    signature: (self)
    return: ncclUniqueId
    class: NCCLLibrary
  - name: ncclCommInitRank
    signature: (self, world_size: int, unique_id: ncclUniqueId, rank: int)
    return: ncclComm_t
    class: NCCLLibrary
  - name: ncclAllReduce
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclReduce
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclReduceScatter
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclAllGather
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclSend
    signature: (self, sendbuff: buffer_type, count: int, datatype: int, dest: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclRecv
    signature: (self, recvbuff: buffer_type, count: int, datatype: int, src: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclBroadcast
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclCommDestroy
    signature: (self, comm: ncclComm_t)
    return: None
    class: NCCLLibrary
  - name: ncclCommWindowRegister
    signature: (self, comm: ncclComm_t, buff: buffer_type, size: int, win_flags: int)
    return: ncclWindow_t
    class: NCCLLibrary
  - name: ncclCommWindowDeregister
    signature: (self, comm: ncclComm_t, window: ncclWindow_t)
    return: None
    class: NCCLLibrary
  - name: ncclGroupStart
    signature: (self)
    return: None
    class: NCCLLibrary
  - name: ncclGroupEnd
    signature: (self)
    return: None
    class: NCCLLibrary

File: distributed/device_communicators/quick_all_reduce.py
  - name: qr_rocm_arch_available
    signature: ()
  - name: __init__
    signature: (self, group: ProcessGroup, device: Union[int, str, torch.device])
    return: None
    class: QuickAllReduce
    doc: Custom allreduce provides non-destructive acceleration and is
  - name: init_quick_all_reduce
    signature: (self)
    class: QuickAllReduce
  - name: create_shared_buffer
    signature: (self)
    class: QuickAllReduce
    doc: Creates a shared buffer for quickreduce.
  - name: should_quick_allreduce
    signature: (self, inp: torch.Tensor)
    class: QuickAllReduce
    doc: Check if quickreduce is available
  - name: quick_all_reduce
    signature: (self, inp: torch.Tensor, *, out: torch.Tensor = None)
    class: QuickAllReduce
    doc: Performs an out-of-place custom quick all reduce.
  - name: close
    signature: (self)
    class: QuickAllReduce
  - name: __del__
    signature: (self)
    class: QuickAllReduce

File: distributed/device_communicators/shm_broadcast.py
  - name: __init__
    signature: (self, n_reader: int, max_chunk_bytes: int, max_chunks: int, name: Optional[str] = None)
    class: ShmRingBuffer
    doc: A shared memory ring buffer implementation for broadcast communication.
  - name: __reduce__
    signature: (self)
    class: ShmRingBuffer
  - name: __del__
    signature: (self)
    class: ShmRingBuffer
  - name: get_data
    signature: (self, current_idx: int)
    class: ShmRingBuffer
  - name: get_metadata
    signature: (self, current_idx: int)
    class: ShmRingBuffer
  - name: __init__
    signature: (self, n_reader, n_local_reader, local_reader_ranks: Optional[List[int]] = None, max_chunk_bytes: int = 1024 * 1024 * 10, max_chunks: int = 10, connect_ip: Optional[str] = None)
    class: MessageQueue
  - name: export_handle
    signature: (self)
    return: Handle
    class: MessageQueue
  - name: create_from_handle
    signature: (handle: Handle, rank)
    return: 'MessageQueue'
    class: MessageQueue
  - name: wait_until_ready
    signature: (self)
    class: MessageQueue
    doc: This is a collective operation. All processes (including the
  - name: acquire_write
    signature: (self)
    class: MessageQueue
  - name: acquire_read
    signature: (self)
    class: MessageQueue
  - name: enqueue
    signature: (self, obj)
    class: MessageQueue
  - name: dequeue
    signature: (self)
    class: MessageQueue
  - name: broadcast_object
    signature: (self, obj = None)
    class: MessageQueue
  - name: create_from_process_group
    signature: (pg: ProcessGroup, max_chunk_bytes, max_chunks, writer_rank = 0)
    return: 'MessageQueue'
    class: MessageQueue

File: distributed/device_communicators/xpu_communicator.py
  - name: __init__
    signature: (self, group: ProcessGroup)
    class: XpuCommunicator
  - name: all_reduce
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: XpuCommunicator
  - name: gather
    signature: (self, input_: torch.Tensor, rank_in_group: int, dst: int = 0, dim: int = -1)
    class: XpuCommunicator

File: distributed/naive_distributed.py
  - name: __init__
    signature: (self, rank: int, world_size: int, rendezvous: str)
    class: NaiveDistributed
  - name: get_rank
    signature: (self)
    class: NaiveDistributed
  - name: get_world_size
    signature: (self)
    class: NaiveDistributed
  - name: scatter
    signature: (self, tensor: torch.Tensor, scatter_list: List[torch.Tensor], src: int = 0)
    class: NaiveDistributed
  - name: all_gather_object
    signature: (self, obj: Any)
    return: List[Any]
    class: NaiveDistributed
  - name: _get_path
    signature: (interesting_rank: int)
    class: NaiveDistributed
  - name: _read_one
    signature: (interesting_rank: int)
    class: NaiveDistributed
  - name: barrier
    signature: (self)
    class: NaiveDistributed
  - name: get_naive_distributed
    signature: ()
  - name: set_naive_distributed
    signature: (instance: NaiveDistributed)

File: distributed/parallel_state.py
  - name: _split_tensor_dict
    signature: (tensor_dict: Dict[str, Union[torch.Tensor, Any]])
    return: Tuple[List[Tuple[str, Any]], List[torch.Tensor]]
    doc: Split the tensor dictionary into two parts:
  - name: _get_unique_name
    signature: (name: str)
    return: str
    doc: Get a unique name for the group.
  - name: _register_group
    signature: (group: 'GroupCoordinator')
    return: None
  - name: inplace_all_reduce
    signature: (tensor: torch.Tensor, group_name: str)
    return: None
  - name: inplace_all_reduce_fake
    signature: (tensor: torch.Tensor, group_name: str)
    return: None
  - name: outplace_all_reduce
    signature: (tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str)
    return: torch.Tensor
  - name: outplace_all_reduce_fake
    signature: (tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str)
    return: torch.Tensor
  - name: reg_all_gather_into_tensor
    signature: (output: torch.Tensor, input: torch.Tensor, group_name: str)
    return: None
  - name: reg_all_gather_into_tensor_fake
    signature: (output: torch.Tensor, input: torch.Tensor, group_name: str)
    return: None
  - name: __init__
    signature: (self, group_ranks: List[List[int]], local_rank: int, torch_distributed_backend: Union[str, Backend], use_pynccl: bool, use_pymscclpp: bool, use_custom_allreduce: bool, use_hpu_communicator: bool, use_xpu_communicator: bool, use_npu_communicator: bool, use_message_queue_broadcaster: bool = False, group_name: Optional[str] = None)
    class: GroupCoordinator
  - name: __repr__
    signature: (self)
    class: GroupCoordinator
  - name: first_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return the global rank of the first process in the group
  - name: last_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return the global rank of the last process in the group
  - name: is_first_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return whether the caller is the first process in the group
  - name: is_last_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return whether the caller is the last process in the group
  - name: next_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return the global rank of the process that follows the caller
  - name: prev_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return the global rank of the process that precedes the caller
  - name: graph_capture
    signature: (self, graph_capture_context: Optional[GraphCaptureContext] = None)
    class: GroupCoordinator
  - name: all_reduce
    signature: (self, input_: torch.Tensor)
    return: torch.Tensor
    class: GroupCoordinator
    doc: User-facing all-reduce function before we actually call the
  - name: _all_reduce_out_place
    signature: (self, input_: torch.Tensor, outplace_all_reduce_method: str)
    return: torch.Tensor
    class: GroupCoordinator
  - name: _all_reduce_in_place
    signature: (self, input_: torch.Tensor)
    return: None
    class: GroupCoordinator
  - name: reduce_scatter_tensor
    signature: (self, output: torch.Tensor, input: torch.Tensor)
    return: None
    class: GroupCoordinator
  - name: reduce_scatter
    signature: (self, output: torch.Tensor, input_list: List[torch.Tensor])
    return: None
    class: GroupCoordinator
  - name: reduce_scatterv
    signature: (self, input_: torch.Tensor, output: Optional[torch.Tensor] = None, sizes: Optional[List[int]] = None)
    return: torch.Tensor
    class: GroupCoordinator
  - name: _all_gather_into_tensor
    signature: (self, output: torch.Tensor, input: torch.Tensor)
    class: GroupCoordinator
  - name: all_gather_into_tensor
    signature: (self, output: torch.Tensor, input: torch.Tensor)
    class: GroupCoordinator
  - name: all_gather
    signature: (self, input_: torch.Tensor, dim: int = -1, output_tensor_list: Optional[List[torch.Tensor]] = None)
    return: torch.Tensor
    class: GroupCoordinator
  - name: all_gatherv
    signature: (self, input_: Union[torch.Tensor, List[torch.Tensor]], sizes: Optional[List[int]] = None)
    return: Union[torch.Tensor, List[torch.Tensor]]
    class: GroupCoordinator
    doc: Supports varying sizes per rank and input tensor list.
  - name: _all_gather_single
    signature: (input_: torch.Tensor, sizes: Optional[List[int]] = None)
    class: GroupCoordinator
  - name: gather
    signature: (self, input_: torch.Tensor, dst: int = 0, dim: int = -1)
    return: Optional[torch.Tensor]
    class: GroupCoordinator
    doc: NOTE: We assume that the input tensor is on the same device across
  - name: broadcast
    signature: (self, input_: torch.Tensor, src: int = 0)
    class: GroupCoordinator
    doc: Broadcast the input tensor.
  - name: broadcast_object
    signature: (self, obj: Optional[Any] = None, src: int = 0)
    class: GroupCoordinator
    doc: Broadcast the input object.
  - name: broadcast_object_list
    signature: (self, obj_list: List[Any], src: int = 0, group: Optional[ProcessGroup] = None)
    class: GroupCoordinator
    doc: Broadcast the input object list.
  - name: all_gather_object
    signature: (self, obj: Any)
    return: List[Any]
    class: GroupCoordinator
  - name: send_object
    signature: (self, obj: Any, dst: int)
    return: None
    class: GroupCoordinator
    doc: Send the input object list to the destination rank.
  - name: recv_object
    signature: (self, src: int)
    return: Any
    class: GroupCoordinator
    doc: Receive the input object list from the source rank.
  - name: broadcast_tensor_dict
    signature: (self, tensor_dict: Optional[Dict[str, Union[torch.Tensor, Any]]] = None, src: int = 0, group: Optional[ProcessGroup] = None, metadata_group: Optional[ProcessGroup] = None)
    return: Optional[Dict[str, Union[torch.Tensor, Any]]]
    class: GroupCoordinator
    doc: Broadcast the input tensor dictionary.
  - name: send_tensor_dict
    signature: (self, tensor_dict: Dict[str, Union[torch.Tensor, Any]], dst: Optional[int] = None, all_gather_group: Optional['GroupCoordinator'] = None)
    return: Optional[Dict[str, Union[torch.Tensor, Any]]]
    class: GroupCoordinator
    doc: Send the input tensor dictionary.
  - name: recv_tensor_dict
    signature: (self, src: Optional[int] = None, all_gather_group: Optional['GroupCoordinator'] = None)
    return: Optional[Dict[str, Union[torch.Tensor, Any]]]
    class: GroupCoordinator
    doc: Recv the input tensor dictionary.
  - name: barrier
    signature: (self)
    class: GroupCoordinator
    doc: Barrier synchronization among the group.
  - name: send
    signature: (self, tensor: torch.Tensor, dst: Optional[int] = None)
    return: None
    class: GroupCoordinator
    doc: Sends a tensor to the destination rank in a non-blocking way
  - name: recv
    signature: (self, size: torch.Size, dtype: torch.dtype, src: Optional[int] = None)
    return: torch.Tensor
    class: GroupCoordinator
    doc: Receives a tensor from the source rank.
  - name: destroy
    signature: (self)
    class: GroupCoordinator
  - name: get_world_group
    signature: ()
    return: GroupCoordinator
  - name: init_world_group
    signature: (ranks: List[int], local_rank: int, backend: str)
    return: GroupCoordinator
  - name: init_model_parallel_group
    signature: (group_ranks: List[List[int]], local_rank: int, backend: str, use_custom_allreduce: Optional[bool] = None, use_message_queue_broadcaster: bool = False, group_name: Optional[str] = None, use_mscclpp_allreduce: Optional[bool] = None)
    return: GroupCoordinator
  - name: set_pdmux_status
    signature: (enable_prefill_multiplexing: bool)
  - name: get_tp_group
    signature: ()
    return: GroupCoordinator
  - name: get_moe_ep_group
    signature: ()
    return: GroupCoordinator
  - name: get_moe_tp_group
    signature: ()
    return: GroupCoordinator
  - name: get_pp_group
    signature: ()
    return: GroupCoordinator
  - name: graph_capture
    signature: ()
    doc: `graph_capture` is a context manager which should surround the code that
  - name: set_custom_all_reduce
    signature: (enable: bool)
  - name: set_mscclpp_all_reduce
    signature: (enable: bool)
  - name: init_distributed_environment
    signature: (world_size: int = -1, rank: int = -1, distributed_init_method: str = 'env://', local_rank: int = -1, backend: str = 'nccl', timeout: Optional[int] = None)
  - name: initialize_model_parallel
    signature: (tensor_model_parallel_size: int = 1, expert_model_parallel_size: int = 1, pipeline_model_parallel_size: int = 1, backend: Optional[str] = None, duplicate_tp_group: bool = False)
    return: None
    doc: Initialize model parallel groups.
  - name: ensure_model_parallel_initialized
    signature: (tensor_model_parallel_size: int, expert_model_parallel_size: int, pipeline_model_parallel_size: int, backend: Optional[str] = None)
    return: None
    doc: Helper to initialize model parallel groups if they are not initialized,
  - name: model_parallel_is_initialized
    signature: ()
    doc: Check if tensor and pipeline parallel groups are initialized.
  - name: patch_tensor_parallel_group
    signature: (tp_group: GroupCoordinator)
    doc: Patch the tp group temporarily until this function ends.
  - name: get_tensor_model_parallel_world_size
    signature: ()
    doc: Return world size for the tensor model parallel group.
  - name: get_tensor_model_parallel_rank
    signature: ()
    doc: Return my rank for the tensor model parallel group.
  - name: get_moe_expert_parallel_world_size
    signature: ()
    doc: Return world size for the moe expert parallel group.
  - name: get_moe_expert_parallel_rank
    signature: ()
    doc: Return my rank for the moe expert parallel group.
  - name: get_moe_tensor_parallel_world_size
    signature: ()
    doc: Return world size for the moe tensor parallel group.
  - name: get_moe_tensor_parallel_rank
    signature: ()
    doc: Return my rank for the moe tensor parallel group.
  - name: destroy_model_parallel
    signature: ()
    doc: Set the groups to none and destroy them.
  - name: destroy_distributed_environment
    signature: ()
  - name: cleanup_dist_env_and_memory
    signature: (shutdown_ray: bool = False)
  - name: in_the_same_node_as
    signature: (pg: ProcessGroup, source_rank: int = 0)
    return: List[bool]
    doc: This is a collective operation that returns if each rank is in the same node
  - name: monkey_patch_vllm_parallel_state
    signature: (reverse: bool = False)

File: distributed/utils.py
  - name: ensure_divisibility
    signature: (numerator, denominator)
    doc: Ensure that numerator is divisible by the denominator.
  - name: divide
    signature: (numerator, denominator)
    doc: Ensure that numerator is divisible by the denominator and return
  - name: split_tensor_along_last_dim
    signature: (tensor: torch.Tensor, num_partitions: int, contiguous_split_chunks: bool = False)
    return: Sequence[torch.Tensor]
    doc: Split a tensor along its last dimension.
  - name: get_pp_indices
    signature: (num_hidden_layers: int, pp_rank: int, pp_size: int)
    return: Tuple[int, int]
    doc: Try to evenly distribute layers across partitions.
  - name: __post_init__
    signature: (self)
    class: StatelessProcessGroup
  - name: send_obj
    signature: (self, obj: Any, dst: int)
    class: StatelessProcessGroup
    doc: Send an object to a destination rank.
  - name: expire_data
    signature: (self)
    class: StatelessProcessGroup
    doc: Expire data that is older than `data_expiration_seconds` seconds.
  - name: recv_obj
    signature: (self, src: int)
    return: Any
    class: StatelessProcessGroup
    doc: Receive an object from a source rank.
  - name: broadcast_obj
    signature: (self, obj: Optional[Any], src: int)
    return: Any
    class: StatelessProcessGroup
    doc: Broadcast an object from a source rank to all other ranks.
  - name: all_gather_obj
    signature: (self, obj: Any)
    return: list[Any]
    class: StatelessProcessGroup
    doc: All gather an object from all ranks.
  - name: barrier
    signature: (self)
    class: StatelessProcessGroup
    doc: A barrier to synchronize all ranks.
  - name: create
    signature: (host: str, port: int, rank: int, world_size: int, data_expiration_seconds: int = 3600)
    return: 'StatelessProcessGroup'
    class: StatelessProcessGroup
    doc: A replacement for `torch.distributed.init_process_group` that does not
