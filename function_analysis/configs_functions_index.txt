AST Function Index
Root: /Users/vincentzed/Documents/Github/open_source/sglang/python/sglang/srt
Excluded directories:

File: configs/__init__.py
  (no function definitions found)
File: configs/chatglm.py
  - name: __init__
    signature: (self, num_layers = 28, padded_vocab_size = 65024, hidden_size = 4096, ffn_hidden_size = 13696, kv_channels = 128, num_attention_heads = 32, seq_length = 2048, hidden_dropout = 0.0, attention_dropout = 0.0, layernorm_epsilon = 1e-05, rmsnorm = True, apply_residual_connection_post_layernorm = False, post_layer_norm = True, add_bias_linear = False, add_qkv_bias = False, interleaved_qkv = False, bias_dropout_fusion = True, multi_query_attention = False, multi_query_group_num = 1, apply_query_key_layer_scaling = True, attention_softmax_in_fp32 = True, fp32_residual_connection = False, quantization_bit = 0, pre_seq_len = None, prefix_projection = False, **kwargs)
    class: ChatGLMConfig

File: configs/dbrx.py
  - name: __init__
    signature: (self, attn_pdrop: float = 0, clip_qkv: Optional[float] = None, kv_n_heads: int = 1, rope_theta: float = 10000.0, **kwargs: Any)
    class: DbrxAttentionConfig
  - name: from_pretrained
    signature: (cls, pretrained_model_name_or_path: str, **kwargs: Any)
    return: 'PretrainedConfig'
    class: DbrxAttentionConfig
  - name: __init__
    signature: (self, ffn_act_fn: Optional[dict] = None, ffn_hidden_size: int = 3584, moe_num_experts: int = 4, moe_top_k: int = 1, moe_jitter_eps: Optional[float] = None, moe_loss_weight: float = 0.01, moe_normalize_expert_weights: Optional[float] = 1, uniform_expert_assignment: bool = False, **kwargs: Any)
    class: DbrxFFNConfig
  - name: from_pretrained
    signature: (cls, pretrained_model_name_or_path: str, **kwargs: Any)
    return: 'PretrainedConfig'
    class: DbrxFFNConfig
  - name: __init__
    signature: (self, d_model: int = 2048, n_heads: int = 16, n_layers: int = 24, max_seq_len: int = 2048, vocab_size: int = 32000, resid_pdrop: float = 0.0, emb_pdrop: float = 0.0, attn_config: Optional[DbrxAttentionConfig] = None, ffn_config: Optional[DbrxFFNConfig] = None, use_cache: bool = True, initializer_range: float = 0.02, output_router_logits: bool = False, router_aux_loss_coef: float = 0.05, **kwargs: Any)
    class: DbrxConfig

File: configs/deepseekvl2.py
  - name: select_best_resolution
    signature: (image_size, candidate_resolutions)
  - name: items
    signature: (self)
    class: DictOutput
  - name: keys
    signature: (self)
    class: DictOutput
  - name: __getitem__
    signature: (self, item)
    class: DictOutput
  - name: __contains__
    signature: (self, key)
    class: DictOutput
  - name: __setitem__
    signature: (self, key, value)
    class: DictOutput
  - name: __len__
    signature: (self)
    class: VLChatProcessorOutput
  - name: __init__
    signature: (self, mean: Optional[Tuple[float, float, float]] = (0.5, 0.5, 0.5), std: Optional[Tuple[float, float, float]] = (0.5, 0.5, 0.5), normalize: bool = True)
    class: ImageTransform
  - name: __call__
    signature: (self, pil_img: Image.Image)
    class: ImageTransform
  - name: __init__
    signature: (self, tokenizer: LlamaTokenizerFast, candidate_resolutions: Tuple[Tuple[int, int]], patch_size: int, downsample_ratio: int, image_mean: Tuple[float, float, float] = (0.5, 0.5, 0.5), image_std: Tuple[float, float, float] = (0.5, 0.5, 0.5), normalize: bool = True, image_token: str = '<image>', pad_token: str = '<｜▁pad▁｜>', add_special_token: bool = False, sft_format: str = 'deepseek', mask_prompt: bool = True, ignore_id: int = -100, **kwargs)
    class: DeepseekVLV2Processor
  - name: format_messages_v2
    signature: (self, messages, pil_images, max_req_input_len = -1)
    class: DeepseekVLV2Processor
    doc: play the role of format_messages_v2 and get_images_info in the last version
  - name: bos_id
    signature: (self)
    class: DeepseekVLV2Processor
  - name: eos_id
    signature: (self)
    class: DeepseekVLV2Processor
  - name: pad_id
    signature: (self)
    class: DeepseekVLV2Processor
  - name: encode
    signature: (self, text: str, bos: bool = True, eos: bool = False)
    class: DeepseekVLV2Processor
  - name: decode
    signature: (self, t: List[int], **kwargs)
    return: str
    class: DeepseekVLV2Processor
  - name: process_one
    signature: (self, prompt: str = None, conversations: List[Dict[str, str]] = None, images: List[Image.Image] = None, apply_sft_format: bool = False, inference_mode: bool = True, system_prompt: str = '', max_req_input_len: int = -1, **kwargs)
    class: DeepseekVLV2Processor
    doc: Args:
  - name: __call__
    signature: (self, *, prompt: str = None, conversations: List[Dict[str, str]] = None, images: List[Image.Image] = None, apply_sft_format: bool = False, inference_mode: bool = True, system_prompt: str = '', max_req_input_len: int = -1, **kwargs)
    class: DeepseekVLV2Processor
  - name: find_all_indices
    signature: (self, messages, target_value)
    class: DeepseekVLV2Processor
  - name: tokenize_with_images
    signature: (self, conversation: str, images: List[Image.Image], bos: bool = True, eos: bool = True, cropping: bool = True, max_req_input_len: int = -1)
    class: DeepseekVLV2Processor
    doc: Tokenize text with <image> tags.
  - name: __init__
    signature: (self, model_name: str = 'siglip_large_patch16_384', image_size: int = 384, patch_size: int = 16, width: int = 1024, layers: int = 24, heads: int = 16, mlp_ratio: int = 4, global_pool: str = 'map', ignore_head: bool = True, class_token: bool = False, num_classes: int = 0, use_checkpoint: bool = False, **kwargs)
    class: DeepseekVL2VisionEncoderConfig
  - name: __init__
    signature: (self, projector_type: str = 'downsample_mlp_gelu', input_dim: int = 1152, n_embed: int = 2048, depth: int = 2, mlp_ratio: int = 1, downsample_ratio: int = 2, **kwargs)
    class: DeepseekVL2MlpProjectorConfig
  - name: __init__
    signature: (self, vocab_size = 102400, hidden_size = 4096, intermediate_size = 11008, moe_intermediate_size = 1407, num_hidden_layers = 30, num_attention_heads = 32, num_key_value_heads = 32, n_shared_experts = None, n_routed_experts = None, ep_size = 1, routed_scaling_factor = 1.0, kv_lora_rank = 512, q_lora_rank = 1536, qk_rope_head_dim = 64, v_head_dim = 128, qk_nope_head_dim = 128, topk_method = 'gready', n_group = None, topk_group = None, num_experts_per_tok = None, moe_layer_freq = 1, first_k_dense_replace = 0, norm_topk_prob = False, scoring_func = 'softmax', aux_loss_alpha = 0.001, seq_aux = True, hidden_act = 'silu', max_position_embeddings = 2048, initializer_range = 0.02, rms_norm_eps = 1e-06, use_cache = True, pad_token_id = None, bos_token_id = 100000, eos_token_id = 100001, pretraining_tp = 1, tie_word_embeddings = False, rope_theta = 10000.0, rope_scaling = None, attention_bias = False, attention_dropout = 0.0, use_mla = True, **kwargs)
    class: DeepseekV2Config
  - name: __init__
    signature: (self, tile_tag: str = 'tile_tag', global_view_pos: str = 'head', candidate_resolutions: Tuple[Tuple[int, int]] = ((384, 384),), **kwargs)
    class: DeepseekVL2Config

File: configs/device_config.py
  - name: __init__
    signature: (self, device: str = 'cuda')
    return: None
    class: DeviceConfig

File: configs/exaone.py
  - name: __init__
    signature: (self, vocab_size = 102400, max_position_embeddings = 2048, hidden_size = 2048, num_layers = 32, num_attention_heads = 32, num_key_value_heads = None, intermediate_size = None, activation_function = 'silu', rope_theta = 10000.0, rope_scaling = None, embed_dropout = 0.0, attention_dropout = 0.0, layer_norm_epsilon = 1e-05, initializer_range = 0.02, use_cache = True, bos_token_id = 0, eos_token_id = 2, tie_word_embeddings = True, **kwargs)
    class: ExaoneConfig

File: configs/internvl.py
  - name: __init__
    signature: (self, vocab_size = 103168, hidden_size = 4096, intermediate_size = 11008, num_hidden_layers = 32, num_attention_heads = 32, num_key_value_heads = None, hidden_act = 'silu', max_position_embeddings = 2048, initializer_range = 0.02, rms_norm_eps = 1e-06, use_cache = True, pad_token_id = 0, bos_token_id = 1, eos_token_id = 2, tie_word_embeddings = False, bias = True, rope_theta = 10000, rope_scaling = None, attn_implementation = 'eager', **kwargs)
    class: InternLM2Config
  - name: _rope_scaling_validation
    signature: (self)
    class: InternLM2Config
    doc: Validate the `rope_scaling` configuration.
  - name: __init__
    signature: (self, num_channels = 3, patch_size = 14, image_size = 224, qkv_bias = False, hidden_size = 3200, num_attention_heads = 25, intermediate_size = 12800, qk_normalization = True, num_hidden_layers = 48, use_flash_attn = True, hidden_act = 'gelu', layer_norm_eps = 1e-06, dropout = 0.0, drop_path_rate = 0.0, attention_dropout = 0.0, initializer_range = 0.02, initializer_factor = 0.1, **kwargs)
    class: InternVisionConfig
  - name: from_pretrained
    signature: (cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs)
    return: 'PretrainedConfig'
    class: InternVisionConfig
  - name: __init__
    signature: (self, vision_config = None, llm_config = None, use_backbone_lora = 0, use_llm_lora = 0, pad2square = False, select_layer = -1, force_image_size = None, downsample_ratio = 0.5, template = None, dynamic_image_size = False, use_thumbnail = False, ps_version = 'v1', min_dynamic_patch = 1, max_dynamic_patch = 6, **kwargs)
    class: InternVLChatConfig
  - name: to_dict
    signature: (self)
    class: InternVLChatConfig
    doc: Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].
  - name: __init__
    signature: (self, vocab_file, unk_token = '<unk>', bos_token = '<s>', eos_token = '</s>', pad_token = '</s>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token = True, add_eos_token = False, decode_with_prefix_space = False, clean_up_tokenization_spaces = False, **kwargs)
    class: InternLM2Tokenizer
  - name: no_prefix_space_tokens
    signature: (self)
    class: InternLM2Tokenizer
  - name: vocab_size
    signature: (self)
    class: InternLM2Tokenizer
    doc: Returns vocab size
  - name: bos_token_id
    signature: (self)
    return: Optional[int]
    class: InternLM2Tokenizer
  - name: eos_token_id
    signature: (self)
    return: Optional[int]
    class: InternLM2Tokenizer
  - name: get_vocab
    signature: (self)
    class: InternLM2Tokenizer
    doc: Returns vocab as a dict
  - name: _tokenize
    signature: (self, text)
    class: InternLM2Tokenizer
    doc: Returns a tokenized string.
  - name: _convert_token_to_id
    signature: (self, token)
    class: InternLM2Tokenizer
    doc: Converts a token (str) in an id using the vocab.
  - name: _convert_id_to_token
    signature: (self, index)
    class: InternLM2Tokenizer
    doc: Converts an index (integer) in a token (str) using the vocab.
  - name: _maybe_add_prefix_space
    signature: (self, tokens, decoded)
    class: InternLM2Tokenizer
  - name: convert_tokens_to_string
    signature: (self, tokens)
    class: InternLM2Tokenizer
    doc: Converts a sequence of tokens (string) in a single string.
  - name: save_vocabulary
    signature: (self, save_directory, filename_prefix: Optional[str] = None)
    return: Tuple[str]
    class: InternLM2Tokenizer
    doc: Save the vocabulary and special tokens file to a directory.
  - name: build_inputs_with_special_tokens
    signature: (self, token_ids_0, token_ids_1 = None)
    class: InternLM2Tokenizer
  - name: get_special_tokens_mask
    signature: (self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False)
    return: List[int]
    class: InternLM2Tokenizer
    doc: Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
  - name: create_token_type_ids_from_sequences
    signature: (self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None)
    return: List[int]
    class: InternLM2Tokenizer
    doc: Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make

File: configs/janus_pro.py
  - name: __init__
    signature: (self, dictionary)
    class: DictToObject
  - name: __init__
    signature: (self, **kwargs)
    class: VisionConfig
  - name: __init__
    signature: (self, **kwargs)
    class: GenAlignerConfig
  - name: __init__
    signature: (self, **kwargs)
    class: GenHeadConfig
  - name: __init__
    signature: (self, **kwargs)
    class: AlignerConfig
  - name: __init__
    signature: (self, **kwargs)
    class: GenVisionConfig
  - name: __init__
    signature: (self, **kwargs)
    class: MultiModalityConfig
  - name: __init__
    signature: (self, image_size: int, min_size: int = 14, image_mean: Union[Tuple[float, float, float], List[float]] = (0.48145466, 0.4578275, 0.40821073), image_std: Union[Tuple[float, float, float], List[float]] = (0.26862954, 0.26130258, 0.27577711), rescale_factor: float = 1.0 / 255.0, do_normalize: bool = True, **kwargs)
    class: VLMImageProcessor
  - name: resize
    signature: (self, pil_img: Image)
    return: np.ndarray
    class: VLMImageProcessor
    doc: Args:
  - name: resize
    signature: (pil_img, size, interpolation = PIL.Image.Resampling.BICUBIC, antialias = True)
    class: VLMImageProcessor
  - name: preprocess
    signature: (self, images, return_tensors: str = 'pt', **kwargs)
    return: BatchFeature
    class: VLMImageProcessor
  - name: default_shape
    signature: (self)
    class: VLMImageProcessor
  - name: items
    signature: (self)
    class: DictOutput
  - name: keys
    signature: (self)
    class: DictOutput
  - name: __getitem__
    signature: (self, item)
    class: DictOutput
  - name: __contains__
    signature: (self, key)
    class: DictOutput
  - name: __setitem__
    signature: (self, key, value)
    class: DictOutput
  - name: __len__
    signature: (self)
    class: VLChatProcessorOutput
  - name: __init__
    signature: (self, image_processor: VLMImageProcessor, tokenizer: LlamaTokenizerFast, image_tag: str = '<image_placeholder>', image_start_tag: str = '<begin_of_image>', image_end_tag: str = '<end_of_image>', pad_tag: str = '<｜▁pad▁｜>', num_image_tokens: int = 576, add_special_token: bool = False, sft_format: str = 'deepseek', mask_prompt: bool = True, ignore_id: int = -100, **kwargs)
    class: VLChatProcessor
  - name: image_token
    signature: (self)
    class: VLChatProcessor
  - name: image_id
    signature: (self)
    return: int
    class: VLChatProcessor
  - name: image_start_id
    signature: (self)
    class: VLChatProcessor
  - name: image_end_id
    signature: (self)
    class: VLChatProcessor
  - name: image_start_token
    signature: (self)
    class: VLChatProcessor
  - name: image_end_token
    signature: (self)
    class: VLChatProcessor
  - name: pad_id
    signature: (self)
    class: VLChatProcessor
  - name: add_image_token
    signature: (self, image_indices: List[int], input_ids: torch.LongTensor)
    class: VLChatProcessor
    doc: Args:
  - name: process_one
    signature: (self, prompt: str = None, images: List[Image] = None, **kwargs)
    class: VLChatProcessor
    doc: Args:
  - name: __call__
    signature: (self, *, prompt: str = None, conversations: List[Dict[str, str]] = None, images: List[Image] = None, force_batchify: bool = True, **kwargs)
    class: VLChatProcessor
    doc: Args:
  - name: batchify
    signature: (self, prepare_list: List[VLChatProcessorOutput])
    return: BatchedVLChatProcessorOutput
    class: VLChatProcessor
    doc: Preprocesses the inputs for multimodal inference.
  - name: __init__
    signature: (self, image_size: int, min_size: int = 14, image_mean: Union[Tuple[float, float, float], List[float]] = (0.48145466, 0.4578275, 0.40821073), image_std: Union[Tuple[float, float, float], List[float]] = (0.26862954, 0.26130258, 0.27577711), rescale_factor: float = 1.0 / 255.0, do_normalize: bool = True, **kwargs)
    class: VLMImageProcessorConfig

File: configs/kimi_vl.py
  - name: __init__
    signature: (self, vision_config: Optional[Union[dict, MoonViTConfig]] = None, text_config: Optional[Union[dict, DeepseekV2Config]] = None, ignore_index: int = -100, media_placeholder_token_id: int = 163605, pad_token_id: int = 0, **kwargs)
    class: KimiVLConfig

File: configs/kimi_vl_moonvit.py
  - name: __init__
    signature: (self, patch_size: int = 14, init_pos_emb_height: int = 64, init_pos_emb_width: int = 64, num_attention_heads: int = 16, num_hidden_layers: int = 27, hidden_size: int = 1152, intermediate_size: int = 4304, merge_kernel_size: tuple[int, int] = (2, 2), **kwargs)
    class: MoonViTConfig

File: configs/load_config.py
  - name: __post_init__
    signature: (self)
    class: LoadConfig
  - name: _verify_load_format
    signature: (self)
    return: None
    class: LoadConfig

File: configs/longcat_flash.py
  - name: __init__
    signature: (self, vocab_size = 131072, hidden_size = 6144, intermediate_size = None, ffn_hidden_size = 12288, expert_ffn_hidden_size = 2048, num_layers = 28, num_hidden_layers = None, num_attention_heads = 64, ep_size = 1, kv_lora_rank = 512, q_lora_rank = 1536, qk_rope_head_dim = 128, qk_nope_head_dim = 128, v_head_dim = 128, n_routed_experts = 512, moe_topk = 12, norm_topk_prob = False, max_position_embeddings = 131072, rms_norm_eps = 1e-05, use_cache = True, pad_token_id = None, bos_token_id = 1, eos_token_id = 2, pretraining_tp = 1, tie_word_embeddings = False, rope_theta = 10000000.0, rope_scaling = None, attention_bias = False, attention_dropout = 0.0, mla_scale_q_lora = True, mla_scale_kv_lora = True, torch_dtype = 'bfloat16', params_dtype = 'bfloat16', rounter_params_dtype = 'float32', router_bias = False, topk_method = None, routed_scaling_factor = 6.0, zero_expert_num = 256, zero_expert_type = 'identity', nextn_use_scmoe = False, num_nextn_predict_layers = 1, **kwargs)
    class: LongcatFlashConfig

File: configs/model_config.py
  - name: __init__
    signature: (self, model_path: str, trust_remote_code: bool = True, revision: Optional[str] = None, context_length: Optional[int] = None, model_override_args: str = '{}', is_embedding: Optional[bool] = None, enable_multimodal: Optional[bool] = None, dtype: str = 'auto', quantization: Optional[str] = None, override_config_file: Optional[str] = None, is_draft_model: bool = False, hybrid_kvcache_ratio: Optional[float] = None, model_impl: Union[str, ModelImpl] = ModelImpl.AUTO)
    return: None
    class: ModelConfig
  - name: from_server_args
    signature: (server_args: ServerArgs, model_path: str = None, **kwargs)
    class: ModelConfig
  - name: get_total_num_attention_heads
    signature: (self)
    return: int
    class: ModelConfig
  - name: get_num_attention_heads
    signature: (self, tensor_parallel_size)
    return: int
    class: ModelConfig
  - name: get_total_num_kv_heads
    signature: (self)
    return: int
    class: ModelConfig
    doc: Returns the total number of KV heads.
  - name: get_num_kv_heads
    signature: (self, tensor_parallel_size)
    return: int
    class: ModelConfig
    doc: Returns the number of KV heads per GPU.
  - name: _parse_quant_hf_config
    signature: (self)
    class: ModelConfig
  - name: _verify_quantization
    signature: (self)
    return: None
    class: ModelConfig
  - name: _verify_dual_chunk_attention_config
    signature: (self)
    return: None
    class: ModelConfig
  - name: get_hf_eos_token_id
    signature: (self)
    return: Optional[Set[int]]
    class: ModelConfig
  - name: maybe_pull_model_tokenizer_from_remote
    signature: (self)
    return: None
    class: ModelConfig
    doc: Pull the model config files to a temporary
  - name: _get_and_verify_dtype
    signature: (config: PretrainedConfig, dtype: Union[str, torch.dtype])
    return: torch.dtype
  - name: is_generation_model
    signature: (model_architectures: List[str], is_embedding: bool = False)
  - name: is_multimodal_model
    signature: (model_architectures: List[str])
  - name: is_multimodal_gen_model
    signature: (model_architectures: List[str])
  - name: is_image_gen_model
    signature: (model_architectures: List[str])
  - name: is_audio_model
    signature: (model_architectures: List[str])
  - name: is_encoder_decoder_model
    signature: (model_architectures: List[str])
  - name: is_multimodal_chunked_prefill_supported
    signature: (model_architectures: List[str])
    doc: Check if chunked prefill is supported for a MultiModal model.
  - name: yarn_get_mscale
    signature: (scale: float = 1, mscale: float = 1)
    return: float
  - name: is_hybrid_model
    signature: (model_architectures: List[str], hybrid_kvcache_ratio: Optional[float], context_length: Optional[int], attention_chunk_size: Optional[int])
  - name: get_hybrid_layer_ids
    signature: (model_architectures: List[str], num_hidden_layers: int)

File: configs/step3_vl.py
  - name: __init__
    signature: (self, hidden_size = 1792, intermediate_size = 3072, output_hidden_size = 4096, num_hidden_layers = 63, num_attention_heads = 16, num_channels = 3, image_size = 728, patch_size = 14, hidden_act = 'quick_gelu', layer_norm_eps = 1e-05, **kwargs)
    class: Step3VisionEncoderConfig
  - name: __init__
    signature: (self, hidden_size: int = 7168, intermediate_size: int = 18432, num_attention_heads: int = 64, num_attention_groups: int = 1, num_hidden_layers: int = 61, max_seq_len: int = 65536, vocab_size: int = 128815, rms_norm_eps: float = 1e-05, moe_intermediate_size: int = 5120, moe_num_experts: int = 48, moe_top_k: int = 3, rope_theta: float = 500000, rope_scaling: Optional[dict[str, Any]] = None, max_position_embedding: int = 65536, share_expert_dim: int = 5120, share_q_dim: int = 2048, head_dim: int = 256, norm_expert_weight: bool = False, moe_layers_enum: tuple[int] = (4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59), **kwargs)
    return: None
    class: Step3TextConfig
  - name: __init__
    signature: (self, vision_config: Optional[Union[dict, Step3VisionEncoderConfig]] = None, text_config: Optional[Union[dict, Step3TextConfig]] = None, understand_projector_stride: int = 1, projector_bias: bool = True, image_token_id: int = 128001, **kwargs)
    return: None
    class: Step3VLConfig

File: configs/update_config.py
  - name: may_get_weight_block_size
    signature: (model_config, load_config)
  - name: get_moe_padding_size
    signature: (weight_block_size)
  - name: get_num_heads_padding_size
    signature: (tp_size, weight_block_size)
  - name: update_intermediate_size
    signature: (model_config, attr_name, intermediate_padding_size)
  - name: adjust_config_with_unaligned_cpu_tp
    signature: (model_config: ModelConfig, load_config: LoadConfig, tp_size: int)
    return: ModelConfig

File: configs/utils.py
  - name: register_image_processor
    signature: (config: Type[PretrainedConfig], image_processor: Type[BaseImageProcessor])
    doc: register customized hf image processor while removing hf impl
  - name: register_processor
    signature: (config: Type[PretrainedConfig], processor: Type[ProcessorMixin])
    doc: register customized hf processor while removing hf impl
