AST Function Index
Root: /Users/vincentzed/Documents/Github/open_source/sglang/python/sglang/srt
Excluded directories:

File: model_loader/__init__.py
  - name: get_model
    signature: (*, model_config: ModelConfig, load_config: LoadConfig, device_config: DeviceConfig)
    return: nn.Module

File: model_loader/loader.py
  - name: device_loading_context
    signature: (module: torch.nn.Module, target_device: torch.device)
  - name: _get_quantization_config
    signature: (model_config: ModelConfig, load_config: LoadConfig, packed_modules_mapping: Dict[str, List[str]])
    return: Optional[QuantizationConfig]
    doc: Get the quantization config.
  - name: _initialize_model
    signature: (model_config: ModelConfig, load_config: LoadConfig)
    return: nn.Module
    doc: Initialize a model with the given configurations.
  - name: __init__
    signature: (self, load_config: LoadConfig)
    class: BaseModelLoader
  - name: download_model
    signature: (self, model_config: ModelConfig)
    return: None
    class: BaseModelLoader
    doc: Download a model so that it can be immediately loaded.
  - name: load_model
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: BaseModelLoader
    doc: Load a model with the given configurations.
  - name: init_new
    signature: (cls, model_config: ModelConfig, model)
    class: DefaultModelLoader.Source
  - name: __init__
    signature: (self, load_config: LoadConfig)
    class: DefaultModelLoader
  - name: _maybe_download_from_modelscope
    signature: (self, model: str, revision: Optional[str])
    return: Optional[str]
    class: DefaultModelLoader
    doc: Download model from ModelScope hub if SGLANG_USE_MODELSCOPE is True.
  - name: _prepare_weights
    signature: (self, model_name_or_path: str, revision: Optional[str], fall_back_to_pt: bool)
    return: Tuple[str, List[str], bool]
    class: DefaultModelLoader
    doc: Prepare weights for the model.
  - name: _get_weights_iterator
    signature: (self, source: 'Source')
    return: Generator[Tuple[str, torch.Tensor], None, None]
    class: DefaultModelLoader
    doc: Get an iterator for the model weights based on the load format.
  - name: _get_all_weights
    signature: (self, model_config: ModelConfig, model: nn.Module)
    return: Generator[Tuple[str, torch.Tensor], None, None]
    class: DefaultModelLoader
  - name: download_model
    signature: (self, model_config: ModelConfig)
    return: None
    class: DefaultModelLoader
  - name: load_model
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: DefaultModelLoader
  - name: load_weights_and_postprocess
    signature: (model, weights, target_device)
    class: DefaultModelLoader
  - name: __init__
    signature: (self, load_config: LoadConfig)
    class: LayeredModelLoader
  - name: load_model
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: LayeredModelLoader
  - name: fill_module
    signature: (module, fqn: List[str], weights)
    class: LayeredModelLoader
    doc: fqn: list of strings representing the fully qualified name of `module`.
  - name: __init__
    signature: (self, load_config: LoadConfig)
    class: DummyModelLoader
  - name: download_model
    signature: (self, model_config: ModelConfig)
    return: None
    class: DummyModelLoader
  - name: load_model
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: DummyModelLoader
  - name: __init__
    signature: (self, load_config: LoadConfig)
    class: ShardedStateLoader
  - name: _filter_subtensors
    signature: (tensors: Dict[str, torch.Tensor])
    return: Dict[str, torch.Tensor]
    class: ShardedStateLoader
    doc: Filter out all tensors that share the same memory or a subset of the
  - name: get_end_ptr
    signature: (tensor: torch.Tensor)
    return: int
    class: ShardedStateLoader
  - name: _prepare_weights
    signature: (self, model_name_or_path: str, revision: Optional[str])
    class: ShardedStateLoader
  - name: download_model
    signature: (self, model_config: ModelConfig)
    return: None
    class: ShardedStateLoader
  - name: load_model
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: ShardedStateLoader
  - name: save_model
    signature: (model: torch.nn.Module, path: str, pattern: Optional[str] = None, max_size: Optional[int] = None)
    return: None
    class: ShardedStateLoader
  - name: __init__
    signature: (self, load_config: LoadConfig)
    class: BitsAndBytesModelLoader
  - name: _get_config_file
    signature: (self, qlora_adapter: str)
    return: str
    class: BitsAndBytesModelLoader
  - name: _get_weight_files
    signature: (self, model_name_or_path: str, allowed_patterns: List[str], revision: Optional[str] = None)
    return: Tuple[List[str], str]
    class: BitsAndBytesModelLoader
    doc: Retrieve weight files. Download the files if necessary.
  - name: _prepare_weights
    signature: (self, model_name_or_path: str, revision: Optional[str])
    return: Tuple[List[str], bool]
    class: BitsAndBytesModelLoader
    doc: Prepare weight files for the model.
  - name: _hf_weight_iter
    signature: (self, hf_weights_files, use_safetensors: bool)
    class: BitsAndBytesModelLoader
  - name: _get_quantized_weights_iterator
    signature: (self, model_name_or_path: str, revision: Optional[str], pre_quant: bool, load_8bit: bool)
    return: Tuple[Generator[Tuple[str, torch.Tensor], None, None], Dict[str, Any]]
    class: BitsAndBytesModelLoader
    doc: Get an iterator to the model weights with bitsandbytes quantization,
  - name: _is_8bit_weight_name
    signature: (self, weight_name: str)
    class: BitsAndBytesModelLoader
  - name: _is_4bit_weight_name
    signature: (self, weight_name: str)
    class: BitsAndBytesModelLoader
  - name: _quantized_8bit_generator
    signature: (self, hf_weights_files, use_safetensors, quant_state_dict)
    return: Generator
    class: BitsAndBytesModelLoader
  - name: _quantized_4bit_generator
    signature: (self, hf_weights_files, use_safetensors, quant_state_dict)
    return: Generator
    class: BitsAndBytesModelLoader
  - name: _parse_quant_state
    signature: (param_name: str, temp_state_dict: Dict)
    return: QuantState
    class: BitsAndBytesModelLoader
  - name: _unquantized_generator
    signature: (self, hf_weights_files, use_safetensors, quant_state_dict)
    return: Generator
    class: BitsAndBytesModelLoader
  - name: _load_weights
    signature: (self, model_config: ModelConfig, model: nn.Module)
    return: None
    class: BitsAndBytesModelLoader
  - name: download_model
    signature: (self, model_config: ModelConfig)
    return: None
    class: BitsAndBytesModelLoader
  - name: load_model
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: BitsAndBytesModelLoader
  - name: __init__
    signature: (self, load_config: LoadConfig)
    class: GGUFModelLoader
  - name: _prepare_weights
    signature: (self, model_name_or_path: str)
    class: GGUFModelLoader
  - name: _get_gguf_weights_map
    signature: (self, model_config: ModelConfig)
    class: GGUFModelLoader
    doc: GGUF uses this naming convention for their tensors from HF checkpoint:
  - name: _get_weights_iterator
    signature: (self, model_name_or_path: str, gguf_to_hf_name_map: Dict[str, str])
    return: Generator[Tuple[str, torch.Tensor], None, None]
    class: GGUFModelLoader
  - name: download_model
    signature: (self, model_config: ModelConfig)
    return: None
    class: GGUFModelLoader
  - name: load_model
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: GGUFModelLoader
  - name: __init__
    signature: (self, load_config: LoadConfig)
    class: RemoteModelLoader
  - name: _get_weights_iterator_kv
    signature: (self, client)
    return: Generator[Tuple[str, torch.Tensor], None, None]
    class: RemoteModelLoader
    doc: Get an iterator for the model weights from remote storage.
  - name: _get_weights_iterator_fs
    signature: (self, client)
    return: Generator[Tuple[str, torch.Tensor], None, None]
    class: RemoteModelLoader
    doc: Get an iterator for the model weights from remote storage.
  - name: download_model
    signature: (self, model_config: ModelConfig)
    return: None
    class: RemoteModelLoader
  - name: save_model
    signature: (model: torch.nn.Module, model_path: str, url: str)
    return: None
    class: RemoteModelLoader
  - name: _load_model_from_remote_kv
    signature: (self, model: nn.Module, model_config: ModelConfig, client)
    class: RemoteModelLoader
  - name: _load_model_from_remote_fs
    signature: (self, model, client, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: RemoteModelLoader
  - name: load_model
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
    class: RemoteModelLoader
  - name: load_model_with_cpu_quantization
    signature: (self, *, model_config: ModelConfig, device_config: DeviceConfig)
    return: nn.Module
  - name: get_model_loader
    signature: (load_config: LoadConfig)
    return: BaseModelLoader
    doc: Get a model loader based on the load format.

File: model_loader/utils.py
  - name: set_default_torch_dtype
    signature: (dtype: torch.dtype)
    doc: Sets the default torch dtype to the given dtype.
  - name: resolve_transformers_arch
    signature: (model_config: ModelConfig, architectures: list[str])
  - name: get_model_architecture
    signature: (model_config: ModelConfig)
    return: Tuple[Type[nn.Module], str]
  - name: get_architecture_class_name
    signature: (model_config: ModelConfig)
    return: str
  - name: post_load_weights
    signature: (model: nn.Module, model_config: ModelConfig)

File: model_loader/weight_utils.py
  - name: enable_hf_transfer
    signature: ()
    doc: automatically activates hf_transfer
  - name: __init__
    signature: (self, *args, **kwargs)
    class: DisabledTqdm
  - name: get_lock
    signature: (model_name_or_path: str, cache_dir: Optional[str] = None)
  - name: _shared_pointers
    signature: (tensors)
  - name: convert_bin_to_safetensor_file
    signature: (pt_filename: str, sf_filename: str)
    return: None
  - name: get_quant_config
    signature: (model_config: ModelConfig, load_config: LoadConfig, packed_modules_mapping: Dict[str, List[str]])
    return: QuantizationConfig
  - name: download_weights_from_hf
    signature: (model_name_or_path: str, cache_dir: Optional[str], allow_patterns: List[str], revision: Optional[str] = None, ignore_patterns: Optional[Union[str, List[str]]] = None)
    return: str
    doc: Download model weights from Hugging Face Hub.
  - name: download_safetensors_index_file_from_hf
    signature: (model_name_or_path: str, index_file: str, cache_dir: Optional[str], revision: Optional[str] = None)
    return: None
    doc: Download hf safetensors index file from Hugging Face Hub.
  - name: filter_duplicate_safetensors_files
    signature: (hf_weights_files: List[str], hf_folder: str, index_file: str)
    return: List[str]
  - name: filter_files_not_needed_for_inference
    signature: (hf_weights_files: List[str])
    return: List[str]
    doc: Exclude files that are not needed for inference.
  - name: np_cache_weights_iterator
    signature: (model_name_or_path: str, cache_dir: Optional[str], hf_folder: str, hf_weights_files: List[str])
    return: Generator[Tuple[str, torch.Tensor], None, None]
    doc: Iterate over the weights in the model np files.
  - name: decrypt
    signature: (fn, key)
  - name: safetensors_encrypted_weights_iterator
    signature: (hf_weights_files: List[str], is_all_weights_sharded: bool = False, decryption_key: Optional[str] = None)
  - name: safetensors_weights_iterator
    signature: (hf_weights_files: List[str], is_all_weights_sharded: bool = False, decryption_key: Optional[str] = None, disable_mmap: bool = False)
    return: Generator[Tuple[str, torch.Tensor], None, None]
    doc: Iterate over the weights in the model safetensor files.
  - name: multi_thread_safetensors_weights_iterator
    signature: (hf_weights_files: List[str], is_all_weights_sharded: bool = False, decryption_key: Optional[str] = None, max_workers: int = 4, disable_mmap: bool = False)
    return: Generator[Tuple[str, torch.Tensor], None, None]
    doc: Multi-Thread iterate over the weights in the model safetensor files.
  - name: _load_file
    signature: (st_file: str)
  - name: pt_weights_iterator
    signature: (hf_weights_files: List[str])
    return: Generator[Tuple[str, torch.Tensor], None, None]
    doc: Iterate over the weights in the model bin/pt files.
  - name: multi_thread_pt_weights_iterator
    signature: (hf_weights_files: List[str], max_workers: int = 4)
    return: Generator[Tuple[str, torch.Tensor], None, None]
    doc: Multi-Thread iterate over the weights in the model bin/pt files.
  - name: _load_file
    signature: (bin_file: str)
  - name: get_gguf_extra_tensor_names
    signature: (gguf_file: str, gguf_to_hf_name_map: Dict[str, str])
    return: List[str]
  - name: gguf_quant_weights_iterator
    signature: (gguf_file: str, gguf_to_hf_name_map: Dict[str, str])
    return: Generator[Tuple[str, torch.Tensor], None, None]
    doc: Iterate over the quant weights in the model gguf files and convert
  - name: convert_pyslice_to_tensor
    signature: (x: Any)
    return: torch.Tensor
    doc: convert PySafeSlice object from safetensors to torch.Tensor
  - name: default_weight_loader
    signature: (param: torch.Tensor, loaded_weight: torch.Tensor)
    return: None
    doc: Default weight loader.
  - name: row_parallel_weight_loader
    signature: (param: torch.Tensor, loaded_weight: torch.Tensor)
    return: None
    doc: Load weights that are row-parallelized.
  - name: sharded_weight_loader
    signature: (shard_axis: int)
    return: LoaderFunction
    doc: Create a weight loader that shards the weights along the given axis
  - name: loader
    signature: (param: torch.Tensor, loaded_weight: torch.Tensor)
    return: None
  - name: composed_weight_loader
    signature: (loader: LoaderFunction, fn: Callable[[torch.Tensor], torch.Tensor])
    return: LoaderFunction
    doc: Create a weight loader that post-processes the weights after loading
  - name: composed_loader
    signature: (param: torch.Tensor, loaded_weight: torch.Tensor)
    return: None
  - name: runai_safetensors_weights_iterator
    signature: (hf_weights_files: List[str])
    return: Generator[Tuple[str, torch.Tensor], None, None]
    doc: Iterate over the weights in the model safetensor files.
  - name: set_runai_streamer_env
    signature: (load_config: LoadConfig)
  - name: initialize_dummy_weights
    signature: (model: torch.nn.Module, low: float = -0.001, high: float = 0.001, seed: int = 1234)
    return: None
    doc: Initialize model weights with random values.
  - name: maybe_remap_kv_scale_name
    signature: (name: str, params_dict: dict)
    return: Optional[str]
    doc: Remap the name of FP8 k/v_scale parameters.
  - name: check_is_fp8
    signature: (self)
    return: 'KVCacheQuantSchema'
    class: KVCacheQuantSchema
  - name: check_tp_ranks
    signature: (self, info: ValidationInfo)
    return: 'KVCacheQuantSchema'
    class: KVCacheQuantSchema
  - name: check_current_rank
    signature: (self, info: ValidationInfo)
    return: 'KVCacheQuantSchema'
    class: KVCacheQuantSchema
  - name: check_model_type
    signature: (self, info: ValidationInfo)
    return: 'QuantParamSchema'
    class: QuantParamSchema
  - name: kv_cache_scales_loader
    signature: (filename: str, tp_rank: int, tp_size: int, num_hidden_layers: int, model_type: Optional[str])
    return: Iterable[Tuple[int, float]]
    doc: A simple utility to read in KV cache scaling factors that have been
  - name: get_actual_shard_size
    signature: (shard_size, weight_start, weight_end)
  - name: reset_param_data_if_needed
    signature: (param_data, dim, start, length)
  - name: narrow_padded_param_and_loaded_weight
    signature: (param_data, loaded_weight, param_data_start, weight_start, dim, shard_size, narrow_weight = True)
