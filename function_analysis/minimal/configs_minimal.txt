configs/chatglm.py:
  ChatGLMConfig.__init__(self,num_layers,padded_vocab_size,hidden_size,ffn_hidden_size,kv_channels,num_attention_heads,seq_length,hidden_dropout,attention_dropout,layernorm_epsilon,rmsnorm,apply_residual_connection_post_layernorm,post_layer_norm,add_bias_linear,add_qkv_bias,interleaved_qkv,bias_dropout_fusion,multi_query_attention,multi_query_group_num,apply_query_key_layer_scaling,attention_softmax_in_fp32,fp32_residual_connection,quantization_bit,pre_seq_len,prefix_projection,**kwargs)
configs/dbrx.py:
  DbrxAttentionConfig.__init__(self,attn_pdrop,clip_qkv,kv_n_heads,rope_theta,**kwargs)
  DbrxAttentionConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
  DbrxConfig.__init__(self,d_model,n_heads,n_layers,max_seq_len,vocab_size,resid_pdrop,emb_pdrop,attn_config,ffn_config,use_cache,initializer_range,output_router_logits,router_aux_loss_coef,**kwargs)
  DbrxFFNConfig.__init__(self,ffn_act_fn,ffn_hidden_size,moe_num_experts,moe_top_k,moe_jitter_eps,moe_loss_weight,moe_normalize_expert_weights,uniform_expert_assignment,**kwargs)
  DbrxFFNConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
configs/deepseekvl2.py:
  DeepseekV2Config.__init__(self,vocab_size,hidden_size,intermediate_size,moe_intermediate_size,num_hidden_layers,num_attention_heads,num_key_value_heads,n_shared_experts,n_routed_experts,ep_size,routed_scaling_factor,kv_lora_rank,q_lora_rank,qk_rope_head_dim,v_head_dim,qk_nope_head_dim,topk_method,n_group,topk_group,num_experts_per_tok,moe_layer_freq,first_k_dense_replace,norm_topk_prob,scoring_func,aux_loss_alpha,seq_aux,hidden_act,max_position_embeddings,initializer_range,rms_norm_eps,use_cache,pad_token_id,bos_token_id,eos_token_id,pretraining_tp,tie_word_embeddings,rope_theta,rope_scaling,attention_bias,attention_dropout,use_mla,**kwargs)
  DeepseekVL2Config.__init__(self,tile_tag,global_view_pos,candidate_resolutions,**kwargs)
  DeepseekVL2MlpProjectorConfig.__init__(self,projector_type,input_dim,n_embed,depth,mlp_ratio,downsample_ratio,**kwargs)
  DeepseekVL2VisionEncoderConfig.__init__(self,model_name,image_size,patch_size,width,layers,heads,mlp_ratio,global_pool,ignore_head,class_token,num_classes,use_checkpoint,**kwargs)
  DeepseekVLV2Processor.__call__(self,prompt,conversations,images,apply_sft_format,inference_mode,system_prompt,max_req_input_len,**kwargs)
  DeepseekVLV2Processor.__init__(self,tokenizer,candidate_resolutions,patch_size,downsample_ratio,image_mean,image_std,normalize,image_token,pad_token,add_special_token,sft_format,mask_prompt,ignore_id,**kwargs)
  DeepseekVLV2Processor.bos_id(self)
  DeepseekVLV2Processor.decode(self,t,**kwargs)
  DeepseekVLV2Processor.encode(self,text,bos,eos)
  DeepseekVLV2Processor.eos_id(self)
  DeepseekVLV2Processor.find_all_indices(self,messages,target_value)
  DeepseekVLV2Processor.format_messages_v2(self,messages,pil_images,max_req_input_len)
  DeepseekVLV2Processor.pad_id(self)
  DeepseekVLV2Processor.process_one(self,prompt,conversations,images,apply_sft_format,inference_mode,system_prompt,max_req_input_len,**kwargs)
  DeepseekVLV2Processor.tokenize_with_images(self,conversation,images,bos,eos,cropping,max_req_input_len)
  DictOutput.__contains__(self,key)
  DictOutput.__getitem__(self,item)
  DictOutput.__setitem__(self,key,value)
  DictOutput.items(self)
  DictOutput.keys(self)
  ImageTransform.__call__(self,pil_img)
  ImageTransform.__init__(self,mean,std,normalize)
  VLChatProcessorOutput.__len__(self)
  select_best_resolution(image_size,candidate_resolutions)
configs/device_config.py:
  DeviceConfig.__init__(self,device)
configs/exaone.py:
  ExaoneConfig.__init__(self,vocab_size,max_position_embeddings,hidden_size,num_layers,num_attention_heads,num_key_value_heads,intermediate_size,activation_function,rope_theta,rope_scaling,embed_dropout,attention_dropout,layer_norm_epsilon,initializer_range,use_cache,bos_token_id,eos_token_id,tie_word_embeddings,**kwargs)
configs/internvl.py:
  InternLM2Config.__init__(self,vocab_size,hidden_size,intermediate_size,num_hidden_layers,num_attention_heads,num_key_value_heads,hidden_act,max_position_embeddings,initializer_range,rms_norm_eps,use_cache,pad_token_id,bos_token_id,eos_token_id,tie_word_embeddings,bias,rope_theta,rope_scaling,attn_implementation,**kwargs)
  InternLM2Config._rope_scaling_validation(self)
  InternLM2Tokenizer.__init__(self,vocab_file,unk_token,bos_token,eos_token,pad_token,sp_model_kwargs,add_bos_token,add_eos_token,decode_with_prefix_space,clean_up_tokenization_spaces,**kwargs)
  InternLM2Tokenizer._convert_id_to_token(self,index)
  InternLM2Tokenizer._convert_token_to_id(self,token)
  InternLM2Tokenizer._maybe_add_prefix_space(self,tokens,decoded)
  InternLM2Tokenizer._tokenize(self,text)
  InternLM2Tokenizer.bos_token_id(self)
  InternLM2Tokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1)
  InternLM2Tokenizer.convert_tokens_to_string(self,tokens)
  InternLM2Tokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1)
  InternLM2Tokenizer.eos_token_id(self)
  InternLM2Tokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1,already_has_special_tokens)
  InternLM2Tokenizer.get_vocab(self)
  InternLM2Tokenizer.no_prefix_space_tokens(self)
  InternLM2Tokenizer.save_vocabulary(self,save_directory,filename_prefix)
  InternLM2Tokenizer.vocab_size(self)
  InternVLChatConfig.__init__(self,vision_config,llm_config,use_backbone_lora,use_llm_lora,pad2square,select_layer,force_image_size,downsample_ratio,template,dynamic_image_size,use_thumbnail,ps_version,min_dynamic_patch,max_dynamic_patch,**kwargs)
  InternVLChatConfig.to_dict(self)
  InternVisionConfig.__init__(self,num_channels,patch_size,image_size,qkv_bias,hidden_size,num_attention_heads,intermediate_size,qk_normalization,num_hidden_layers,use_flash_attn,hidden_act,layer_norm_eps,dropout,drop_path_rate,attention_dropout,initializer_range,initializer_factor,**kwargs)
  InternVisionConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
configs/janus_pro.py:
  AlignerConfig.__init__(self,**kwargs)
  DictOutput.__contains__(self,key)
  DictOutput.__getitem__(self,item)
  DictOutput.__setitem__(self,key,value)
  DictOutput.items(self)
  DictOutput.keys(self)
  DictToObject.__init__(self,dictionary)
  GenAlignerConfig.__init__(self,**kwargs)
  GenHeadConfig.__init__(self,**kwargs)
  GenVisionConfig.__init__(self,**kwargs)
  MultiModalityConfig.__init__(self,**kwargs)
  VLChatProcessor.__call__(self,prompt,conversations,images,force_batchify,**kwargs)
  VLChatProcessor.__init__(self,image_processor,tokenizer,image_tag,image_start_tag,image_end_tag,pad_tag,num_image_tokens,add_special_token,sft_format,mask_prompt,ignore_id,**kwargs)
  VLChatProcessor.add_image_token(self,image_indices,input_ids)
  VLChatProcessor.batchify(self,prepare_list)
  VLChatProcessor.image_end_id(self)
  VLChatProcessor.image_end_token(self)
  VLChatProcessor.image_id(self)
  VLChatProcessor.image_start_id(self)
  VLChatProcessor.image_start_token(self)
  VLChatProcessor.image_token(self)
  VLChatProcessor.pad_id(self)
  VLChatProcessor.process_one(self,prompt,images,**kwargs)
  VLChatProcessorOutput.__len__(self)
  VLMImageProcessor.__init__(self,image_size,min_size,image_mean,image_std,rescale_factor,do_normalize,**kwargs)
  VLMImageProcessor.default_shape(self)
  VLMImageProcessor.preprocess(self,images,return_tensors,**kwargs)
  VLMImageProcessor.resize(pil_img,size,interpolation,antialias)
  VLMImageProcessor.resize(self,pil_img)
  VLMImageProcessorConfig.__init__(self,image_size,min_size,image_mean,image_std,rescale_factor,do_normalize,**kwargs)
  VisionConfig.__init__(self,**kwargs)
configs/kimi_vl.py:
  KimiVLConfig.__init__(self,vision_config,text_config,ignore_index,media_placeholder_token_id,pad_token_id,**kwargs)
configs/kimi_vl_moonvit.py:
  MoonViTConfig.__init__(self,patch_size,init_pos_emb_height,init_pos_emb_width,num_attention_heads,num_hidden_layers,hidden_size,intermediate_size,merge_kernel_size,**kwargs)
configs/load_config.py:
  LoadConfig.__post_init__(self)
  LoadConfig._verify_load_format(self)
configs/longcat_flash.py:
  LongcatFlashConfig.__init__(self,vocab_size,hidden_size,intermediate_size,ffn_hidden_size,expert_ffn_hidden_size,num_layers,num_hidden_layers,num_attention_heads,ep_size,kv_lora_rank,q_lora_rank,qk_rope_head_dim,qk_nope_head_dim,v_head_dim,n_routed_experts,moe_topk,norm_topk_prob,max_position_embeddings,rms_norm_eps,use_cache,pad_token_id,bos_token_id,eos_token_id,pretraining_tp,tie_word_embeddings,rope_theta,rope_scaling,attention_bias,attention_dropout,mla_scale_q_lora,mla_scale_kv_lora,torch_dtype,params_dtype,rounter_params_dtype,router_bias,topk_method,routed_scaling_factor,zero_expert_num,zero_expert_type,nextn_use_scmoe,num_nextn_predict_layers,**kwargs)
configs/model_config.py:
  ModelConfig.__init__(self,model_path,trust_remote_code,revision,context_length,model_override_args,is_embedding,enable_multimodal,dtype,quantization,override_config_file,is_draft_model,hybrid_kvcache_ratio,model_impl)
  ModelConfig._parse_quant_hf_config(self)
  ModelConfig._verify_dual_chunk_attention_config(self)
  ModelConfig._verify_quantization(self)
  ModelConfig.from_server_args(server_args,model_path,**kwargs)
  ModelConfig.get_hf_eos_token_id(self)
  ModelConfig.get_num_attention_heads(self,tensor_parallel_size)
  ModelConfig.get_num_kv_heads(self,tensor_parallel_size)
  ModelConfig.get_total_num_attention_heads(self)
  ModelConfig.get_total_num_kv_heads(self)
  ModelConfig.maybe_pull_model_tokenizer_from_remote(self)
  _get_and_verify_dtype(config,dtype)
  get_hybrid_layer_ids(model_architectures,num_hidden_layers)
  is_audio_model(model_architectures)
  is_encoder_decoder_model(model_architectures)
  is_generation_model(model_architectures,is_embedding)
  is_hybrid_model(model_architectures,hybrid_kvcache_ratio,context_length,attention_chunk_size)
  is_image_gen_model(model_architectures)
  is_multimodal_chunked_prefill_supported(model_architectures)
  is_multimodal_gen_model(model_architectures)
  is_multimodal_model(model_architectures)
  yarn_get_mscale(scale,mscale)
configs/step3_vl.py:
  Step3TextConfig.__init__(self,hidden_size,intermediate_size,num_attention_heads,num_attention_groups,num_hidden_layers,max_seq_len,vocab_size,rms_norm_eps,moe_intermediate_size,moe_num_experts,moe_top_k,rope_theta,rope_scaling,max_position_embedding,share_expert_dim,share_q_dim,head_dim,norm_expert_weight,moe_layers_enum,**kwargs)
  Step3VLConfig.__init__(self,vision_config,text_config,understand_projector_stride,projector_bias,image_token_id,**kwargs)
  Step3VisionEncoderConfig.__init__(self,hidden_size,intermediate_size,output_hidden_size,num_hidden_layers,num_attention_heads,num_channels,image_size,patch_size,hidden_act,layer_norm_eps,**kwargs)
configs/update_config.py:
  adjust_config_with_unaligned_cpu_tp(model_config,load_config,tp_size)
  get_moe_padding_size(weight_block_size)
  get_num_heads_padding_size(tp_size,weight_block_size)
  may_get_weight_block_size(model_config,load_config)
  update_intermediate_size(model_config,attr_name,intermediate_padding_size)
configs/utils.py:
  register_image_processor(config,image_processor)
  register_processor(config,processor)
