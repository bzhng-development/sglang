_custom_ops.py:
  init_custom_ar(ipc_tensors,rank_data,rank,full_nvlink)
  all_reduce(fa,inp,out,reg_buffer,reg_buffer_sz_bytes)
  dispose(fa)
  meta_size()
  register_buffer(fa,ipc_tensors)
  get_graph_buffer_ipc_meta(fa)
  register_graph_buffers(fa,handles,offsets)
  init_custom_ar(meta,rank_data,handles,offsets,rank,full_nvlink)
  all_reduce_reg(fa,inp,out)
  all_reduce_unreg(fa,inp,reg_buffer,out)
  dispose(fa)
  meta_size()
  register_buffer(fa,t,handles,offsets)
  get_graph_buffer_ipc_meta(fa)
  register_graph_buffers(fa,handles,offsets)
  allocate_meta_buffer(size)
  get_meta_buffer_ipc_handle(inp)
  init_custom_qr(rank,world_size,qr_max_size)
  qr_get_handle(fa)
  qr_open_handles(fa,handles)
  qr_all_reduce(fa,inp,out,quant_level,cast_bf2half)
  qr_destroy(fa)
  qr_max_size()
  mscclpp_generate_unique_id()
  mscclpp_init_context(unique_id,rank,world_size,scratch,put_buffer,nranks_per_node,rank_to_node,rank_to_ib,context_selection)
  mscclpp_allreduce(context,inp,out,nthreads,nblocks)
aio_rwlock.py:
  RWLock.__init__(self)
  RWLock.reader_lock(self)
  RWLock.writer_lock(self)
  async RWLock.acquire_reader(self)
  async RWLock.release_reader(self)
  async RWLock.acquire_writer(self)
  async RWLock.release_writer(self)
  _ReaderLock.__init__(self,rwlock)
  async _ReaderLock.__aenter__(self)
  async _ReaderLock.__aexit__(self,exc_type,exc_val,exc_tb)
  _WriterLock.__init__(self,rwlock)
  async _WriterLock.__aenter__(self)
  async _WriterLock.__aexit__(self,exc_type,exc_val,exc_tb)
bench_utils.py:
  suppress_stdout_stderr.__enter__(self)
  suppress_stdout_stderr.__exit__(self,*_)
  bench_kineto(fn,kernel_names,num_tests,suppress_kineto_output,trace_path,flush_l2,with_multiple_kernels)
code_completion_parser.py:
  register_completion_template(template,override)
  completion_template_exists(template_name)
  is_completion_template_defined()
  generate_completion_prompt_from_request(request)
  generate_completion_prompt(prompt,suffix,template_name)
conversation.py:
  Conversation.get_prompt(self)
  Conversation.set_system_message(self,system_message)
  Conversation.append_message(self,role,message)
  Conversation.append_image(self,image,detail)
  Conversation.append_video(self,video)
  Conversation.append_audio(self,audio)
  Conversation.update_last_message(self,message)
  Conversation.to_gradio_chatbot(self)
  Conversation.to_openai_api_messages(self)
  Conversation.copy(self)
  Conversation.dict(self)
  register_conv_template(template,override)
  register_conv_template_matching_function(func)
  get_conv_template_by_model_path(model_path)
  chat_template_exists(template_name)
  generate_embedding_convs(texts,images,template_name)
  _get_full_multimodal_text_prompt(modality_token,modality_count,text_prompt)
  generate_chat_conv(request,template_name)
  get_model_type(model_path)
  match_internvl(model_path)
  match_deepseek_janus_pro(model_path)
  match_vicuna(model_path)
  match_deepseek_vl(model_path)
  match_qwen_chat_ml(model_path)
  match_minicpm(model_path)
  match_phi_4_mm(model_path)
custom_op.py:
  CustomOp.__init__(self)
  CustomOp.enter_torch_compile(self,num_tokens)
  CustomOp.leave_torch_compile(self)
  CustomOp.forward(self,*args,**kwargs)
  CustomOp.forward_native(self,*args,**kwargs)
  CustomOp.forward_cuda(self,*args,**kwargs)
  CustomOp.forward_npu(self,*args,**kwargs)
  CustomOp.forward_hip(self,*args,**kwargs)
  CustomOp.forward_xpu(self,*args,**kwargs)
  CustomOp.forward_hpu(self,*args,**kwargs)
  CustomOp.forward_cpu(self,*args,**kwargs)
  CustomOp.dispatch_forward(self)
disaggregation/ascend/conn.py:
  AscendKVManager.init_engine(self)
  AscendKVManager.register_buffer_to_engine(self)
  AscendKVManager.send_kvcache(self,mooncake_session_id,prefill_kv_indices,dst_kv_ptrs,dst_kv_indices,executor)
  AscendKVManager.set_transfer_blocks(src_ptr,dst_ptr,item_len)
  AscendKVManager.process_layer(src_ptr,dst_ptr,item_len)
  AscendKVManager.process_layers(layers_params)
disaggregation/ascend/transfer_engine.py:
  AscendTransferEngine.__init__(self,hostname,npu_id,disaggregation_mode)
  AscendTransferEngine.initialize(self)
  AscendTransferEngine.batch_register(self,ptrs,lengths)
disaggregation/base/conn.py:
  BaseKVManager.__init__(self,args,disaggregation_mode,server_args,is_mla_backend)
  BaseKVSender.__init__(self,mgr,bootstrap_addr,bootstrap_room,dest_tp_ranks,pp_rank)
  BaseKVSender.init(self,num_kv_indices,aux_index)
  BaseKVSender.send(self,kv_indices)
  BaseKVSender.poll(self)
  BaseKVSender.failure_exception(self)
  BaseKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room)
  BaseKVReceiver.init(self,kv_indices,aux_index)
  BaseKVReceiver.poll(self)
  BaseKVReceiver.failure_exception(self)
  BaseKVBootstrapServer.__init__(self,port)
disaggregation/common/conn.py:
  CommonKVManager.__init__(self,args,disaggregation_mode,server_args,is_mla_backend)
  CommonKVManager._register_to_bootstrap(self)
  CommonKVManager._connect(self,endpoint,is_ipv6)
  CommonKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room,data_parallel_rank)
  CommonKVReceiver._get_bootstrap_info_from_server(self,engine_rank,target_dp_group)
  CommonKVReceiver._get_prefill_dp_size_from_server(self)
  CommonKVReceiver._connect(cls,endpoint,is_ipv6)
  CommonKVReceiver._connect_to_bootstrap_server(cls,bootstrap_info)
  CommonKVReceiver._register_kv_args(self)
  CommonKVReceiver.failure_exception(self)
  CommonKVBootstrapServer.__init__(self,port)
  CommonKVBootstrapServer.run(self)
  CommonKVBootstrapServer._setup_routes(self)
  async CommonKVBootstrapServer._handle_route(self,request)
  async CommonKVBootstrapServer._handle_route_put(self,request)
  async CommonKVBootstrapServer._handle_route_get(self,request)
  CommonKVBootstrapServer._run_server(self)
  CommonKVBootstrapServer.close(self)
  CommonKVBootstrapServer.poll(self)
disaggregation/common/utils.py:
  FastQueue.__init__(self)
  FastQueue.put(self,item)
  FastQueue.get(self)
  group_concurrent_contiguous(src_indices,dst_indices)
disaggregation/decode.py:
  DecodeReqToTokenPool.__init__(self,size,max_context_len,device,enable_memory_saver,pre_alloc_size)
  DecodeReqToTokenPool.write(self,indices,values)
  DecodeReqToTokenPool.available_size(self)
  DecodeReqToTokenPool.alloc(self,need_size)
  DecodeReqToTokenPool.free(self,free_index)
  DecodeReqToTokenPool.clear(self)
  DecodePreallocQueue.__init__(self,req_to_token_pool,token_to_kv_pool_allocator,draft_token_to_kv_pool,req_to_metadata_buffer_idx_allocator,metadata_buffers,scheduler,transfer_queue,tree_cache,gloo_group,tp_rank,tp_size,dp_size,gpu_id,bootstrap_port,max_total_num_tokens,prefill_pp_size,num_reserved_decode_tokens,transfer_backend)
  DecodePreallocQueue._init_kv_manager(self)
  DecodePreallocQueue.add(self,req,is_retracted)
  DecodePreallocQueue._check_if_req_exceed_kv_capacity(self,req)
  DecodePreallocQueue.extend(self,reqs,is_retracted)
  DecodePreallocQueue.resume_retracted_reqs(self)
  DecodePreallocQueue._update_handshake_waiters(self)
  DecodePreallocQueue.pop_preallocated(self)
  DecodePreallocQueue.num_tokens_pre_allocated(self)
  DecodePreallocQueue._allocatable_tokens(self,retractable_tokens,count_retracted)
  DecodePreallocQueue._pre_alloc(self,req)
  DecodeTransferQueue.__init__(self,gloo_group,req_to_metadata_buffer_idx_allocator,tp_rank,metadata_buffers,scheduler,tree_cache)
  DecodeTransferQueue.add(self,decode_req)
  DecodeTransferQueue.extend(self,decode_reqs)
  DecodeTransferQueue.pop_transferred(self)
  SchedulerDisaggregationDecodeMixin.event_loop_normal_disagg_decode(self)
  SchedulerDisaggregationDecodeMixin.event_loop_overlap_disagg_decode(self)
  SchedulerDisaggregationDecodeMixin._prepare_idle_batch_and_run(self,batch,delay_process)
  SchedulerDisaggregationDecodeMixin.get_next_disagg_decode_batch_to_run(self)
  SchedulerDisaggregationDecodeMixin.get_new_prebuilt_batch(self)
  SchedulerDisaggregationDecodeMixin.process_decode_queue(self)
disaggregation/decode_schedule_batch_mixin.py:
  ScheduleBatchDisaggregationDecodeMixin.prepare_for_prebuilt_extend(self)
  ScheduleBatchDisaggregationDecodeMixin.process_prebuilt_extend(self,server_args,model_config)
disaggregation/fake/conn.py:
  FakeKVSender.__init__(self,mgr,bootstrap_addr,bootstrap_room,dest_tp_ranks,pp_rank)
  FakeKVSender.poll(self)
  FakeKVSender.init(self,kv_indices,aux_index)
  FakeKVSender.send(self,kv_indices)
  FakeKVSender.failure_exception(self)
  FakeKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room,data_parallel_rank)
  FakeKVReceiver.poll(self)
  FakeKVReceiver.init(self,kv_indices,aux_index)
  FakeKVReceiver.failure_exception(self)
disaggregation/kv_events.py:
  EventPublisher.__init__(self,attn_dp_rank)
  EventPublisher.publish(self,events)
  EventPublisher.shutdown(self)
  NullEventPublisher.publish(self,events)
  NullEventPublisher.shutdown(self)
  ZmqEventPublisher.__init__(self,attn_dp_rank,endpoint,replay_endpoint,buffer_steps,hwm,max_queue_size,topic)
  ZmqEventPublisher.publish(self,events)
  ZmqEventPublisher.shutdown(self)
  ZmqEventPublisher._socket_setup(self)
  ZmqEventPublisher._publisher_thread(self)
  ZmqEventPublisher._service_replay(self)
  ZmqEventPublisher.offset_endpoint_port(endpoint,data_parallel_rank)
  KVEventsConfig.from_cli(cls,cli_value)
  EventPublisherFactory.register_publisher(cls,name,ctor)
  EventPublisherFactory.create(cls,config,attn_dp_rank)
disaggregation/launch_lb.py:
  LBArgs.add_cli_args(parser)
  LBArgs.from_cli_args(cls,args)
  main()
disaggregation/mini_lb.py:
  setup_logger()
  MiniLoadBalancer.__init__(self,prefill_configs,decode_servers,timeout)
  MiniLoadBalancer.add_prefill_server(self,new_prefill_config)
  MiniLoadBalancer.add_decode_server(self,new_decode_server)
  MiniLoadBalancer.select_pair(self)
  async MiniLoadBalancer.generate(self,modified_request,prefill_server,decode_server,endpoint)
  async MiniLoadBalancer.generate_stream(self,modified_request,prefill_server,decode_server,endpoint)
  async MiniLoadBalancer.stream_results()
  async health_check()
  async health_check()
  async flush_cache()
  async get_server_info()
  async get_model_info()
  async handle_generate_request(request_data)
  async _forward_to_backend(request_data,endpoint_name)
  async handle_chat_completion_request(request_data)
  async handle_completion_request(request_data)
  _generate_bootstrap_room()
  _get_request_batch_size(request)
  async get_models()
  async register(obj)
  run(prefill_configs,decode_addrs,host,port,timeout)
disaggregation/mooncake/conn.py:
  KVTransferError.__init__(self,bootstrap_room,failure_reason)
  KVTransferError.__str__(self)
  TransferInfo.from_zmq(cls,msg)
  KVArgsRegisterInfo.from_zmq(cls,msg)
  AuxDataCodec.serialize_data_from_buffer(src_addr,data_length)
  AuxDataCodec.deserialize_data_to_buffer(kv_args,buffer_index,aux_index,data)
  MooncakeKVManager.__init__(self,args,disaggregation_mode,server_args,is_mla_backend)
  MooncakeKVManager.init_engine(self)
  MooncakeKVManager.register_buffer_to_engine(self)
  MooncakeKVManager._connect(self,endpoint,is_ipv6)
  MooncakeKVManager._transfer_data(self,mooncake_session_id,transfer_blocks)
  MooncakeKVManager.send_kvcache(self,mooncake_session_id,prefill_kv_indices,dst_kv_ptrs,dst_kv_indices,executor)
  MooncakeKVManager.set_transfer_blocks(src_ptr,dst_ptr,item_len)
  MooncakeKVManager.process_layer(src_ptr,dst_ptr,item_len)
  MooncakeKVManager.process_layers(layers_params)
  MooncakeKVManager.send_kvcache_slice(self,mooncake_session_id,prefill_kv_indices,dst_kv_ptrs,dst_kv_indices,dst_tp_rank,dst_attn_tp_size,dst_kv_item_len,executor)
  MooncakeKVManager.process_layer_tp_aware(layer_params)
  MooncakeKVManager.send_aux(self,req,prefill_aux_index,dst_aux_ptrs)
  MooncakeKVManager.send_aux_tcp(self,req,prefill_aux_index,dst_aux_ptrs)
  MooncakeKVManager.send_aux_data_to_endpoint(self,remote,dst_port,room,buffer_index,aux_index,data)
  MooncakeKVManager.sync_status_to_decode_endpoint(self,remote,dst_port,room,status,prefill_rank)
  MooncakeKVManager.transfer_worker(self,queue,executor)
  MooncakeKVManager._bind_server_socket(self)
  MooncakeKVManager.start_prefill_thread(self)
  MooncakeKVManager.bootstrap_thread()
  MooncakeKVManager._handle_aux_data(self,msg)
  MooncakeKVManager.start_decode_thread(self)
  MooncakeKVManager.decode_thread()
  MooncakeKVManager.heartbeat_checker()
  MooncakeKVManager.add_transfer_request(self,bootstrap_room,kv_indices,index_slice,is_last,aux_index)
  MooncakeKVManager.check_status(self,bootstrap_room)
  MooncakeKVManager.update_status(self,bootstrap_room,status)
  MooncakeKVManager.record_failure(self,bootstrap_room,failure_reason)
  MooncakeKVManager.get_session_id(self)
  MooncakeKVManager._register_to_bootstrap(self)
  MooncakeKVManager._handle_node_failure(self,failed_bootstrap_addr)
  MooncakeKVSender.__init__(self,mgr,bootstrap_addr,bootstrap_room,dest_tp_ranks,pp_rank)
  MooncakeKVSender.init(self,num_kv_indices,aux_index)
  MooncakeKVSender.send(self,kv_indices)
  MooncakeKVSender.poll(self)
  MooncakeKVSender.clear(self)
  MooncakeKVSender.failure_exception(self)
  MooncakeKVSender.abort(self)
  MooncakeKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room,data_parallel_rank)
  MooncakeKVReceiver._get_bootstrap_info_from_server(self,engine_rank,target_dp_group,target_pp_rank)
  MooncakeKVReceiver._get_prefill_parallel_info_from_server(self)
  MooncakeKVReceiver._register_kv_args(self)
  MooncakeKVReceiver._connect(cls,endpoint,is_ipv6)
  MooncakeKVReceiver._connect_to_bootstrap_server(cls,bootstrap_info)
  MooncakeKVReceiver.init(self,kv_indices,aux_index)
  MooncakeKVReceiver.poll(self)
  MooncakeKVReceiver.clear(self)
  MooncakeKVReceiver.failure_exception(self)
  MooncakeKVReceiver.abort(self)
  MooncakeKVBootstrapServer.__init__(self,port)
  MooncakeKVBootstrapServer.run(self)
  MooncakeKVBootstrapServer._setup_routes(self)
  async MooncakeKVBootstrapServer._handle_health_check(self,request)
  async MooncakeKVBootstrapServer._handle_route(self,request)
  async MooncakeKVBootstrapServer._handle_route_put(self,request)
  async MooncakeKVBootstrapServer._handle_route_get(self,request)
  MooncakeKVBootstrapServer._run_server(self)
  MooncakeKVBootstrapServer.close(self)
  MooncakeKVBootstrapServer.poll(self)
disaggregation/mooncake/transfer_engine.py:
  MooncakeTransferEngine.__init__(self,hostname,gpu_id,ib_device)
  MooncakeTransferEngine.register(self,ptr,length)
  MooncakeTransferEngine.deregister(self,ptr)
  MooncakeTransferEngine.batch_register(self,ptrs,lengths)
  MooncakeTransferEngine.batch_deregister(self,ptrs)
  MooncakeTransferEngine.initialize(self,hostname,device_name)
  MooncakeTransferEngine.transfer_sync(self,session_id,buffer,peer_buffer_address,length)
  MooncakeTransferEngine.batch_transfer_sync(self,session_id,buffers,peer_buffer_addresses,lengths)
  MooncakeTransferEngine.get_session_id(self)
disaggregation/nixl/conn.py:
  TransferInfo.is_dummy(self)
  TransferInfo.from_zmq(cls,msg)
  KVArgsRegisterInfo.from_zmq(cls,msg)
  TransferStatus.is_done(self)
  NixlKVManager.__init__(self,args,disaggregation_mode,server_args,is_mla_backend)
  NixlKVManager.check_status(self,bootstrap_room)
  NixlKVManager.update_status(self,bootstrap_room,status)
  NixlKVManager.register_buffer_to_engine(self)
  NixlKVManager._add_remote_peer(self,decode_kv_args)
  NixlKVManager.send_kvcache(self,peer_name,prefill_kv_indices,dst_kv_ptrs,dst_kv_indices,dst_gpu_id,notif)
  NixlKVManager.send_aux(self,peer_name,prefill_aux_index,dst_aux_ptrs,dst_aux_index,notif)
  NixlKVManager.add_transfer_request(self,bootstrap_room,kv_indices,index_slice,is_last,chunk_id,aux_index)
  NixlKVManager.update_transfer_status(self)
  NixlKVManager.check_transfer_done(self,room)
  NixlKVManager._bind_server_socket(self)
  NixlKVManager._start_bootstrap_thread(self)
  NixlKVManager.bootstrap_thread()
  NixlKVSender.__init__(self,mgr,bootstrap_addr,bootstrap_room,dest_tp_ranks,pp_rank)
  NixlKVSender.init(self,num_kv_indices,aux_index)
  NixlKVSender.send(self,kv_indices)
  NixlKVSender.poll(self)
  NixlKVSender.failure_exception(self)
  NixlKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room,data_parallel_rank)
  NixlKVReceiver.init(self,kv_indices,aux_index)
  NixlKVReceiver.poll(self)
  NixlKVReceiver._register_kv_args(self)
  NixlKVReceiver.failure_exception(self)
disaggregation/prefill.py:
  PrefillBootstrapQueue.__init__(self,token_to_kv_pool,draft_token_to_kv_pool,req_to_metadata_buffer_idx_allocator,metadata_buffers,tp_rank,tp_size,gpu_id,bootstrap_port,gloo_group,max_total_num_tokens,decode_tp_size,decode_dp_size,scheduler,pp_rank,pp_size,transfer_backend)
  PrefillBootstrapQueue._init_kv_manager(self)
  PrefillBootstrapQueue.add(self,req,num_kv_heads)
  PrefillBootstrapQueue.extend(self,reqs,num_kv_heads)
  PrefillBootstrapQueue._check_if_req_exceed_kv_capacity(self,req)
  PrefillBootstrapQueue._process_req(self,req)
  PrefillBootstrapQueue.pop_bootstrapped(self,return_failed_reqs,rids_to_check)
  SchedulerDisaggregationPrefillMixin.event_loop_normal_disagg_prefill(self)
  SchedulerDisaggregationPrefillMixin.event_loop_overlap_disagg_prefill(self)
  SchedulerDisaggregationPrefillMixin.process_batch_result_disagg_prefill(self,batch,result,launch_done)
  SchedulerDisaggregationPrefillMixin.process_disagg_prefill_inflight_queue(self,rids_to_check)
  SchedulerDisaggregationPrefillMixin.get_transferred_rids(self)
  SchedulerDisaggregationPrefillMixin.process_prefill_chunk(self)
  SchedulerDisaggregationPrefillMixin.send_kv_chunk(self,req,last_chunk,end_idx)
  SchedulerDisaggregationPrefillMixin.event_loop_pp_disagg_prefill(self)
  SchedulerDisaggregationPrefillMixin.send_pyobj_to_next_stage(self,data)
  SchedulerDisaggregationPrefillMixin.recv_pyobj_from_prev_stage(self)
disaggregation/utils.py:
  poll_and_all_reduce(pollers,gloo_group)
  ReqToMetadataIdxAllocator.__init__(self,size)
  ReqToMetadataIdxAllocator.available_size(self)
  ReqToMetadataIdxAllocator.alloc(self)
  ReqToMetadataIdxAllocator.free(self,free_index)
  MetadataBuffers.__init__(self,size,hidden_size,dtype,max_top_logprobs_num,custom_mem_pool)
  MetadataBuffers.get_buf_infos(self)
  MetadataBuffers.get_buf(self,idx)
  MetadataBuffers.set_buf(self,req)
  get_kv_class(transfer_backend,class_type)
  kv_to_page_indices(kv_indices,page_size)
  kv_to_page_num(num_kv_indices,page_size)
  PDRegistryRequest.__post_init__(self)
  register_disaggregation_server(mode,server_port,bootstrap_port,pdlb_url)
  is_mla_backend(target_kv_pool)
  prepare_abort(req,error_message,status_code)
distributed/communication_op.py:
  tensor_model_parallel_all_reduce(input_)
  tensor_model_parallel_all_gather(input_,dim)
  tensor_model_parallel_gather(input_,dst,dim)
  broadcast_tensor_dict(tensor_dict,src)
distributed/device_communicators/cuda_wrapper.py:
  find_loaded_library(lib_name)
  CudaRTLibrary.__init__(self,so_file)
  CudaRTLibrary.CUDART_CHECK(self,result)
  CudaRTLibrary.cudaGetErrorString(self,error)
  CudaRTLibrary.cudaSetDevice(self,device)
  CudaRTLibrary.cudaDeviceSynchronize(self)
  CudaRTLibrary.cudaDeviceReset(self)
  CudaRTLibrary.cudaMalloc(self,size)
  CudaRTLibrary.cudaFree(self,devPtr)
  CudaRTLibrary.cudaMemset(self,devPtr,value,count)
  CudaRTLibrary.cudaMemcpy(self,dst,src,count)
  CudaRTLibrary.cudaIpcGetMemHandle(self,devPtr)
  CudaRTLibrary.cudaIpcOpenMemHandle(self,handle)
distributed/device_communicators/custom_all_reduce.py:
  _can_p2p(rank,world_size)
  CustomAllreduce.__init__(self,group,device,max_size)
  CustomAllreduce.create_shared_buffer(size_in_bytes,group)
  CustomAllreduce.free_shared_buffer(pointers,group)
  CustomAllreduce.capture(self)
  CustomAllreduce._get_ipc_meta(self,inp)
  CustomAllreduce._gather_ipc_meta(self,shard_data)
  CustomAllreduce.register_buffer(self,inp)
  CustomAllreduce.register_graph_buffers(self)
  CustomAllreduce.should_custom_ar(self,inp)
  CustomAllreduce.all_reduce_reg(self,inp,out)
  CustomAllreduce.all_reduce_unreg(self,inp,out)
  CustomAllreduce.all_reduce(self,inp,out,registered)
  CustomAllreduce.custom_all_reduce(self,input)
  CustomAllreduce.close(self)
  CustomAllreduce.__del__(self)
distributed/device_communicators/custom_all_reduce_utils.py:
  update_environment_variables(envs)
  producer(batch_src,producer_queue,consumer_queue,result_queue,cuda_visible_devices)
  consumer(batch_tgt,producer_queue,consumer_queue,result_queue,cuda_visible_devices)
  can_actually_p2p(batch_src,batch_tgt)
  gpu_p2p_access_check(src,tgt)
  with_nvml_context(fn)
  wrapper(*args,**kwargs)
  is_full_nvlink(physical_device_ids,world_size)
  is_weak_contiguous(inp)
distributed/device_communicators/hpu_communicator.py:
  HpuCommunicator.__init__(self,group)
  HpuCommunicator.all_reduce(self,x)
  HpuCommunicator.all_gather(self,x,dim)
distributed/device_communicators/npu_communicator.py:
  NpuCommunicator.__init__(self,group)
  NpuCommunicator.all_reduce(self,x)
  NpuCommunicator.all_gather(self,x,dim)
distributed/device_communicators/pymscclpp.py:
  mscclpp_is_weak_contiguous(inp)
  mscclpp_convert_to_bytes(size_str)
  mscclpp_bench_time(func,test_niter,warmup_niter)
  PyMscclppCommunicator.__init__(self,group,device,max_bytes)
  PyMscclppCommunicator.pre_tune_config(self,dtype)
  PyMscclppCommunicator.should_mscclpp_allreduce(self,inp,op)
  PyMscclppCommunicator.all_reduce(self,tensor,op)
  PyMscclppCommunicator.change_state(self,enable)
distributed/device_communicators/pynccl.py:
  PyNcclCommunicator.__init__(self,group,device,library_path)
  PyNcclCommunicator.all_reduce(self,tensor,op,stream)
  PyNcclCommunicator.all_gather(self,output_tensor,input_tensor,stream,sizes)
  PyNcclCommunicator.reduce_scatter(self,output_tensor,input_tensor,op,stream,sizes)
  PyNcclCommunicator.send(self,tensor,dst,stream)
  PyNcclCommunicator.recv(self,tensor,src,stream)
  PyNcclCommunicator.broadcast(self,tensor,src,stream)
  PyNcclCommunicator.register_comm_window_raw(self,ptr,size)
  PyNcclCommunicator.deregister_comm_window(self,window)
  PyNcclCommunicator.group_start(self)
  PyNcclCommunicator.group_end(self)
  PyNcclCommunicator.change_state(self,enable,stream)
distributed/device_communicators/pynccl_allocator.py:
  is_symmetric_memory_enabled()
  set_graph_pool_id(graph_pool_id)
  get_nccl_mem_pool()
  use_symmetric_memory.__init__(self,group_coordinator)
  use_symmetric_memory.__enter__(self)
  use_symmetric_memory.tag(self,tensor)
  use_symmetric_memory.__exit__(self,exc_type,exc_val,exc_tb)
distributed/device_communicators/pynccl_wrapper.py:
  find_nccl_library()
  ncclDataTypeEnum.from_torch(cls,dtype)
  ncclRedOpTypeEnum.from_torch(cls,op)
  NCCLLibrary.__init__(self,so_file)
  NCCLLibrary.ncclGetErrorString(self,result)
  NCCLLibrary.NCCL_CHECK(self,result)
  NCCLLibrary.ncclGetRawVersion(self)
  NCCLLibrary.ncclGetVersion(self)
  NCCLLibrary.ncclGetUniqueId(self)
  NCCLLibrary.ncclCommInitRank(self,world_size,unique_id,rank)
  NCCLLibrary.ncclAllReduce(self,sendbuff,recvbuff,count,datatype,op,comm,stream)
  NCCLLibrary.ncclReduce(self,sendbuff,recvbuff,count,datatype,op,root,comm,stream)
  NCCLLibrary.ncclReduceScatter(self,sendbuff,recvbuff,count,datatype,op,comm,stream)
  NCCLLibrary.ncclAllGather(self,sendbuff,recvbuff,count,datatype,comm,stream)
  NCCLLibrary.ncclSend(self,sendbuff,count,datatype,dest,comm,stream)
  NCCLLibrary.ncclRecv(self,recvbuff,count,datatype,src,comm,stream)
  NCCLLibrary.ncclBroadcast(self,sendbuff,recvbuff,count,datatype,root,comm,stream)
  NCCLLibrary.ncclCommDestroy(self,comm)
  NCCLLibrary.ncclCommWindowRegister(self,comm,buff,size,win_flags)
  NCCLLibrary.ncclCommWindowDeregister(self,comm,window)
  NCCLLibrary.ncclGroupStart(self)
  NCCLLibrary.ncclGroupEnd(self)
distributed/device_communicators/quick_all_reduce.py:
  qr_rocm_arch_available()
  QuickAllReduce.__init__(self,group,device)
  QuickAllReduce.init_quick_all_reduce(self)
  QuickAllReduce.create_shared_buffer(self)
  QuickAllReduce.should_quick_allreduce(self,inp)
  QuickAllReduce.quick_all_reduce(self,inp,out)
  QuickAllReduce.close(self)
  QuickAllReduce.__del__(self)
distributed/device_communicators/shm_broadcast.py:
  ShmRingBuffer.__init__(self,n_reader,max_chunk_bytes,max_chunks,name)
  ShmRingBuffer.__reduce__(self)
  ShmRingBuffer.__del__(self)
  ShmRingBuffer.get_data(self,current_idx)
  ShmRingBuffer.get_metadata(self,current_idx)
  MessageQueue.__init__(self,n_reader,n_local_reader,local_reader_ranks,max_chunk_bytes,max_chunks,connect_ip)
  MessageQueue.export_handle(self)
  MessageQueue.create_from_handle(handle,rank)
  MessageQueue.wait_until_ready(self)
  MessageQueue.acquire_write(self)
  MessageQueue.acquire_read(self)
  MessageQueue.enqueue(self,obj)
  MessageQueue.dequeue(self)
  MessageQueue.broadcast_object(self,obj)
  MessageQueue.create_from_process_group(pg,max_chunk_bytes,max_chunks,writer_rank)
distributed/device_communicators/xpu_communicator.py:
  XpuCommunicator.__init__(self,group)
  XpuCommunicator.all_reduce(self,x)
  XpuCommunicator.gather(self,input_,rank_in_group,dst,dim)
distributed/naive_distributed.py:
  NaiveDistributed.__init__(self,rank,world_size,rendezvous)
  NaiveDistributed.get_rank(self)
  NaiveDistributed.get_world_size(self)
  NaiveDistributed.scatter(self,tensor,scatter_list,src)
  NaiveDistributed.all_gather_object(self,obj)
  NaiveDistributed._get_path(interesting_rank)
  NaiveDistributed._read_one(interesting_rank)
  NaiveDistributed.barrier(self)
  get_naive_distributed()
  set_naive_distributed(instance)
distributed/parallel_state.py:
  _split_tensor_dict(tensor_dict)
  _get_unique_name(name)
  _register_group(group)
  inplace_all_reduce(tensor,group_name)
  inplace_all_reduce_fake(tensor,group_name)
  outplace_all_reduce(tensor,group_name,outplace_all_reduce_method)
  outplace_all_reduce_fake(tensor,group_name,outplace_all_reduce_method)
  reg_all_gather_into_tensor(output,input,group_name)
  reg_all_gather_into_tensor_fake(output,input,group_name)
  GroupCoordinator.__init__(self,group_ranks,local_rank,torch_distributed_backend,use_pynccl,use_pymscclpp,use_custom_allreduce,use_hpu_communicator,use_xpu_communicator,use_npu_communicator,use_message_queue_broadcaster,group_name)
  GroupCoordinator.__repr__(self)
  GroupCoordinator.first_rank(self)
  GroupCoordinator.last_rank(self)
  GroupCoordinator.is_first_rank(self)
  GroupCoordinator.is_last_rank(self)
  GroupCoordinator.next_rank(self)
  GroupCoordinator.prev_rank(self)
  GroupCoordinator.graph_capture(self,graph_capture_context)
  GroupCoordinator.all_reduce(self,input_)
  GroupCoordinator._all_reduce_out_place(self,input_,outplace_all_reduce_method)
  GroupCoordinator._all_reduce_in_place(self,input_)
  GroupCoordinator.reduce_scatter_tensor(self,output,input)
  GroupCoordinator.reduce_scatter(self,output,input_list)
  GroupCoordinator.reduce_scatterv(self,input_,output,sizes)
  GroupCoordinator._all_gather_into_tensor(self,output,input)
  GroupCoordinator.all_gather_into_tensor(self,output,input)
  GroupCoordinator.all_gather(self,input_,dim,output_tensor_list)
  GroupCoordinator.all_gatherv(self,input_,sizes)
  GroupCoordinator._all_gather_single(input_,sizes)
  GroupCoordinator.gather(self,input_,dst,dim)
  GroupCoordinator.broadcast(self,input_,src)
  GroupCoordinator.broadcast_object(self,obj,src)
  GroupCoordinator.broadcast_object_list(self,obj_list,src,group)
  GroupCoordinator.all_gather_object(self,obj)
  GroupCoordinator.send_object(self,obj,dst)
  GroupCoordinator.recv_object(self,src)
  GroupCoordinator.broadcast_tensor_dict(self,tensor_dict,src,group,metadata_group)
  GroupCoordinator.send_tensor_dict(self,tensor_dict,dst,all_gather_group)
  GroupCoordinator.recv_tensor_dict(self,src,all_gather_group)
  GroupCoordinator.barrier(self)
  GroupCoordinator.send(self,tensor,dst)
  GroupCoordinator.recv(self,size,dtype,src)
  GroupCoordinator.destroy(self)
  get_world_group()
  init_world_group(ranks,local_rank,backend)
  init_model_parallel_group(group_ranks,local_rank,backend,use_custom_allreduce,use_message_queue_broadcaster,group_name,use_mscclpp_allreduce)
  set_pdmux_status(enable_prefill_multiplexing)
  get_tp_group()
  get_moe_ep_group()
  get_moe_tp_group()
  get_pp_group()
  graph_capture()
  set_custom_all_reduce(enable)
  set_mscclpp_all_reduce(enable)
  init_distributed_environment(world_size,rank,distributed_init_method,local_rank,backend,timeout)
  initialize_model_parallel(tensor_model_parallel_size,expert_model_parallel_size,pipeline_model_parallel_size,backend,duplicate_tp_group)
  ensure_model_parallel_initialized(tensor_model_parallel_size,expert_model_parallel_size,pipeline_model_parallel_size,backend)
  model_parallel_is_initialized()
  patch_tensor_parallel_group(tp_group)
  get_tensor_model_parallel_world_size()
  get_tensor_model_parallel_rank()
  get_moe_expert_parallel_world_size()
  get_moe_expert_parallel_rank()
  get_moe_tensor_parallel_world_size()
  get_moe_tensor_parallel_rank()
  destroy_model_parallel()
  destroy_distributed_environment()
  cleanup_dist_env_and_memory(shutdown_ray)
  in_the_same_node_as(pg,source_rank)
  monkey_patch_vllm_parallel_state(reverse)
distributed/utils.py:
  ensure_divisibility(numerator,denominator)
  divide(numerator,denominator)
  split_tensor_along_last_dim(tensor,num_partitions,contiguous_split_chunks)
  get_pp_indices(num_hidden_layers,pp_rank,pp_size)
  StatelessProcessGroup.__post_init__(self)
  StatelessProcessGroup.send_obj(self,obj,dst)
  StatelessProcessGroup.expire_data(self)
  StatelessProcessGroup.recv_obj(self,src)
  StatelessProcessGroup.broadcast_obj(self,obj,src)
  StatelessProcessGroup.all_gather_obj(self,obj)
  StatelessProcessGroup.barrier(self)
  StatelessProcessGroup.create(host,port,rank,world_size,data_expiration_seconds)
eplb/eplb_algorithms/__init__.py:
  rebalance_experts(tokens_per_expert,num_physical_experts,num_local_physical_experts,num_groups,num_nodes,algorithm)
  compute_algorithm(raw_algorithm,num_groups,num_nodes)
eplb/eplb_algorithms/deepseek.py:
  balanced_packing(weight,num_packs)
  replicate_experts(weight,num_phy)
  rebalance_experts_hierarchical(weight,num_physical_experts,num_groups,num_nodes,num_gpus)
  inverse(perm)
  rebalance_experts(weight,num_replicas,num_groups,num_nodes,num_gpus,enable_hierarchical)
eplb/eplb_algorithms/deepseek_vec.py:
  pack_groups(tokens_per_group,num_nodes)
  key_func(rank)
  make_redundant_experts_chunkwise(tokens_per_expert,num_physical_experts,num_local_physical_experts,num_physical_experts_per_chunk)
  decode_rebalance_experts(tokens_per_expert,num_physical_experts,num_local_physical_experts)
  prefill_rebalance_experts(tokens_per_expert,num_physical_experts,num_local_physical_experts,num_groups,num_nodes)
  rebalance_experts(tokens_per_expert,num_physical_experts,num_local_physical_experts,num_groups,num_nodes,enable_hierarchical)
eplb/eplb_manager.py:
  EPLBManager.__init__(self,model_runner)
  EPLBManager.on_forward_pass_end(self)
  EPLBManager._entrypoint(self)
  EPLBManager.rebalance(self)
  EPLBManager._check_rebalance_needed(self,average_utilization_rate_over_window)
  EPLBManager._compute_update_layer_ids_chunks(self)
  _chunk_list(items,chunk_size)
eplb/eplb_simulator/reader.py:
  read_mode_per_pass(dir_data)
eplb/expert_distribution.py:
  ExpertDistributionRecorder.init_new(server_args,expert_location_metadata,rank)
  ExpertDistributionRecorder.with_current_layer(self,layer_idx)
  ExpertDistributionRecorder.with_debug_name(self,debug_name)
  ExpertDistributionRecorder.disable_this_region(self)
  ExpertDistributionRecorder.with_forward_pass(self,forward_pass_id,forward_batch)
  ExpertDistributionRecorder.on_select_experts(self,topk_ids)
  ExpertDistributionRecorder.on_deepep_dispatch_normal(self,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  ExpertDistributionRecorder.on_deepep_dispatch_low_latency(self,local_physical_count_of_layer)
  ExpertDistributionRecorder.start_record(self)
  ExpertDistributionRecorder.stop_record(self)
  ExpertDistributionRecorder.dump_record(self,output_mode)
  ExpertDistributionRecorder.recording(self)
  ExpertDistributionRecorder._on_not_implemented(self)
  _ExpertDistributionRecorderReal.__init__(self,server_args,expert_location_metadata,rank)
  _ExpertDistributionRecorderReal.with_current_layer(self,layer_idx)
  _ExpertDistributionRecorderReal.with_debug_name(self,debug_name)
  _ExpertDistributionRecorderReal.with_forward_pass(self,forward_pass_id,forward_batch)
  _ExpertDistributionRecorderReal.disable_this_region(self)
  _ExpertDistributionRecorderReal._on_forward_pass_start(self,forward_batch)
  _ExpertDistributionRecorderReal._on_forward_pass_end(self,forward_pass_id)
  _ExpertDistributionRecorderReal.on_select_experts(self,topk_ids)
  _ExpertDistributionRecorderReal.on_deepep_dispatch_normal(self,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  _ExpertDistributionRecorderReal.on_deepep_dispatch_low_latency(self,local_physical_count_of_layer)
  _ExpertDistributionRecorderReal._on_hook(self,hook_name,**kwargs)
  _ExpertDistributionRecorderReal._reset(self)
  _ExpertDistributionRecorderReal.start_record(self)
  _ExpertDistributionRecorderReal.stop_record(self)
  _ExpertDistributionRecorderReal.dump_record(self,output_mode)
  _ExpertDistributionRecorderReal.recording(self)
  get_global_expert_distribution_recorder()
  set_global_expert_distribution_recorder(value)
  _SinglePassGatherer.init_new(server_args,expert_location_metadata,rank)
  _SinglePassGatherer.__init__(self,expert_location_metadata,rank)
  _SinglePassGatherer.on_forward_pass_start(self,forward_batch)
  _SinglePassGatherer.on_select_experts(self,layer_idx,topk_ids)
  _SinglePassGatherer.on_deepep_dispatch_normal(self,layer_idx,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  _SinglePassGatherer.on_deepep_dispatch_low_latency(self,layer_idx,local_physical_count_of_layer)
  _SinglePassGatherer.reset(self)
  _SinglePassGatherer.collect(self)
  _DetailSinglePassGatherer.__init__(self,server_args,expert_location_metadata,rank)
  _DetailSinglePassGatherer.on_forward_pass_start(self,forward_batch)
  _DetailSinglePassGatherer.on_select_experts(self,layer_idx,topk_ids)
  _DetailSinglePassGatherer.on_deepep_dispatch_normal(self,layer_idx,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  _DetailSinglePassGatherer.reset(self)
  _DetailSinglePassGatherer.collect(self)
  _LayerBasedCpuSinglePassGatherer.__init__(self,*args,**kwargs)
  _LayerBasedCpuSinglePassGatherer._on_layer_data(self,layer_idx,objects)
  _LayerBasedCpuSinglePassGatherer.reset(self)
  _LayerBasedCpuSinglePassGatherer._collect_objects(self,pad_len)
  _list_sum(a,b)
  _LayerBasedGpuSinglePassGatherer.__init__(self,*args,enable_global_physical_experts,**kwargs)
  _LayerBasedGpuSinglePassGatherer.reset(self)
  _LayerBasedGpuSinglePassGatherer.collect(self)
  _SelectExpertsSinglePassGatherer.__init__(self,*args,**kwargs)
  _SelectExpertsSinglePassGatherer.on_select_experts(self,layer_idx,topk_ids)
  _DeepepNormalSinglePassGatherer.__init__(self,*args,**kwargs)
  _DeepepNormalSinglePassGatherer.on_deepep_dispatch_normal(self,layer_idx,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  _DeepepNormalSinglePassGatherer.collect(self)
  _DeepepLowLatencySinglePassGatherer.__init__(self,*args,**kwargs)
  _DeepepLowLatencySinglePassGatherer.on_deepep_dispatch_low_latency(self,layer_idx,local_physical_count_of_layer)
  _convert_local_to_global_physical_count(local_physical_count,rank,num_local_physical_experts,num_physical_experts)
  _Accumulator.init_new(server_args,expert_location_metadata,rank)
  _Accumulator.get_class(server_args)
  _Accumulator.__init__(self,server_args,expert_location_metadata,rank)
  _Accumulator.get_single_pass_gatherer_keys(self)
  _Accumulator.get_single_pass_gatherer_key(self,debug_name)
  _Accumulator.append(self,forward_pass_id,gatherer_key,single_pass_data)
  _Accumulator.reset(self)
  _Accumulator.dump(self,output_mode)
  _UtilizationRateAccumulatorMixin.__init__(self,*args,**kwargs)
  _UtilizationRateAccumulatorMixin.append(self,forward_pass_id,gatherer_key,single_pass_data)
  _UtilizationRateAccumulatorMixin.reset(self)
  _UtilizationRateAccumulatorMixin._append_utilization_rate(self,forward_pass_id,single_pass_global_physical_count)
  _DequeCollection.__init__(self,maxlens)
  _DequeCollection.append(self,value)
  _DequeCollection.clear(self)
  _DequeCollection.mean(self)
  _DetailAccumulator.__init__(self,*args,**kwargs)
  _DetailAccumulator.get_single_pass_gatherer_keys(self)
  _DetailAccumulator.get_single_pass_gatherer_key(self,debug_name)
  _DetailAccumulator.append(self,forward_pass_id,gatherer_key,single_pass_data)
  _DetailAccumulator._process_object(obj)
  _DetailAccumulator.reset(self)
  _DetailAccumulator.dump(self,output_mode)
  _StatAccumulator.__init__(self,*args,**kwargs)
  _StatAccumulator.append(self,forward_pass_id,gatherer_key,single_pass_data)
  _StatAccumulator.reset(self)
  _StatAccumulator.dump(self,output_mode)
  _StatAccumulator._get_global_average_utilization_rate(self)
  _dump_to_file(name,data)
  _Buffer.init_new(item_shape,buffer_size,dtype,device)
  _Buffer.append(self,value)
  _Buffer.get_all(self)
  _Buffer.reset(self)
  _CircularBuffer.__init__(self,item_shape,buffer_size,dtype,device)
  _CircularBuffer.append(self,value)
  _CircularBuffer.get_all(self)
  _CircularBuffer.reset(self)
  _InfiniteBuffer.__init__(self,item_shape,dtype,device)
  _InfiniteBuffer.append(self,value)
  _InfiniteBuffer.get_all(self)
  _InfiniteBuffer.reset(self)
  _convert_global_physical_count_to_logical_count(global_physical_count,num_layers,num_logical_experts,physical_to_logical_map)
  compute_gpu_physical_count(physical_count_of_whatever,num_gpu)
  compute_utilization_rate(gpu_physical_count_of_batch)
eplb/expert_location.py:
  ExpertLocationMetadata.num_layers(self)
  ExpertLocationMetadata.num_physical_experts(self)
  ExpertLocationMetadata.num_local_physical_experts(self)
  ExpertLocationMetadata.num_logical_experts(self)
  ExpertLocationMetadata.ep_size(self)
  ExpertLocationMetadata.__post_init__(self)
  ExpertLocationMetadata.init_trivial(server_args,model_config)
  ExpertLocationMetadata.init_by_mapping(server_args,model_config,physical_to_logical_map)
  ExpertLocationMetadata.init_by_eplb(server_args,model_config,logical_count)
  ExpertLocationMetadata._init_common(server_args,model_config)
  ExpertLocationMetadata._init_raw(server_args,ep_size,physical_to_logical_map,logical_to_all_physical_map)
  ExpertLocationMetadata.update(self,other,update_layer_ids)
  ExpertLocationMetadata.logical_to_all_physical(self,layer_id,logical_expert_id)
  get_global_expert_location_metadata()
  set_global_expert_location_metadata(value)
  _compute_logical_to_all_physical_map(physical_to_logical_map,num_logical_experts)
  _pad_nested_array(arr,pad_value)
  compute_logical_to_rank_dispatch_physical_map(logical_to_all_physical_map,num_gpus,num_physical_experts,ep_rank,seed)
  _logical_to_all_physical_raw(logical_to_all_physical_map,layer_id,logical_expert_id)
  _compute_gpu_id_of_physical_expert(physical_expert_id,num_local_physical_experts)
  _fair_choices(arr,k,r)
  ModelConfigForExpertLocation.from_model_config(model_config)
  compute_initial_expert_location_metadata(server_args,model_config)
eplb/expert_location_dispatch.py:
  ExpertLocationDispatchInfo.init_new(cls,layer_id)
  transform_select_experts_inputs(router_logits,correction_bias,info)
  topk_ids_logical_to_physical(topk_ids,info)
  _topk_ids_logical_to_physical_static(topk_ids,info)
  _topk_ids_logical_to_physical_dynamic(topk_ids,info)
eplb/expert_location_updater.py:
  ExpertLocationUpdater.__init__(self)
  ExpertLocationUpdater.update(self,routed_experts_weights_of_layer,new_expert_location_metadata,update_layer_ids,nnodes,rank)
  _update_expert_weights(**kwargs)
  _update_expert_weights_with_canary(routed_experts_weights_of_layer,old_expert_location_metadata,new_expert_location_metadata,update_layer_ids,nnodes,rank)
  _get_canary_value(meta,layer_id)
  _update_expert_weights_raw(routed_experts_weights_of_layer,old_expert_location_metadata,new_expert_location_metadata,update_layer_ids,nnodes,rank)
  create_temp_buffers(sample_tensors)
  update_expert_weights_single_layer(routed_experts_weights,temp_buffers,old_physical_to_logical_map,new_physical_to_logical_map,num_local_physical_experts,num_gpu_per_node,rank,world_size,debug,log_metrics)
  _entrypoint()
  _handle_recv(buffer2weight_copy_infos,p2p_op_infos)
  _handle_recv_of_dst_expert_location(dst_expert_location,buffer2weight_copy_infos,p2p_op_infos)
  _create_p2p_recv_and_buffer2weight_copy(buffer2weight_copy_infos,p2p_op_infos,logical_expert_id,src_rank,dst_expert_location)
  _create_isend_ops(p2p_op_infos)
  _create_isend_ops_of_logical_expert_id(logical_expert_id,src_expert_location,p2p_op_infos)
  _compute_comm_info(logical_expert_id)
  _execute_p2p_ops(p2p_op_infos)
  _execute_buffer2weight_copies(buffer2weight_copy_infos)
  _get_tensor(tensors,tensor_index,expert_location)
  _get_local_expert_location(expert_location)
  _ChunkUtils.__init__(self,chunk_values,element_values)
  _ChunkUtils.chunk_value_from_element_value(self,element_value)
  _ChunkUtils.element_values_from_chunk_value(self,chunk_value)
  _ChunkUtils._chunk_index_from_element_index(num_elements,num_chunks,element_index)
  _ChunkUtils._element_slice_from_chunk_index(num_elements,num_chunks,chunk_index)
  _deduplicate_ordered(arr)
  _log_p2p_op_metrics(p2p_op_infos,num_gpu_per_node,world_size,self_node_id)
  _get_direction_from_op(op)
  _group_by(items,keyfunc)
harmony_parser.py:
  prefix_hold(text,tokens)
  iter_tokens(text,start_pos)
  CanonicalStrategy.__init__(self)
  CanonicalStrategy.parse(self,text)
  CanonicalStrategy._parse_partial_analysis(self,text,tokens,start_pos)
  CanonicalStrategy._extract_channel_type(self,header_text)
  CanonicalStrategy._parse_block(self,text,tokens,start_pos)
  CanonicalStrategy._is_commentary_filler_between_blocks(self,text,tokens,pos)
  CanonicalStrategy._is_standalone_structural_token(self,content)
  TextStrategy.__init__(self)
  TextStrategy.set_buffer_context(self,buffer)
  TextStrategy.parse(self,text)
  HarmonyParser.__init__(self)
  HarmonyParser.parse(self,chunk)
hf_transformers_utils.py:
  download_from_hf(model_path,allow_patterns)
  get_hf_text_config(config)
  get_config(model,trust_remote_code,revision,model_override_args,**kwargs)
  get_generation_config(model,trust_remote_code,revision,**kwargs)
  get_sparse_attention_config(model,sparse_attention_config_filename)
  get_context_length(config)
  get_tokenizer(tokenizer_name,*args,tokenizer_mode,trust_remote_code,tokenizer_revision,**kwargs)
  get_tokenizer_from_processor(processor)
  get_processor(tokenizer_name,*args,tokenizer_mode,trust_remote_code,tokenizer_revision,use_fast,**kwargs)
  attach_additional_stop_token_ids(tokenizer)
  check_gguf_file(model)
host_shared_memory.py:
  HostSharedMemoryManager.__init__(self,base_name)
  HostSharedMemoryManager.malloc(self,shape,dtype)
  HostSharedMemoryManager._malloc_raw(self,num_bytes)
  get_host_shared_memory_manager()
  set_host_shared_memory_manager(instance)
jinja_template_utils.py:
  _is_var_access(node,varname)
  _is_attr_access(node,varname,key)
  _is_var_or_elems_access(node,varname,key)
  _try_extract_ast(chat_template)
  detect_jinja_template_content_format(chat_template)
  process_content_for_template_format(msg_dict,content_format,image_data,video_data,audio_data,modalities)
layers/activation.py:
  SiluAndMul.forward_native(self,x)
  SiluAndMul.forward_cuda(self,x)
  SiluAndMul.forward_cpu(self,x)
  SiluAndMul.forward_npu(self,x)
  GeluAndMul.__init__(self,approximate)
  GeluAndMul.forward_native(self,x)
  GeluAndMul.forward_cuda(self,x)
  GeluAndMul.forward_npu(self,x)
  NewGELU.forward_native(self,x)
  NewGELU.forward_cuda(self,x)
  ReLU2.forward(self,x)
  QuickGELU.forward_native(self,x)
  QuickGELU.forward_cuda(self,x)
  QuickGELU.forward_hip(self,x)
  QuickGELU.forward_npu(self,x)
  ScaledActivation.__init__(self,act_module,intermediate_size,input_is_parallel,params_dtype)
  ScaledActivation.forward(self,x)
  ScaledActivation.weight_loader(self,param,loaded_weight)
  get_act_fn(act_fn_name,quant_config,intermediate_size,input_is_parallel,params_dtype)
  get_cross_encoder_activation_function(config)
layers/amx_utils.py:
  amx_process_weight_after_loading(weight)
  dim_is_supported(weight)
  _amx_process_weight_after_loading(module,weight_names,transpose_dims)
  PackWeightMethod.__init__(self,weight_names,transpose_dims)
  PackWeightMethod.process_weights_after_loading(self,module)
layers/attention/aiter_backend.py:
  AiterAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  AiterAttnBackend.init_forward_metadata(self,forward_batch)
  AiterAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  AiterAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AiterAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  AiterAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  AiterAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AiterAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  AiterIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  AiterIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,encoder_lens,spec_info)
  AiterIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,encoder_lens,spec_info)
  AiterMlaIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  AiterMlaIndicesUpdaterPrefill.update(self,req_pool_indices,kv_lens,kv_lens_sum,extend_lens,max_q_len,max_kv_len,spec_info)
  AiterMlaIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,kv_lens,kv_lens_sum,extend_lens,max_q_len,max_kv_len,spec_info)
  AiterMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  AiterMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  AiterMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
  AiterMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AiterMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
  AiterMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
layers/attention/ascend_backend.py:
  AscendAttnBackend.gen_attention_mask(self,max_seq_len,dtype)
  AscendAttnBackend.__init__(self,model_runner)
  AscendAttnBackend.init_forward_metadata(self,forward_batch)
  AscendAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AscendAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AscendAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  AscendAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  AscendAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AscendAttnBackend.forward_decode_graph(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  AscendAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
layers/attention/base_attn_backend.py:
  AttentionBackend.init_forward_metadata(self,forward_batch)
  AttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  AttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  AttentionBackend.forward(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  AttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  AttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AttentionBackend.support_triton(self)
layers/attention/cutlass_mla_backend.py:
  CutlassMLADecodeMetadata.__init__(self,workspace,block_kv_indices)
  CutlassMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  CutlassMLABackend.init_forward_metadata(self,forward_batch)
  CutlassMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,block_kv_indices)
  CutlassMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  CutlassMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  CutlassMLABackend.get_cuda_graph_seq_len_fill_value(self)
  CutlassMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
layers/attention/double_sparsity_backend.py:
  DoubleSparseAttnBackend.__init__(self,model_runner)
  DoubleSparseAttnBackend.init_forward_metadata(self,forward_batch)
  DoubleSparseAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  DoubleSparseAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
layers/attention/dual_chunk_flashattention_backend.py:
  DualChunkFlashAttentionBackend.__init__(self,model_runner)
  DualChunkFlashAttentionBackend.get_sparse_attention_config(self,layer_idx)
  DualChunkFlashAttentionBackend.init_forward_metadata(self,forward_batch)
  DualChunkFlashAttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  DualChunkFlashAttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  DualChunkFlashAttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  DualChunkFlashAttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  DualChunkFlashAttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu,out_cache_loc)
  DualChunkFlashAttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_prefill(self,q,q_succ,q_inter,q_succ_critical,q_inter_critical,k,v,cu_seqlens_q,cu_seqlens_k,orig_seq_lens,scaling_factor,softmax_scale,causal,window_size,block_table,chunk_size,local_size)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_prefill_func(self,q,q_succ,q_inter,q_succ_critical,q_inter_critical,k,v,block_table,softmax_scale,chunk_size,local_size,scaling_factor,k_length,sparse_attn_enabled,heads_vertical_size,heads_slash_size,group_size)
  DualChunkFlashAttentionBackend._do_flash_attn(self,query_states,key_states,value_states,softmax_scale,causal,max_seqlen_k,stage,vertical_indices,slash_indices,vertical_indices_count,slash_indices_count,mergehead_softmax_scale,sparse_attn_enabled)
  DualChunkFlashAttentionBackend._merge_attn_outputs(self,flash_results,return_lse)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_decoding(self,query,query_succ,query_inter,key_cache,value_cache,block_table,cache_seqlens,softmax_scale,causal,chunk_size,local_size,original_max_position_embeddings,decode_meta)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_decoding_with_exp_sums(self,query,key_cache,value_cache,block_table,cache_seqlens,softmax_scale,causal)
  _vertical_slash_sparse_attention(query,key,value,v_idx,s_idx,softmax_scale,causal,stage,block_size_M,block_size_N,vertical_indices_count,slash_indices_count)
  _sum_all_diagonal_matrix(mat)
  _get_block(block_table,block_size,begin,end)
layers/attention/flashattention_backend.py:
  make_local_attention_virtual_batches(attn_chunk_size,query_start_loc_np,seq_lens_np,block_table,page_size)
  cdiv(a,b)
  merge_state_v2_wrapper(o,s_a,o_exp,s_b)
  FlashAttentionBackend.__init__(self,model_runner,skip_prefill,speculative_step_id,topk,speculative_num_steps)
  FlashAttentionBackend.init_forward_metadata(self,forward_batch)
  FlashAttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,sinks)
  FlashAttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,sinks)
  FlashAttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashAttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashAttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu,out_cache_loc)
  FlashAttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashAttentionBackend._init_local_attn_metadata(self,forwardbatch,metadata,device)
  FlashAttentionBackend._update_local_attn_metadata_for_capture(self,metadata,bs)
  FlashAttentionBackend._update_local_attn_metadata_for_replay(self,metadata,bs)
  FlashAttentionBackend._init_sliding_window_attn_spec_metadata(self,metadata,metadata_expand,metadata_swa)
  _prepare_swa_spec_page_table_kernel(dst_ptr,src_a_ptr,src_b_ptr,seq_len_a_ptr,seq_len_b_ptr,dst_stride_m,dst_stride_n,a_stride_m,a_stride_n,b_stride_m,b_stride_n,LEN_A,LEN_B,REPEAT_STEP,BLOCK_N)
  prepare_swa_spec_page_table_triton(page_table_dst,page_table_a,page_table_b,seq_len_a,seq_len_b,speculative_num_draft_tokens)
  FlashAttentionMultiStepBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashAttentionMultiStepBackend.init_forward_metadata(self,forward_batch)
  FlashAttentionMultiStepBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  normal_decode_set_metadata(cache_seqlens_int32,cu_seqlens_k,page_table,req_to_token,req_pool_indices,strided_indices,max_seq_pages,seq_lens,seq_len_delta,page_size)
layers/attention/flashinfer_backend.py:
  FlashInferAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  FlashInferAttnBackend.init_forward_metadata(self,forward_batch)
  FlashInferAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  FlashInferAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashInferAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashInferAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashInferAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashInferAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashInferAttnBackend._get_wrapper_idx(self,layer)
  FlashInferIndicesUpdaterDecode.__init__(self,model_runner,attn_backend)
  FlashInferIndicesUpdaterDecode.update(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_sliding_window(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_cross_attention(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.call_begin_forward(self,wrapper,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,kv_indptr,kv_start_idx,spec_info,seq_lens_cpu,use_sliding_window_kv_pool)
  FlashInferIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  FlashInferIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_sliding_window(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_cross_attention(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.call_begin_forward(self,wrapper_ragged,wrapper_paged,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,seq_lens,prefix_lens,kv_start_idx,kv_indptr,qo_indptr,use_ragged,spec_info,use_sliding_window_kv_pool)
  FlashInferMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashInferMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  FlashInferMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashInferMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  should_use_tensor_core(kv_cache_dtype,num_attention_heads,num_kv_heads)
  fast_decode_plan(self,indptr,indices,last_page_len,num_qo_heads,num_kv_heads,head_dim,page_size,pos_encoding_mode,window_left,logits_soft_cap,q_data_type,kv_data_type,data_type,sm_scale,rope_scale,rope_theta,non_blocking)
layers/attention/flashinfer_mla_backend.py:
  FlashInferMhaChunkKVRunner.__init__(self,model_runner,attn_backend)
  FlashInferMhaChunkKVRunner.update_prefix_chunks(self,num_prefix_chunks)
  FlashInferMhaChunkKVRunner.update_wrapper(self,forward_batch)
  FlashInferMhaChunkKVRunner.forward(self,q,k,v,layer,forward_batch)
  FlashInferMLAAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,q_indptr_decode_buf)
  FlashInferMLAAttnBackend.init_forward_metadata(self,forward_batch)
  FlashInferMLAAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  FlashInferMLAAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashInferMLAAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashInferMLAAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashInferMLAAttnBackend.init_mha_chunk_metadata(self,forward_batch)
  FlashInferMLAAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  FlashInferMLAAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  FlashInferMLAIndicesUpdaterDecode.__init__(self,model_runner,attn_backend)
  FlashInferMLAIndicesUpdaterDecode.update(self,req_pool_indices,seq_lens,seq_lens_sum,decode_wrapper,init_metadata_replay,spec_info,**fast_decode_kwargs)
  FlashInferMLAIndicesUpdaterDecode.call_begin_forward(self,wrapper,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,q_indptr,kv_indptr,init_metadata_replay,spec_info,**fast_decode_kwargs)
  FlashInferMLAIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  FlashInferMLAIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,prefill_wrapper_paged,use_ragged,spec_info)
  FlashInferMLAIndicesUpdaterPrefill.call_begin_forward(self,wrapper_ragged,wrapper_paged,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,seq_lens,prefix_lens,kv_indptr,qo_indptr,use_ragged,spec_info)
  FlashInferMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashInferMLAMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  fast_mla_decode_plan(self,qo_indptr_cpu,kv_indptr_cpu,kv_indices,kv_len_arr_cpu,num_heads,head_dim_ckv,head_dim_kpe,page_size,causal,sm_scale,q_data_type,kv_data_type)
layers/attention/flashmla_backend.py:
  FlashMLADecodeMetadata.__init__(self,flashmla_metadata,num_splits,block_kv_indices)
  FlashMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  FlashMLABackend.init_forward_metadata(self,forward_batch)
  FlashMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,block_kv_indices)
  FlashMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashMLABackend.get_cuda_graph_seq_len_fill_value(self)
  FlashMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashMLABackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashMLAMultiStepDraftBackend.common_template(self,forward_batch,call_fn)
  FlashMLAMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashMLAMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
layers/attention/hybrid_attn_backend.py:
  HybridAttnBackend.__init__(self,model_runner,prefill_backend,decode_backend)
  HybridAttnBackend.init_forward_metadata(self,forward_batch)
  HybridAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  HybridAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  HybridAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  HybridAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  HybridAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  HybridAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
layers/attention/intel_amx_backend.py:
  IntelAMXAttnBackend.__init__(self,model_runner)
  IntelAMXAttnBackend.init_forward_metadata(self,forward_batch)
  IntelAMXAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  IntelAMXAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  IntelAMXAttnBackend.support_triton(self)
layers/attention/merge_state.py:
  _supported_dtypes(o)
  _supported_headdim(o)
  merge_state(prefix_output,prefix_lse,suffix_output,suffix_lse,output,output_lse)
layers/attention/tbo_backend.py:
  TboAttnBackend.__init__(self,primary,children)
  TboAttnBackend.init_new(cls,creator)
  TboAttnBackend.init_forward_metadata(self,forward_batch)
  TboAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TboAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TboAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TboAttnBackend._init_forward_metadata_cuda_graph_children(self,fn_name,bs,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info,capture_num_tokens,replay_seq_lens_sum,replay_seq_lens_cpu)
  TboAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TboAttnBackend.forward_extend(self,*args,**kwargs)
  TboAttnBackend.forward_decode(self,*args,**kwargs)
  _init_forward_metadata_cuda_graph_split(fn_name,seq_slice,output_bs,bs,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info,capture_num_tokens,replay_seq_lens_sum,replay_seq_lens_cpu)
layers/attention/torch_native_backend.py:
  TorchNativeAttnBackend.__init__(self,model_runner)
  TorchNativeAttnBackend.init_forward_metadata(self,forward_batch)
  TorchNativeAttnBackend._run_sdpa_forward_extend(self,query,output,k_cache,v_cache,req_to_token,req_pool_indices,seq_lens,extend_prefix_lens,extend_seq_lens,scaling,enable_gqa,causal)
  TorchNativeAttnBackend._run_sdpa_forward_decode(self,query,output,k_cache,v_cache,req_to_token,req_pool_indices,seq_lens,scaling,enable_gqa,causal)
  TorchNativeAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  TorchNativeAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  TorchNativeAttnBackend.support_triton(self)
layers/attention/triton_backend.py:
  logit_capping_mod(logit_capping_method,logit_cap)
  TritonAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  TritonAttnBackend.get_num_kv_splits(self,num_kv_splits,seq_lens)
  TritonAttnBackend.init_forward_metadata(self,forward_batch)
  TritonAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TritonAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TritonAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TritonAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TritonAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,sinks)
  TritonAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,sinks)
  TritonMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  TritonMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  TritonMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  TritonMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TritonMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  TritonMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  get_num_kv_splits_triton(num_kv_splits_ptr,seq_lens_ptr,num_seq,num_group,num_head,num_kv_head,max_kv_splits,device_core_count,MAX_NUM_SEQ)
  update_sliding_window_buffer(window_kv_indptr,req_to_token,sliding_window_size,seq_lens,req_pool_indices,bs,device,token_to_kv_pool_allocator)
  update_sliding_window_buffer_cuda_graph(window_kv_indptr,window_kv_indices,req_to_token,sliding_window_size,seq_lens,req_pool_indices,bs,token_to_kv_pool_allocator)
layers/attention/triton_ops/decode_attention.py:
  tanh(x)
  _fwd_kernel_stage1(Q,K_Buffer,V_Buffer,sm_scale,kv_indptr,kv_indices,Att_Out,Att_Lse,num_kv_splits,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,kv_group_num,BLOCK_DMODEL,BLOCK_DV,BLOCK_N,MIN_BLOCK_KV,logit_cap,Lk,Lv,xai_temperature_len)
  _decode_att_m_fwd(q,k_buffer,v_buffer,att_out,att_lse,kv_indptr,kv_indices,num_kv_splits,max_kv_splits,sm_scale,logit_cap,xai_temperature_len)
  _fwd_grouped_kernel_stage1(Q,K_Buffer,V_Buffer,sm_scale,kv_indptr,kv_indices,Att_Out,Att_Lse,num_kv_splits,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,kv_group_num,q_head_num,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_N,BLOCK_H,MIN_BLOCK_KV,logit_cap,xai_temperature_len,Lk,Lv)
  _decode_grouped_att_m_fwd(q,k_buffer,v_buffer,att_out,att_lse,kv_indptr,kv_indices,num_kv_splits,max_kv_splits,sm_scale,logit_cap,xai_temperature_len)
  _fwd_kernel_stage2(Mid_O,Mid_O_1,O,kv_indptr,num_kv_splits,sink_ptr,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_obs,stride_oh,MAX_KV_SPLITS,MIN_BLOCK_KV,BLOCK_DV,Lv,HAS_SINK)
  _decode_softmax_reducev_fwd(logits,lse,q,o,v_buffer,kv_indptr,num_kv_splits,max_kv_splits,sinks)
  decode_attention_fwd_normal(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
  decode_attention_fwd_grouped(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
  decode_attention_fwd(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
layers/attention/triton_ops/double_sparsity_attention.py:
  tanh(x)
  _fwd_kernel_flash_decode_stage1(Q,K,V,sm_scale,Req_to_tokens,B_req_idx,B_Seqlen,Mid_O,Mid_O_LogExpSum,stride_req_to_tokens_b,stride_req_to_tokens_s,stride_qbs,stride_qh,stride_qd,stride_kbs,stride_kh,stride_kd,stride_vbs,stride_vh,stride_vd,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_od,stride_mid_o_eb,stride_mid_o_eh,stride_mid_o_es,gqa_group_size,BLOCK_SEQ,BLOCK_DMODEL,BLOCK_N)
  _fwd_kernel_flash_decode_stage2(B_Seqlen,Mid_O,Mid_O_LogExpSum,O,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_od,stride_mid_o_eb,stride_mid_o_eh,stride_mid_o_es,stride_obs,stride_oh,stride_od,BLOCK_SEQ,BLOCK_DMODEL)
  flash_decode_stage1(q,k,v,Req_to_tokens,B_req_idx,B_Seqlen,max_len_in_batch,mid_out,mid_out_logsumexp,block_seq)
  flash_decode_stage2(mid_out,mid_out_logexpsum,B_Seqlen,O,block_seq)
  flash_decode_attention_fwd(q,k_buffer,v_buffer,o,req_to_token,b_req_idx,b_start_loc,b_seq_len,attn_logits,max_len_in_batch,sm_scale,logit_cap)
  _sparse_fwd_kernel_flash_decode_stage1(Q_Label,K_Label_Buffer,sm_scale,Req_to_tokens,B_Seqlen,Att_Out,stride_req_to_tokens_b,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,att_stride_h,att_stride_b,kv_group_num,BLOCK_DMODEL,BLOCK_N,logit_cap)
  _sparse_fwd_kernel_flash_decode_stage2(Q,K,V,sm_scale,Req_to_tokens,Topk_token_indices,Mid_O,Mid_O_LogExpSum,Heavy_token_num,stride_req_to_tokens_b,stride_topk_token_indices_h,stride_topk_token_indices_b,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_o_eb,stride_mid_o_eh,gqa_group_size,BLOCK_SEQ,BLOCK_DMODEL,BLOCK_N)
  _sparse_fwd_kernel_flash_decode_stage3(Mid_O,Mid_O_LogExpSum,O,seq_len,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_o_eb,stride_mid_o_eh,stride_obs,stride_oh,BLOCK_SEQ,BLOCK_DMODEL)
  sparse_flash_decode_stage1(q_label,k_label_buffer,att_out,Req_to_tokens,B_Seqlen,max_len_in_batch,sm_scale,logit_cap)
  sparse_flash_decode_stage2(q,k,v,Req_to_tokens,Topk_token_indices,heavy_token_num,mid_out,mid_out_logsumexp,block_seq,sm_scale)
  sparse_flash_decode_stage3(Seqlen,mid_out,mid_out_logexpsum,O,block_seq)
  flash_decode_sparse_attention_fwd(q,k_buffer,v_buffer,o,q_label,k_label_buffer,req_to_token,b_seq_len,max_len_in_batch,sm_scale,logit_cap,heavy_token_num,att_out_approx,mid_out,mid_o_logexpsum,BLOCK_SEQ)
  _fwd_kernel(Q_Extend,K_Extend,V_Extend,O_Extend,K_Buffer,V_Buffer,Req_to_tokens,B_req_idx,B_Seq_Len,B_Start_Loc_Extend,B_Seq_Len_Extend,sm_scale,kv_group_num,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_req_to_tokens_b,logit_cap,Lq,Lv,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_M,BLOCK_N)
  extend_attention_fwd(q_extend,k_extend,v_extend,o_extend,k_buffer,v_buffer,req_to_tokens,b_req_idx,b_seq_len,b_seq_len_extend,b_start_loc_extend,max_len_extend,sm_scale,logit_cap)
layers/attention/triton_ops/extend_attention.py:
  tanh(x)
  _fwd_kernel(Q_Extend,K_Extend,V_Extend,O_Extend,K_Buffer,V_Buffer,qo_indptr,kv_indptr,kv_indices,mask_ptr,mask_indptr,sink_ptr,window_kv_offset_ptr,sm_scale,kv_group_num,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,SLIDING_WINDOW_SIZE,logit_cap,xai_temperature_len,Lq,Lv,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_M,BLOCK_N,USE_CUSTOM_MASK,IS_CAUSAL,SKIP_PREFIX_CUSTOM_MASK,STORE_TRANSPOSE,HAS_SINK)
  extend_attention_fwd(q_extend,k_extend,v_extend,o_extend,k_buffer,v_buffer,qo_indptr,kv_indptr,kv_indices,custom_mask,is_causal,mask_indptr,max_len_extend,sm_scale,logit_cap,skip_prefix_custom_mask,sliding_window_size,sinks,window_kv_offsets,xai_temperature_len)
  redundant_attention(q_extend,o_extend,k_buffer,v_buffer,b_req_idx,b_start_loc,b_seq_len,b_seq_len_prefix,max_len_in_batch)
layers/attention/triton_ops/merge_state.py:
  merge_state_kernel(output,output_lse,prefix_output,prefix_lse,suffix_output,suffix_lse,HEAD_SIZE,PADDED_HEAD_SIZE,OUTPUT_LSE)
  merge_state_triton(prefix_output,prefix_lse,suffix_output,suffix_lse,output,output_lse)
layers/attention/triton_ops/prefill_attention.py:
  _fwd_kernel(Q,K,V,sm_scale,B_Start_Loc,B_Seqlen,Out,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,kv_group_num,BLOCK_M,BLOCK_DMODEL,BLOCK_N,IS_CAUSAL,Lk)
  context_attention_fwd(q,k,v,o,b_start_loc,b_seq_len,max_input_len,is_causal)
layers/attention/triton_ops/rocm_mla_decode_rope.py:
  is_hip()
  tanh(x)
  _fwd_grouped_kernel_stage1_rope(Q,K_Buffer,V_buffer,cos_sin_cache,positions,sm_scale,kv_indptr,kv_indices,Att_Out,k_pe_t_out,stride_qb,stride_qh,stride_buf_kbs,stride_buf_vbs,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_kpe_tokens_out_b,stride_cos_sin_cache_s,stride_positions_b,rotary_dim,kv_lora_rank,qk_rope_head_dim,kv_group_num,q_head_num,BLOCK_C,BLOCK_R,BLOCK_N,BLOCK_H,NUM_KV_SPLITS,logit_cap,USE_ROPE,IS_NEOX_STYLE)
  _decode_grouped_att_m_fwd_rope(q,k_buffer,v_buffer,att_out,k_pe_tokens_out,kv_lora_rank,cos_sin_cache,positions,rotary_dim,kv_indptr,kv_indices,num_kv_splits,sm_scale,logit_cap,use_rope,is_neox_style)
  decode_attention_fwd_grouped_rope(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,k_pe_tokens,kv_lora_rank,rotary_dim,cos_sin_cache,positions,attn_logits,num_kv_splits,sm_scale,logit_cap,use_rope,is_neox_style)
layers/attention/trtllm_mha_backend.py:
  TRTLLMHAAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf,speculative_step_id)
  TRTLLMHAAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TRTLLMHAAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TRTLLMHAAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TRTLLMHAAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TRTLLMHAAttnBackend.init_forward_metadata(self,forward_batch)
  TRTLLMHAAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  TRTLLMHAAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  TRTLLMHAAttnMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
layers/attention/trtllm_mla_backend.py:
  TRTLLMMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,q_indptr_decode_buf)
  TRTLLMMLABackend._calc_padded_blocks(self,max_seq_len)
  TRTLLMMLABackend._create_block_kv_indices(self,batch_size,max_blocks,req_pool_indices,seq_lens,device)
  TRTLLMMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value(self)
  TRTLLMMLABackend.init_forward_metadata(self,forward_batch)
  TRTLLMMLABackend.quantize_and_rope_for_fp8(self,q_nope,q_rope,k_nope,k_rope,forward_batch,cos_sin_cache,is_neox)
  TRTLLMMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,cos_sin_cache,is_neox)
  TRTLLMMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
layers/attention/utils.py:
  create_flashinfer_kv_indices_triton(req_to_token_ptr,req_pool_indices_ptr,page_kernel_lens_ptr,kv_indptr,kv_start_idx,kv_indices_ptr,req_to_token_ptr_stride)
  create_flashmla_kv_indices_triton(req_to_token_ptr,req_pool_indices_ptr,page_kernel_lens_ptr,kv_start_idx,kv_indices_ptr,req_to_token_ptr_stride,kv_indices_ptr_stride,NUM_PAGE_PER_BLOCK,PAGED_SIZE)
layers/attention/vision.py:
  SingletonCache.set_data(self,value)
  SingletonCache.get_data(self)
  SingletonCache.empty(self)
  _get_cu_seqlens_for_shape(batch_size,seqlen,device)
  VisionSdpaAttention.__init__(self,head_dim,num_heads,num_kv_heads,dropout,flatten_batch,softmax_in_single_precision,**kwargs)
  VisionSdpaAttention._generate_mask_cache(s,flatten_batch,cu_seqlens)
  VisionSdpaAttention.generate_patch_attention_mask(self,s,cu_seqlens,flatten_batch)
  VisionSdpaAttention.forward(self,q,k,v,bsz,cu_seqlens,attention_mask,**kwargs)
  VisionTritonAttention.__init__(self,**kwargs)
  VisionTritonAttention.forward(self,q,k,v,cu_seqlens,bsz,seq_len,**kwargs)
  VisionFlash3Attention.__init__(self,**kwargs)
  VisionFlash3Attention.forward(self,q,k,v,cu_seqlens,bsz,seq_len,**kwargs)
  VisionAttention.__init__(self,embed_dim,num_heads,projection_size,use_qkv_parallel,qkv_backend,quant_config,dropout,softmax_in_single_precision,flatten_batch,prefix,proj_bias,num_dummy_heads,qkv_bias,qk_normalization,layer_norm_eps,customized_position_embedding_applier,**kwargs)
  VisionAttention._determine_attention_backend(self,passed_backend)
  VisionAttention._apply_qk_norm(self,q,k)
  VisionAttention.forward(self,x,cu_seqlens,position_embeddings,attention_mask,**kwargs)
layers/attention/vision_utils.py:
  update_vit_attn_dummy_heads_config(config)
  pad_vit_attn_dummy_heads(config,name,loaded_weight)
layers/attention/wave_backend.py:
  get_num_kv_splits_triton(num_kv_splits_ptr,seq_lens_ptr,num_seq,num_group,num_head,num_kv_head,max_kv_splits,device_core_count,MAX_NUM_SEQ)
  WaveAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  WaveAttnBackend.get_num_kv_splits(self,num_kv_splits,seq_lens)
  WaveAttnBackend.init_forward_metadata(self,forward_batch)
  WaveAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  WaveAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  WaveAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  WaveAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  WaveAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  WaveAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
layers/attention/wave_ops/decode_attention.py:
  get_wave_kernel(shape,max_kv_splits,input_dtype,output_dtype,logit_cap)
  decode_attention_intermediate_arrays_shapes(num_seqs,head_size_kv,num_query_heads,max_kv_splits)
  decode_attention_wave(q,k_buffer,v_buffer,o,b_req_idx,req_to_token,attn_logits,attn_logits_max,num_kv_splits,max_kv_splits,sm_scale,logit_cap)
  decode_attention_fwd(q,k_buffer,v_buffer,o,b_req_idx,req_to_token,attn_logits,attn_logits_max,num_kv_splits,max_kv_splits,sm_scale,logit_cap)
layers/attention/wave_ops/extend_attention.py:
  get_wave_kernel(shape,q_shape,k_shape,v_shape,k_cache_shape,v_cache_shape,o_shape,input_dtype,output_dtype,size_dtype,is_causal,logit_cap,layer_scaling)
  extend_attention_wave(q_extend,k_extend,v_extend,k_buffer,v_buffer,qo_indptr,kv_indptr,kv_indices,custom_mask,mask_indptr,max_seq_len,output,is_causal,layer_scaling,logit_cap)
layers/attention/wave_ops/prefill_attention.py:
  prefill_attention_wave(q,k,v,o,b_start_loc,b_seq_len,max_seq_len,is_causal)
layers/communicator.py:
  ScatterMode.model_input_output()
  _LayerModeComputationContext.previous_layer(self)
  LayerScatterModes.init_new(cls,**kwargs)
  LayerScatterModes._compute_layer_input_mode(cls,context)
  LayerScatterModes._compute_mlp_mode(cls,context)
  LayerScatterModes._compute_middle_residual_mode(cls,context)
  LayerScatterModes._compute_layer_output_mode(cls,context)
  enable_moe_dense_fully_dp()
  LayerCommunicator.__init__(self,layer_scatter_modes,input_layernorm,post_attention_layernorm,allow_reduce_scatter,is_last_layer)
  LayerCommunicator.prepare_attn(self,hidden_states,residual,forward_batch)
  LayerCommunicator.prepare_mlp(self,hidden_states,residual,forward_batch)
  LayerCommunicator.postprocess_layer(self,hidden_states,residual,forward_batch)
  LayerCommunicator.should_use_reduce_scatter(self,forward_batch)
  LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer(self,forward_batch)
  CommunicateContext.is_same_group_size(self,a,b)
  CommunicateContext.init_new(cls)
  CommunicateSimpleFn.get_fn(input_mode,output_mode,context)
  CommunicateSimpleFn._trivial(hidden_states,forward_batch,context)
  CommunicateSimpleFn._scattered_to_tp_attn_full(hidden_states,forward_batch,context)
  CommunicateWithAllReduceAndLayerNormFn.get_fn(hidden_states_input_mode,residual_input_mode,hidden_states_output_mode,residual_output_mode,context)
  CommunicateWithAllReduceAndLayerNormFn._simple(hidden_states,residual,forward_batch,layernorm,context)
  CommunicateWithAllReduceAndLayerNormFn._gather_hidden_states_and_residual(hidden_states,residual,forward_batch,layernorm,context,residual_input_mode)
  CommunicateWithAllReduceAndLayerNormFn._scatter_hidden_states_and_residual(hidden_states,residual,forward_batch,layernorm,context,residual_input_mode)
  CommunicateSummableTensorPairFn.execute(cls,hidden_states_input_mode,residual_input_mode,output_mode,context,**kwargs)
  CommunicateSummableTensorPairFn.get_fn(hidden_states_input_mode,residual_input_mode,output_mode,context)
  CommunicateSummableTensorPairFn._trivial(hidden_states,residual,forward_batch,context,**kwargs)
  CommunicateSummableTensorPairFn._scatter_hidden_states(hidden_states,residual,forward_batch,context,allow_reduce_scatter)
  CommunicateSummableTensorPairFn._gather(hidden_states,residual,forward_batch,context,**kwargs)
  CommunicateSummableTensorPairFn._scatter(hidden_states,residual,forward_batch,context)
layers/dp_attention.py:
  DpPaddingMode.is_max_len(self)
  DpPaddingMode.is_sum_len(self)
  DpPaddingMode.get_dp_padding_mode(cls,global_num_tokens)
  DpPaddingMode.get_default_mode_in_cuda_graph(cls)
  _DpGatheredBufferWrapper.set_metadata(cls,hidden_size,dtype,device)
  _DpGatheredBufferWrapper.set_dp_buffer_len(cls,global_dp_buffer_len,local_dp_buffer_len,global_num_tokens)
  _DpGatheredBufferWrapper.get_global_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_global_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.get_dp_global_num_tokens(cls)
  set_dp_buffer_len(global_dp_buffer_len,local_dp_buffer_len,global_num_tokens)
  get_global_dp_buffer()
  get_local_dp_buffer()
  get_global_dp_buffer_len()
  get_local_dp_buffer_len()
  get_dp_global_num_tokens()
  compute_dp_attention_world_info(enable_dp_attention,tp_rank,tp_size,dp_size)
  compute_dp_attention_local_info(enable_dp_attention,tp_rank,tp_size,dp_size,moe_dense_tp_size)
  initialize_dp_attention(server_args,model_config)
  is_dp_attention_enabled()
  get_attention_tp_group()
  get_attention_tp_rank()
  get_attention_tp_size()
  get_attention_dp_rank()
  get_attention_dp_size()
  get_local_attention_dp_rank()
  get_local_attention_dp_size()
  disable_dp_size()
  get_dp_local_info(forward_batch)
  memcpy_triton_kernel(dst_ptr,src_ptr,offset_ptr,sz_ptr,offset_src,chunk_size,BLOCK_SIZE)
  prod(x)
  memcpy_triton(dst,src,dim,offset,sz,offset_src)
  _dp_gather_via_all_reduce(global_tokens,local_tokens,forward_batch,is_partial)
  _dp_gather_via_all_gather(global_tokens,local_tokens,forward_batch,is_partial)
  _dp_gather(global_tokens,local_tokens,forward_batch,is_partial)
  dp_gather_partial(global_tokens,local_tokens,forward_batch)
  dp_gather_replicate(global_tokens,local_tokens,forward_batch)
  dp_scatter(local_tokens,global_tokens,forward_batch)
  dp_reduce_scatter_tensor(output,input)
  attn_tp_reduce_scatter_tensor(output,input)
  attn_tp_all_gather_into_tensor(output,input)
  attn_tp_all_gather(output_list,input)
layers/elementwise.py:
  fused_softcap_kernel(output_ptr,input_ptr,n_ele,softcap_const,BLOCK_SIZE)
  fused_softcap(x,softcap_const,autotune)
  Softcap.__init__(self,softcap_const)
  Softcap.__call__(self,*args,**kwargs)
  Softcap.forward(self,x)
  Softcap.forward_native(self,x)
  Softcap.forward_cuda(self,x,autotune)
  fused_dual_residual_rmsnorm_kernel(output_ptr,mid_ptr,activ_ptr,residual_ptr,weight1_ptr,weight2_ptr,eps,hidden_dim,BLOCK_SIZE)
  fused_dual_residual_rmsnorm(x,residual,weight1,weight2,eps,autotune)
  fused_rmsnorm_kernel(output_ptr,activ_ptr,weight_ptr,eps,hidden_dim,BLOCK_SIZE)
  fused_rmsnorm(x,weight,eps,autotune,inplace)
  FusedDualResidualRMSNorm.__init__(self,rmsnorm1,rmsnorm2)
  FusedDualResidualRMSNorm.__call__(self,*args,**kwargs)
  FusedDualResidualRMSNorm.forward(self,x,residual)
  FusedDualResidualRMSNorm.forward_cuda(self,x,residual,autotune)
  FusedDualResidualRMSNorm.forward_flashinfer(self,x,residual)
  FusedDualResidualRMSNorm.forward_native(self,x,residual)
  experts_combine_kernel(out_hidden_states,moe_hidden_states,mlp_hidden_states,combine_k,hidden_dim,BLOCK_SIZE)
  experts_combine_triton(moe_hidden_states,mlp_hidden_states,output_buffer)
  gelu_and_mul_kernel(out_hidden_states_ptr,out_scales_ptr,hidden_states_ptr,quant_max,static_scale,hidden_dim,BLOCK_SIZE)
  gelu_and_mul_triton(hidden_states,scales,quantize,out)
  silu_and_mul_kernel(out_hidden_states_ptr,out_scales_ptr,hidden_states_ptr,quant_max,static_scale,hidden_dim,BLOCK_SIZE)
  silu_and_mul_triton(hidden_states,scales,quantize,out)
layers/flashinfer_comm_fusion.py:
  FlashInferWorkspaceManager.__init__(self)
  FlashInferWorkspaceManager.initialize(self,world_size,rank,max_token_num,hidden_dim,group,use_fp32_lamport)
  FlashInferWorkspaceManager.cleanup(self)
  ensure_workspace_initialized(max_token_num,hidden_dim,use_fp32_lamport)
  flashinfer_allreduce_residual_rmsnorm(input_tensor,residual,weight,eps,max_token_num,use_oneshot,trigger_completion_at_end,fp32_acc)
  fake_flashinfer_allreduce_residual_rmsnorm(input_tensor,residual,weight,eps,max_token_num,use_oneshot,trigger_completion_at_end,fp32_acc)
  cleanup_flashinfer_workspace()
layers/layernorm.py:
  RMSNorm.__init__(self,hidden_size,eps,var_hidden_size)
  RMSNorm.forward_cuda(self,x,residual)
  RMSNorm.forward_npu(self,x,residual)
  RMSNorm.forward_aiter(self,x,residual)
  RMSNorm.forward_hip(self,x,residual)
  RMSNorm.forward_native(self,x,residual)
  RMSNorm.forward_cpu(self,x,residual)
  RMSNorm.forward_with_allreduce_fusion(self,x,residual)
  GemmaRMSNorm.__init__(self,hidden_size,eps)
  GemmaRMSNorm.forward_native(self,x,residual)
  GemmaRMSNorm.forward_cuda(self,x,residual)
  GemmaRMSNorm.forward_npu(self,x,residual)
  Gemma3RMSNorm.__init__(self,dim,eps)
  Gemma3RMSNorm._norm(self,x)
  Gemma3RMSNorm.forward_native(self,x)
  Gemma3RMSNorm.forward_cuda(self,x)
  Gemma3RMSNorm.forward_npu(self,x)
  Gemma3RMSNorm.extra_repr(self)
layers/linear.py:
  adjust_marlin_shard(param,shard_size,shard_offset)
  adjust_bitsandbytes_4bit_shard(param,shard_offsets,loaded_shard_id)
  adjust_scalar_to_fused_array(param,loaded_weight,shard_id)
  adjust_shard_offsets(shard_offsets,loaded_weight,dim)
  LinearBase.__init__(self,input_size,output_size,skip_bias_add,params_dtype,quant_config,prefix)
  LinearBase.forward(self,x)
  ReplicatedLinear.__init__(self,input_size,output_size,bias,skip_bias_add,params_dtype,quant_config,prefix)
  ReplicatedLinear.weight_loader(self,param,loaded_weight)
  ReplicatedLinear.forward(self,x)
  ReplicatedLinear.extra_repr(self)
  ColumnParallelLinear.__init__(self,input_size,output_size,bias,gather_output,skip_bias_add,params_dtype,quant_config,output_sizes,prefix,tp_rank,tp_size,use_presharded_weights)
  ColumnParallelLinear.weight_loader(self,param,loaded_weight)
  ColumnParallelLinear.weight_loader_v2(self,param,loaded_weight)
  ColumnParallelLinear.forward(self,input_)
  ColumnParallelLinear.extra_repr(self)
  MergedColumnParallelLinear.__init__(self,input_size,output_sizes,bias,gather_output,skip_bias_add,params_dtype,quant_config,prefix,tp_rank,tp_size,use_presharded_weights)
  MergedColumnParallelLinear.weight_loader(self,param,loaded_weight,loaded_shard_id)
  MergedColumnParallelLinear._load_fused_module_from_checkpoint(self,param,loaded_weight)
  MergedColumnParallelLinear.weight_loader_v2(self,param,loaded_weight,loaded_shard_id)
  QKVParallelLinear.__init__(self,hidden_size,head_size,total_num_heads,total_num_kv_heads,bias,skip_bias_add,params_dtype,quant_config,prefix,tp_rank,tp_size,load_presharded_attn)
  QKVParallelLinear._get_shard_offset_mapping(self,loaded_shard_id)
  QKVParallelLinear._get_shard_size_mapping(self,loaded_shard_id)
  QKVParallelLinear._load_fused_module_from_checkpoint(self,param,loaded_weight)
  QKVParallelLinear.weight_loader_v2(self,param,loaded_weight,loaded_shard_id)
  QKVParallelLinear.weight_loader(self,param,loaded_weight,loaded_shard_id)
  RowParallelLinear.__init__(self,input_size,output_size,bias,input_is_parallel,skip_bias_add,params_dtype,reduce_results,quant_config,prefix,tp_rank,tp_size,use_presharded_weights)
  RowParallelLinear.weight_loader(self,param,loaded_weight)
  RowParallelLinear.weight_loader_v2(self,param,loaded_weight)
  RowParallelLinear.forward(self,input_,skip_all_reduce)
  RowParallelLinear.extra_repr(self)
layers/logits_processor.py:
  LogitsMetadata.from_forward_batch(cls,forward_batch)
  LogitsMetadata.compute_dp_attention_metadata(self)
  LogitsProcessor.__init__(self,config,skip_all_gather,logit_scale)
  LogitsProcessor.forward(self,input_ids,hidden_states,lm_head,logits_metadata,aux_hidden_states)
  LogitsProcessor._get_logits(self,hidden_states,lm_head,logits_metadata,embedding_bias)
  LogitsProcessor.get_top_logprobs(all_logprobs,logits_metadata)
  LogitsProcessor.get_token_ids_logprobs(all_logprobs,logits_metadata)
  LogitsProcessor.compute_temp_top_p_normalized_logprobs(last_logits,logits_metadata)
  fused_softcap_kernel(full_logits_ptr,softcapping_value,n_elements,BLOCK_SIZE)
  fused_softcap(full_logits,final_logit_softcapping)
layers/moe/cutlass_moe.py:
  cutlass_fused_experts_fp8(a,w1_q,w2_q,w1_scale,w2_scale,topk_weights,topk_ids,a1_strides,c1_strides,a2_strides,c2_strides,workspace,a_ptrs,b_ptrs,out_ptrs,a_scales_ptrs,b_scales_ptrs,expert_offsets,problem_sizes1,problem_sizes2,use_fp8_blockscale)
  cutlass_moe_fp4(a,a1_gscale,w1_fp4,w1_blockscale,w1_alphas,a2_gscale,w2_fp4,w2_blockscale,w2_alphas,topk_weights,topk_ids,params,apply_router_weight_on_input)
layers/moe/cutlass_moe_params.py:
  CutlassMoEParams.__init__(self,cutlass_moe_type,device,num_experts,intermediate_size_per_partition,hidden_size)
  CutlassMoEParams.to_gemm1_args(self)
  CutlassMoEParams.to_gemm2_args(self)
layers/moe/cutlass_w4a8_moe.py:
  cutlass_w4a8_moe(start_expert_id,end_expert_id,total_num_experts,a,w1_q,w2_q,w1_scale,w2_scale,topk_weights,topk_ids_,local_topk_ids,a_strides1,b_strides1,c_strides1,a_strides2,b_strides2,c_strides2,s_strides13,s_strides2,expert_offsets,problem_sizes1,problem_sizes2,a1_scale,a2_scale,apply_router_weight_on_input)
layers/moe/ep_moe/kernels.py:
  deepep_permute_triton_kernel(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,topk,hidden_size,BLOCK_SIZE)
  deepep_post_reorder_triton_kernel(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,topk,hidden_size,BLOCK_SIZE)
  compute_src2dst_triton_kernel(reorder_ids,src2dst,num_toks,BLOCK_SIZE)
  deepep_compute_src2dst_triton_kernel(reorder_ids,src2dst,num_toks,num_minus_one,BLOCK_SIZE)
  deepep_run_moe_deep_preprocess(topk_ids,num_experts)
  compute_seg_indptr_triton_kernel(reorder_topk_ids,seg_indptr,num_toks)
  run_moe_ep_preproess(topk_ids,num_experts)
  run_cutlass_moe_ep_preproess(local_topk_ids,local_num_experts)
  pre_reorder_triton_kernel_for_cutlass_moe(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,num_experts,topk,hidden_size,BLOCK_SIZE)
  pre_reorder_triton_kernel(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,start_expert_id,end_expert_id,topk,hidden_size,BLOCK_SIZE,use_per_token_if_dynamic)
  silu_and_mul_triton_kernel(gateup_output,down_input,hidden_size,reorder_topk_ids,scales,start_expert_id,end_expert_id,BLOCK_SIZE)
  _silu_and_mul_post_quant_kernel(input_ptr,stride_input_0,stride_input_1,stride_input_2,output_ptr,stride_output_0,stride_output_1,stride_output_2,output_scale_ptr,stride_output_scale_0,stride_output_scale_1,stride_output_scale_2,masked_m_ptr,size_n,fp8_max,fp8_min,BLOCK_N,NUM_STAGE,SCALE_UE8M0)
  silu_and_mul_masked_post_quant_fwd(input,output,output_scale,quant_group_size,masked_m,scale_ue8m0)
  tanh(x)
  gelu_and_mul_triton_kernel(gateup_output,down_input,hidden_size,reorder_topk_ids,scales,start_expert_id,end_expert_id,BLOCK_SIZE)
  post_reorder_triton_kernel(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,start_expert_id,end_expert_id,topk,hidden_size,dst_start,BLOCK_SIZE)
  post_reorder_triton_kernel_for_cutlass_moe(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,num_experts,topk,hidden_size,dst_start,BLOCK_SIZE)
  compute_m_range(pid,batch_size,seg_indptr,weight_indices,m_num_tiles_indptr,BLOCK_SIZE_M)
  grouped_gemm_triton_kernel(a,b,c,batch_size,N,K,seg_indptr,weight_indices,m_num_tiles_indptr,scale_a,scale_b,use_fp8_w8a8,group_n,group_k,a_stride_0,b_stride_0,b_stride_1,as_stride_0,as_stride_1,bs_stride_0,bs_stride_2,bs_stride_1,use_per_token_if_dynamic,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K)
  compute_m_num_tiles_indptr(m_num_tiles_indptr,seg_indptr,batch_size,BLOCK_SIZE_M)
  grouped_gemm_triton(a,b,c,batch_size,weight_column_major,seg_indptr,weight_indices,use_fp8_w8a8,scale_a,scale_b,block_shape,c_dtype,use_per_token_if_dynamic)
  _fwd_kernel_ep_scatter_1(num_recv_tokens_per_expert,expert_start_loc,m_indices,num_experts,BLOCK_E,BLOCK_EXPERT_NUM)
  _fwd_kernel_ep_scatter_2(total_token_num,expert_start_loc,recv_x,recv_x_stride0,recv_x_stride1,recv_x_scale,recv_x_scale_stride0,recv_x_scale_stride1,recv_topk,recv_topk_stride0,recv_topk_stride1,output_tensor,output_tensor_stride0,output_tensor_stride1,output_tensor_scale,output_tensor_scale_stride0,output_tensor_scale_stride1,output_index,output_index_stride0,output_index_stride1,topk_num,HIDDEN_SIZE,HIDDEN_SIZE_PAD,SCALE_HIDDEN_SIZE,SCALE_HIDDEN_SIZE_PAD)
  ep_scatter(recv_x,recv_x_scale,recv_topk,num_recv_tokens_per_expert,expert_start_loc,output_tensor,output_tensor_scale,m_indices,output_index,scale_ue8m0)
  _fwd_kernel_ep_gather(total_token_num,input_tensor,input_tensor_stride0,input_tensor_stride1,recv_topk_ids,recv_topk_ids_stride0,recv_topk_ids_stride1,recv_topk_weight,recv_topk_weight_stride0,recv_topk_weight_stride1,input_index,input_index_stride0,input_index_stride1,output_tensor,output_tensor_stride0,output_tensor_stride1,topk_num,BLOCK_D)
  ep_gather(input_tensor,recv_topk_ids,recv_topk_weight,input_index,output_tensor)
  get_tma_aligned_size(x,element_size)
  _tma_align_input_scale_kernel(input_scale_ptr,output_ptr,m,k_div_block_size,input_scale_stride_m,input_scale_stride_k,output_stride_m,output_stride_k,BLOCK_SIZE_K)
  tma_align_input_scale(input_scale)
  compute_masked_m_triton_kernel(seg_indptr,masked_m)
  deepgemm_compute_src2dst_triton_kernel(topk_ids,reorder_ids,seg_indptr,src2dst,m_max,num_toks,BLOCK_SIZE)
  fill_gateup_input_triton_kernel(input_ptr,scale_ptr,gateup_input_ptr,gateup_input_scale_ptr,src2dst_ptr,topk_ids_ptr,start_expert_id,end_expert_id,topk,m_max,hidden_size,scale_size,BLOCK_SIZE)
  moe_ep_deepgemm_preprocess(topk_ids,num_experts,hidden_states,top_k,start_expert_id,end_expert_id,block_shape,output_dtype)
  compute_identity_kernel(top_k,hidden_states_ptr,expert_scales_ptr,num_tokens,output_ptr,hidden_dim,scales_stride,BLOCK_SIZE)
  zero_experts_compute_triton(expert_indices,expert_scales,num_experts,zero_expert_type,hidden_states)
layers/moe/ep_moe/layer.py:
  _cast_to_e8m0_with_rounding_up(x)
  EPMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,num_fused_shared_experts,params_dtype,quant_config,prefix,activation,routed_scaling_factor,gemm1_alpha,gemm1_clamp_limit,with_bias)
  EPMoE.forward(self,hidden_states,topk_output)
  EPMoE.forward_deepgemm(self,hidden_states,topk_output)
  DeepEPMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,num_fused_shared_experts,params_dtype,quant_config,prefix,activation,routed_scaling_factor)
  DeepEPMoE.forward(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.dispatch(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.moe_impl(self,dispatch_output)
  DeepEPMoE.combine(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.forward_aiter(self,dispatch_output)
  DeepEPMoE.forward_deepgemm_contiguous(self,dispatch_output)
  DeepEPMoE.forward_deepgemm_masked(self,dispatch_output)
  DeepEPMoE.forward_npu(self,dispatch_output)
  get_moe_impl_class(quant_config)
layers/moe/fused_moe_native.py:
  fused_moe_forward_native(layer,x,topk_output,moe_runner_config)
  moe_forward_native(layer,x,topk_output,moe_runner_config)
layers/moe/fused_moe_triton/__init__.py:
  override_config(config)
  get_config()
layers/moe/fused_moe_triton/fused_moe.py:
  write_zeros_to_output(c_ptr,stride_cm,stride_cn,pid_n,N,offs_token,token_mask,BLOCK_SIZE_M,BLOCK_SIZE_N,compute_type)
  fused_moe_kernel_gptq_awq(a_ptr,b_ptr,c_ptr,b_scale_ptr,b_zp_ptr,topk_weights_ptr,sorted_token_ids_ptr,expert_ids_ptr,num_tokens_post_padded_ptr,N,K,EM,num_valid_tokens,stride_am,stride_ak,stride_be,stride_bk,stride_bn,stride_cm,stride_cn,stride_bse,stride_bsk,stride_bsn,stride_bze,stride_bzk,stride_bzn,group_size,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M,MUL_ROUTED_WEIGHT,top_k,compute_type,has_zp,use_int4_w4a16,use_int8_w8a16,even_Ks)
  fused_moe_kernel(a_ptr,b_ptr,bias_ptr,c_ptr,a_scale_ptr,b_scale_ptr,topk_weights_ptr,sorted_token_ids_ptr,expert_ids_ptr,num_tokens_post_padded_ptr,N,K,EM,num_valid_tokens,stride_am,stride_ak,stride_be,stride_bk,stride_bn,stride_bias_e,stride_bias_n,stride_cm,stride_cn,stride_asm,stride_ask,stride_bse,stride_bsk,stride_bsn,group_n,group_k,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M,MUL_ROUTED_WEIGHT,top_k,compute_type,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,per_channel_quant,even_Ks)
  moe_align_block_size(topk_ids,block_size,num_experts)
  invoke_fused_moe_kernel(A,B,bias,C,A_scale,B_scale,B_zp,topk_weights,topk_ids,sorted_token_ids,expert_ids,num_tokens_post_padded,mul_routed_weight,top_k,config,compute_type,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,block_shape,no_combine)
  get_config_file_name(E,N,dtype,block_shape)
  get_moe_configs(E,N,dtype,block_n,block_k)
  get_default_config(M,E,N,K,topk,dtype,is_marlin,block_shape)
  try_get_optimal_moe_config(w1_shape,w2_shape,top_k,dtype,M,is_marlin,block_shape)
  get_config_dtype_str(dtype,use_int8_w8a16,use_int4_w4a16,use_fp8_w8a8,use_int8_w8a8)
  inplace_fused_experts(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  inplace_fused_experts_fake(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  outplace_fused_experts(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  outplace_fused_experts_fake(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  fused_experts(hidden_states,w1,w2,topk_output,moe_runner_config,b1,b2,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape)
  _moe_sum_reduce_kernel(input_ptr,input_stride_0,input_stride_1,input_stride_2,output_ptr,output_stride_0,output_stride_1,token_num,topk_num,hidden_dim,routed_scaling_factor,BLOCK_M,BLOCK_DIM,NUM_STAGE)
  moe_sum_reduce_triton(input,output,routed_scaling_factor)
  moe_sum_reduce_torch_compile(x,out,routed_scaling_factor)
  swiglu_with_alpha_and_limit(x,gemm1_alpha,gemm1_limit)
  fused_experts_impl(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,inplace,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  fused_moe(hidden_states,w1,w2,topk_output,moe_runner_config,b1,b2,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape)
layers/moe/fused_moe_triton/layer.py:
  _is_fp4_quantization_enabled()
  _get_tile_tokens_dim(num_tokens,top_k,num_experts)
  FusedMoE.__init__(self,num_experts,hidden_size,intermediate_size,layer_id,top_k,num_fused_shared_experts,params_dtype,reduce_results,quant_config,prefix,activation,apply_router_weight_on_input,use_presharded_weights,inplace,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_clamp_limit,use_weight_loader_fused,with_bias)
  FusedMoE._load_per_tensor_weight_scale(self,shard_id,param,loaded_weight,expert_id)
  FusedMoE._load_model_weight_or_group_weight_scale(self,shard_dim,expert_data,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._load_per_channel_weight_scale(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank)
  FusedMoE._load_w13(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._load_w2(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._load_single_value(self,param,loaded_weight,expert_id)
  FusedMoE._load_g_idx(self,shard_id,expert_data,shard_dim,loaded_weight,tp_rank)
  FusedMoE._map_global_expert_id_to_local_expert_id(self,expert_id)
  FusedMoE.weight_loader(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE._weight_loader_physical(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE._weight_loader_impl(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE.weight_loader_fused(self,param,loaded_weight,weight_name,shard_id)
  FusedMoE.forward(self,hidden_states,topk_output)
  FusedMoE.make_expert_params_mapping(cls,ckpt_gate_proj_name,ckpt_down_proj_name,ckpt_up_proj_name,num_experts)
  FusedMoE.make_expert_params_mapping_fused(cls,ckpt_gate_up_proj_name,ckpt_down_proj_name,ckpt_gate_up_proj_bias_name,ckpt_down_proj_bias_name)
  FusedMoE.make_expert_params_mapping_fused_mxfp4(cls,ckpt_gate_up_proj_name,ckpt_down_proj_name,ckpt_gate_up_proj_bias_name,ckpt_down_proj_bias_name,ckpt_gate_up_proj_scale_name,ckpt_down_proj_scale_name)
  FusedMoE.make_expert_input_scale_params_mapping(cls,num_experts)
  FusedMoE.should_fuse_routed_scaling_factor_in_topk(self)
  FlashInferFusedMoE.__init__(self,*args,**kwargs)
  FlashInferFusedMoE.forward(self,hidden_states,topk_output)
  FlashInferFP4MoE.__init__(self,*args,**kwargs)
  FlashInferFP4MoE._quantize_hidden_states_fp4(self,hidden_states)
  FlashInferFP4MoE.forward(self,hidden_states,topk_output)
  get_fused_moe_impl_class()
layers/moe/fused_moe_triton/triton_kernels_moe.py:
  quantize(w,dtype,dev,**opt)
  triton_kernel_moe_forward(hidden_states,w1,w2,topk_output,moe_runner_config,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
  triton_kernel_fused_experts(hidden_states,w1,w2,routing_data,gather_indx,scatter_indx,inplace,activation,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
  triton_kernel_moe_with_bias_forward(hidden_states,w1,w1_pcg,b1,w2,w2_pcg,b2,topk_output,moe_runner_config,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
  triton_kernel_fused_experts_with_bias(hidden_states,w1,w1_pcg,b1,w2,w2_pcg,b2,routing_data,gather_indx,scatter_indx,inplace,activation,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape,gemm1_alpha,gemm1_clamp_limit)
layers/moe/rocm_moe_utils.py:
  rocm_aiter_asm_moe_tkw1_impl(hidden_states,w1,w2,topk_weights,topk_ids,fc1_scale,fc2_scale,fc1_smooth_scale,fc2_smooth_scale,a16,per_tensor_quant_scale,expert_mask,activation_method)
  rocm_aiter_asm_moe_tkw1_fake(hidden_states,w1,w2,topk_weights,topk_ids,fc1_scale,fc2_scale,fc1_smooth_scale,fc2_smooth_scale,a16,per_tensor_quant_scale,expert_mask,activation_method)
  rocm_fused_experts_tkw1(hidden_states,w1,w2,topk_weights,topk_ids,activation,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
layers/moe/router.py:
  fused_moe_router_kernel(input_ptr,moe_router_weight_ptr,topk_weights_ptr,topk_ids_ptr,correction_bias_ptr,is_correction_bias,num_experts,topk,moe_softcapping,moe_renormalize,hidden_dim,BLOCK_SIZE)
  fused_moe_router_impl(x,router_weight,topk,moe_softcapping,correction_bias)
  fused_moe_router_large_bs_kernel(a_ptr,b_ptr,topk_weights_ptr,topk_ids_ptr,bs,num_experts,topk,moe_softcapping,moe_renormalize,K,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,stride_am,stride_bn)
  fused_moe_router_large_bs_impl(x,router_weight,topk,moe_softcapping,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K)
  fused_moe_router_shim(moe_softcapping,hidden_states,gating_output,topk,renormalize,correction_bias)
  FusedMoeRouter.__init__(self,router_linear,topk,moe_softcapping)
  FusedMoeRouter.__call__(self,*args,**kwargs)
  FusedMoeRouter.forward(self,x,residual)
  FusedMoeRouter.forward_cuda(self,x,autotune)
  FusedMoeRouter.forward_vllm(self,x)
layers/moe/token_dispatcher/base_dispatcher.py:
  DispatchOutputChecker.format_is_standard(dispatch_output)
  DispatchOutputChecker.format_is_deepep_normal(dispatch_output)
  DispatchOutputChecker.format_is_deepep_ll(dispatch_output)
  DispatchOutputChecker.format_is_deepep(dispatch_output)
  DispatchOutputChecker.format_is_ascent_ll(dispatch_output)
  DispatchOutputFormat.is_standard(self)
  DispatchOutputFormat.is_deepep_normal(self)
  DispatchOutputFormat.is_deepep_ll(self)
  DispatchOutputFormat.is_deepep(self)
  DispatchOutputFormat.is_ascent_ll(self)
  DispatchOutput.format(self)
  BaseDispatcher.dispatch(self,*args,**kwargs)
  BaseDispatcher.combine(self,*args,**kwargs)
layers/moe/token_dispatcher/deepep.py:
  DeepEPNormalOutput.format(self)
  DeepEPLLOutput.format(self)
  AscendDeepEPLLOutput.format(self)
  DeepEPBuffer.get_deepep_buffer(cls,group,hidden_size,param_bytes,deepep_mode,num_max_dispatch_tokens_per_rank,num_experts)
  DeepEPBuffer.clean_buffer(cls)
  DeepEPBuffer.set_dispatch_mode_as_normal(cls)
  DeepEPBuffer.set_dispatch_mode_as_low_latency(cls)
  DeepEPConfig.__init__(self)
  DeepEPConfig.get_instance(cls)
  _DeepEPDispatcherImplBase.__init__(self,group,router_topk,permute_fusion,num_experts,num_local_experts,hidden_size,params_dtype,deepep_mode)
  _DeepEPDispatcherImplBase.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplBase.dispatch_b(self,*args,**kwargs)
  _DeepEPDispatcherImplBase.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplBase.combine_b(self,*args,**kwargs)
  _DeepEPDispatcherImplBase._get_buffer(self)
  _DeepEPDispatcherImplNormal.__init__(self,async_finish,**kwargs)
  _DeepEPDispatcherImplNormal.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplNormal.dispatch_b(self,hidden_states,topk_idx,topk_weights,previous_event)
  _DeepEPDispatcherImplNormal._dispatch_core(self,x,topk_idx,topk_weights,previous_event)
  _DeepEPDispatcherImplNormal.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplNormal.combine_b(self,output,previous_event)
  _DeepEPDispatcherImplNormal._combine_core(self,x,previous_event)
  _DeepEPDispatcherImplNormal._get_buffer(self)
  _DeepEPDispatcherImplLowLatency.__init__(self,return_recv_hook,**kwargs)
  _DeepEPDispatcherImplLowLatency.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency.dispatch_b(self,hidden_states,topk_idx,topk_weights,masked_m,expected_m,event,hook)
  _DeepEPDispatcherImplLowLatency._dispatch_core(self,hidden_states,topk_idx,use_fp8)
  _DeepEPDispatcherImplLowLatency.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency.combine_b(self,hidden_states,event,hook)
  _DeepEPDispatcherImplLowLatency._combine_core(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency._get_buffer(self)
  DeepEPDispatcher.__init__(self,group,router_topk,permute_fusion,num_experts,num_local_experts,hidden_size,params_dtype,deepep_mode,async_finish,return_recv_hook)
  DeepEPDispatcher.dispatch(self,*args,**kwargs)
  DeepEPDispatcher.dispatch_a(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPDispatcher.dispatch_b(self)
  DeepEPDispatcher.combine(self,*args,**kwargs)
  DeepEPDispatcher.combine_a(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPDispatcher.combine_b(self)
  DeepEPDispatcher._get_impl(self,forward_batch)
  DeepEPDispatcher._update_stage(self,old_stage,new_stage)
layers/moe/token_dispatcher/standard.py:
  StandardDispatchOutput.format(self)
layers/moe/topk.py:
  TopKOutputChecker.format_is_standard(topk_output)
  TopKOutputChecker.format_is_triton_kernel(topk_output)
  TopKOutputChecker.format_is_bypassed(topk_output)
  TopKOutputFormat.is_standard(self)
  TopKOutputFormat.is_triton_kernel(self)
  TopKOutputFormat.is_bypassed(self)
  TopKOutput.format(self)
  StandardTopKOutput.format(self)
  TritonKernelTopKOutput.format(self)
  BypassedTopKOutput.format(self)
  TopK.__init__(self,top_k,use_grouped_topk,topk_group,num_expert_group,renormalize,num_fused_shared_experts,custom_routing_function,scoring_func,correction_bias,routed_scaling_factor,apply_routed_scaling_factor_on_output,force_topk)
  TopK.forward_native(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_cuda(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_cpu(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_npu(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.empty_topk_output(self,device)
  fused_topk_torch_native(hidden_states,gating_output,topk,renormalize,correction_bias)
  fused_topk_cpu(hidden_states,gating_output,topk,renormalize,num_token_non_padded,expert_location_dispatch_info,correction_bias)
  apply_topk_weights_cpu(need_apply,topk_weights,inputs)
  fused_topk(hidden_states,gating_output,topk,renormalize,num_token_non_padded,expert_location_dispatch_info)
  grouped_topk_gpu(hidden_states,gating_output,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  grouped_topk_cpu(hidden_states,gating_output,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  biased_grouped_topk_impl(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  is_power_of_two(n)
  _mask_topk_ids_padded_region(topk_ids,num_token_non_padded)
  _biased_grouped_topk_postprocess(topk_ids,expert_location_dispatch_info,num_token_non_padded)
  biased_grouped_topk_gpu(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  biased_grouped_topk_cpu(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,compiled,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  select_experts(hidden_states,router_logits,topk_config,num_token_non_padded,expert_location_dispatch_info)
layers/moe/utils.py:
  MoeA2ABackend._missing_(cls,value)
  MoeA2ABackend.is_none(self)
  MoeA2ABackend.is_deepep(self)
  MoeRunnerBackend.is_auto(self)
  MoeRunnerBackend.is_triton(self)
  MoeRunnerBackend.is_triton_kernel(self)
  MoeRunnerBackend.is_flashinfer_trtllm(self)
  MoeRunnerBackend.is_flashinfer_cutlass(self)
  MoeRunnerBackend.is_flashinfer_mxfp4(self)
  DeepEPMode.enable_normal(self)
  DeepEPMode.enable_low_latency(self)
  DeepEPMode.resolve(self,is_extend_in_batch)
  DeepEPMode.is_normal(self)
  DeepEPMode.is_low_latency(self)
  DeepEPMode.is_auto(self)
  initialize_moe_config(server_args)
  get_moe_a2a_backend()
  get_moe_runner_backend()
  get_deepep_mode()
  get_deepep_config()
  is_tbo_enabled()
  get_tbo_token_distribution_threshold()
  should_use_flashinfer_trtllm_moe()
  should_use_flashinfer_cutlass_moe_fp4_allgather()
layers/multimodal.py:
  _rotl32(x,r)
  _fmix32(x,C1,C2)
  hash_tiles32_kernel_blocked(in_ptr,out_ptr,n_u32,seed1,seed2,FM_C1,FM_C2,POS_A,POS_B,TILE,BLOCK,USE_CG)
  add_tree_reduce_u64_kernel(in_ptr,out_ptr,n_elems,CHUNK)
  _as_uint32_words(t)
  _final_splitmix64(x)
  gpu_tensor_hash(tensor,seed,tile_words,block_words,reduce_chunk,num_warps,num_stages,use_cg)
layers/parameter.py:
  BasevLLMParameter.__new__(cls,data,**kwargs)
  BasevLLMParameter.__init__(self,data,weight_loader)
  BasevLLMParameter.weight_loader(self)
  BasevLLMParameter._assert_and_load(self,loaded_weight)
  BasevLLMParameter.load_column_parallel_weight(self,loaded_weight)
  BasevLLMParameter.load_row_parallel_weight(self,loaded_weight)
  BasevLLMParameter.load_merged_column_weight(self,loaded_weight,**kwargs)
  BasevLLMParameter.load_qkv_weight(self,loaded_weight,**kwargs)
  _ColumnvLLMParameter.__init__(self,output_dim,**kwargs)
  _ColumnvLLMParameter.output_dim(self)
  _ColumnvLLMParameter.load_column_parallel_weight(self,loaded_weight,tp_rank,use_presharded_weights)
  _ColumnvLLMParameter.load_merged_column_weight(self,loaded_weight,**kwargs)
  _ColumnvLLMParameter.load_qkv_weight(self,loaded_weight,tp_rank,use_presharded_weights,**kwargs)
  RowvLLMParameter.__init__(self,input_dim,**kwargs)
  RowvLLMParameter.input_dim(self)
  RowvLLMParameter.load_row_parallel_weight(self,loaded_weight,tp_rank,use_presharded_weights)
  PerTensorScaleParameter.__init__(self,**kwargs)
  PerTensorScaleParameter._shard_id_as_int(self,shard_id)
  PerTensorScaleParameter.load_row_parallel_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_merged_column_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_qkv_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_column_parallel_weight(self,*args,**kwargs)
  PerTensorScaleParameter._load_into_shard_id(self,loaded_weight,shard_id,**kwargs)
  PackedColumnParameter.__init__(self,packed_factor,packed_dim,marlin_tile_size,**kwargs)
  PackedColumnParameter.packed_dim(self)
  PackedColumnParameter.packed_factor(self)
  PackedColumnParameter.marlin_tile_size(self)
  PackedColumnParameter.adjust_shard_indexes_for_packing(self,shard_size,shard_offset)
  PackedvLLMParameter.__init__(self,packed_factor,packed_dim,marlin_tile_size,**kwargs)
  PackedvLLMParameter.packed_dim(self)
  PackedvLLMParameter.packed_factor(self)
  PackedvLLMParameter.marlin_tile_size(self)
  PackedvLLMParameter.adjust_shard_indexes_for_packing(self,shard_size,shard_offset)
  permute_param_layout_(param,input_dim,output_dim,**kwargs)
  _adjust_shard_indexes_for_marlin(shard_size,shard_offset,marlin_tile_size)
  _adjust_shard_indexes_for_packing(shard_size,shard_offset,packed_factor,marlin_tile_size)
layers/pooler.py:
  Pooler.__init__(self,pooling_type,normalize)
  Pooler.forward(self,hidden_states,forward_batch)
  CrossEncodingPooler.__init__(self,config,classifier,pooler)
  CrossEncodingPooler.forward(self,hidden_states,forward_batch)
layers/quantization/__init__.py:
  DummyConfig.override_quantization_method(self,*args,**kwargs)
  get_quantization_config(quantization)
  monkey_patch_isinstance_for_vllm_base_layer(reverse)
  patched_isinstance(obj,classinfo)
  monkey_patch_moe_apply(class_obj)
  new_apply(self,layer,x,topk_output,activation,apply_router_weight_on_input,inplace,no_combine,routed_scaling_factor)
  monkey_patch_quant_configs()
layers/quantization/awq.py:
  is_layer_skipped_awq(prefix,modules_to_not_convert)
  AWQConfig.__init__(self,weight_bits,group_size,zero_point,modules_to_not_convert)
  AWQConfig.__repr__(self)
  AWQConfig.get_scaled_act_names(self)
  AWQConfig.get_name(self)
  AWQConfig.get_supported_act_dtypes(self)
  AWQConfig.get_min_capability(cls)
  AWQConfig.get_config_filenames()
  AWQConfig.from_config(cls,config)
  AWQConfig.get_quant_method(self,layer,prefix)
  AWQMarlinConfig.__init__(self,weight_bits,group_size,zero_point,lm_head_quantized,modules_to_not_convert,full_config)
  AWQMarlinConfig.__repr__(self)
  AWQMarlinConfig.get_scaled_act_names(self)
  AWQMarlinConfig.get_name(cls)
  AWQMarlinConfig.get_supported_act_dtypes(cls)
  AWQMarlinConfig.get_min_capability(cls)
  AWQMarlinConfig.get_config_filenames(cls)
  AWQMarlinConfig.from_config(cls,config)
  AWQMarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  AWQMarlinConfig.get_quant_method(self,layer,prefix)
  AWQMarlinConfig.is_awq_marlin_compatible(cls,quant_config)
  AWQLinearMethod.__init__(self,quant_config)
  AWQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  AWQLinearMethod.process_weights_after_loading(self,layer)
  AWQLinearMethod.apply(self,layer,x,bias)
  AWQMarlinLinearMethod.__init__(self,quant_config)
  AWQMarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  AWQMarlinLinearMethod.process_weights_after_loading(self,layer)
  AWQMarlinLinearMethod.apply(self,layer,x,bias)
  AWQMoEMethod.__init__(self,quant_config)
  AWQMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  AWQMoEMethod.process_weights_after_loading(self,layer)
  AWQMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/awq_triton.py:
  awq_dequantize_kernel(qweight_ptr,scales_ptr,zeros_ptr,group_size,result_ptr,num_cols,num_rows,BLOCK_SIZE_X,BLOCK_SIZE_Y)
  awq_gemm_kernel(a_ptr,b_ptr,c_ptr,zeros_ptr,scales_ptr,M,N,K,group_size,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,SPLIT_K)
  awq_dequantize_triton(qweight,scales,zeros,block_size_x,block_size_y)
  awq_gemm_triton(input,qweight,scales,qzeros,split_k_iters,block_size_m,block_size_n,block_size_k)
layers/quantization/base_config.py:
  QuantizeMethodBase.create_weights(self,layer,*weight_args,**extra_weight_attrs)
  QuantizeMethodBase.apply(self,layer,*args,**kwargs)
  QuantizeMethodBase.process_weights_after_loading(self,layer)
  LinearMethodBase.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  LinearMethodBase.apply(self,layer,x,bias)
  FusedMoEMethodBase.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  FusedMoEMethodBase.apply(self,layer,x,topk_output,moe_runner_config)
  QuantizationConfig.__init__(self)
  QuantizationConfig.get_name(self)
  QuantizationConfig.get_supported_act_dtypes(self)
  QuantizationConfig.get_min_capability(cls)
  QuantizationConfig.get_config_filenames()
  QuantizationConfig.from_config(cls,config)
  QuantizationConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  QuantizationConfig.get_from_keys(config,keys)
  QuantizationConfig.get_from_keys_or(config,keys,default)
  QuantizationConfig.get_quant_method(self,layer,prefix)
  QuantizationConfig.get_scaled_act_names(self)
  method_has_implemented_embedding(method_class)
layers/quantization/blockwise_int8.py:
  BlockInt8Config.__init__(self,is_checkpoint_int8_serialized,activation_scheme,ignored_layers,weight_block_size)
  BlockInt8Config.get_name(cls)
  BlockInt8Config.get_supported_act_dtypes(cls)
  BlockInt8Config.get_min_capability(cls)
  BlockInt8Config.get_config_filenames(cls)
  BlockInt8Config.from_config(cls,config)
  BlockInt8Config.get_quant_method(self,layer,prefix)
  BlockInt8Config.get_scaled_act_names(self)
  BlockInt8LinearMethod.__init__(self,quant_config)
  BlockInt8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  BlockInt8LinearMethod.process_weights_after_loading(self,layer)
  BlockInt8LinearMethod.apply(self,layer,x,bias)
  BlockInt8MoEMethod.__init__(self,quant_config)
  BlockInt8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  BlockInt8MoEMethod.process_weights_after_loading(self,layer)
  BlockInt8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/compressed_tensors/compressed_tensors.py:
  DeviceCapability.as_version_str(self)
  DeviceCapability.to_int(self)
  CompressedTensorsConfig.__init__(self,target_scheme_map,ignore,quant_format,sparsity_scheme_map,sparsity_ignore_list,kv_cache_scheme,config,packed_modules_mapping)
  CompressedTensorsConfig.get_linear_method(self)
  CompressedTensorsConfig.get_supported_act_dtypes(cls)
  CompressedTensorsConfig.get_min_capability(cls)
  CompressedTensorsConfig.get_name(self)
  CompressedTensorsConfig.get_scaled_act_names(self)
  CompressedTensorsConfig.get_quant_method(self,layer,prefix)
  CompressedTensorsConfig.from_config(cls,config)
  CompressedTensorsConfig._parse_sparsity_config(cls,config)
  CompressedTensorsConfig._quantization_scheme_map_from_config(cls,config)
  CompressedTensorsConfig.get_config_filenames(cls)
  CompressedTensorsConfig._check_scheme_supported(self,min_capability,error)
  CompressedTensorsConfig._is_static_tensor_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_dynamic_token_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_fp8_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_fp8_w8a16(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_wNa16_group_channel(self,weight_quant,input_quant)
  CompressedTensorsConfig._get_scheme_from_parts(self,weight_quant,input_quant)
  CompressedTensorsConfig.get_scheme(self,layer,layer_name)
  CompressedTensorsConfig.get_cache_scale(self,name)
  CompressedTensorsConfig.supports_cutlass_24(weight_quant,input_quant,sparsity_scheme)
  CompressedTensorsLinearMethod.__init__(self,quantization_config)
  CompressedTensorsLinearMethod.process_weights_after_loading(self,layer)
  CompressedTensorsLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  CompressedTensorsLinearMethod.apply(self,layer,x,bias)
layers/quantization/compressed_tensors/compressed_tensors_moe.py:
  CompressedTensorsMoEMethod.__new__(cls,*args,**kwargs)
  CompressedTensorsMoEMethod.get_moe_method(quant_config)
  CompressedTensorsW8A8Fp8MoEMethod.__init__(self,quant_config)
  CompressedTensorsW8A8Fp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  CompressedTensorsW8A8Fp8MoEMethod.process_weights_after_loading(self,layer)
  CompressedTensorsW8A8Fp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  CompressedTensorsWNA16MoEMethod.__init__(self,quant_config)
  CompressedTensorsWNA16MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  CompressedTensorsWNA16MoEMethod.process_weights_after_loading(self,layer)
  CompressedTensorsWNA16MoEMethod.replace_tensor(name,new_t)
  CompressedTensorsWNA16MoEMethod.get_scale_perms(num_bits)
  CompressedTensorsWNA16MoEMethod.marlin_permute_scales(s,size_k,size_n,group_size,num_bits)
  CompressedTensorsWNA16MoEMethod.marlin_moe_permute_scales(s,size_k,size_n,group_size,num_bits)
  CompressedTensorsWNA16MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py:
  CompressedTensorsScheme.get_min_capability(cls)
  CompressedTensorsScheme.create_weights(self,*args,**kwargs)
  CompressedTensorsScheme.apply_weights(self,layer,x,bias)
  CompressedTensorsScheme.process_weights_after_loading(self,layer)
layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py:
  apply_fp8_marlin_linear(*args,**kwargs)
  prepare_fp8_layer_for_marlin(*args,**kwargs)
  CompressedTensorsW8A16Fp8.__init__(self,strategy,is_static_input_scheme)
  CompressedTensorsW8A16Fp8.get_min_capability(cls)
  CompressedTensorsW8A16Fp8.process_weights_after_loading(self,layer)
  CompressedTensorsW8A16Fp8.create_weights(self,layer,input_size,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  CompressedTensorsW8A16Fp8.apply_weights(self,layer,x,bias)
layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py:
  CompressedTensorsW8A8Fp8.__init__(self,strategy,is_static_input_scheme)
  CompressedTensorsW8A8Fp8.get_min_capability(cls)
  CompressedTensorsW8A8Fp8.process_weights_after_loading(self,layer)
  CompressedTensorsW8A8Fp8.create_weights(self,layer,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  CompressedTensorsW8A8Fp8.apply_weights(self,layer,x,bias)
layers/quantization/compressed_tensors/utils.py:
  is_activation_quantization_format(format)
  should_ignore_layer(layer_name,ignore,fused_mapping)
  check_equal_or_regex_match(layer_name,targets)
  find_matched_target(layer_name,module,targets,fused_mapping)
  _find_first_match(value,targets,check_contains)
  _is_equal_or_regex_match(value,target,check_contains)
  _match_fused_layer(layer_name,target_layers,fused_mapping)
layers/quantization/deep_gemm_wrapper/compile_utils.py:
  update_deep_gemm_config(gpu_id,server_args)
  _maybe_compile_deep_gemm_one_type_all(kernel_type,n,k,num_groups)
  _compile_deep_gemm_one_type_all(kernel_type,n,k,num_groups,m_list)
  _BaseWarmupExecutor.create(kernel_type,**kwargs)
  _BaseWarmupExecutor.execute(self,m)
  _empty_token_fp8(size)
  _empty_block_fp8(size)
  _NormalWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _NormalWarmupExecutor.execute(self,m)
  _GroupedContWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _GroupedContWarmupExecutor.execute(self,m)
  _GroupedMaskedWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _GroupedMaskedWarmupExecutor.execute(self,m)
  deep_gemm_execution_hook(m,n,k,num_groups,kernel_type)
layers/quantization/deep_gemm_wrapper/configurer.py:
  _compute_enable_deep_gemm()
  _is_blackwell_arch()
layers/quantization/deep_gemm_wrapper/entrypoint.py:
  grouped_gemm_nt_f8f8bf16_masked(lhs,rhs,out,masked_m,expected_m)
  grouped_gemm_nt_f8f8bf16_contig(lhs,rhs,out,m_indices)
  gemm_nt_f8f8bf16(lhs,rhs,out)
  update_deep_gemm_config(gpu_id,server_args)
  configure_deep_gemm_num_sms(num_sms)
layers/quantization/fp8.py:
  dummy_func(*args,**kwargs)
  Fp8Config.__init__(self,is_checkpoint_fp8_serialized,activation_scheme,ignored_layers,weight_block_size)
  Fp8Config.get_name(cls)
  Fp8Config.get_supported_act_dtypes(cls)
  Fp8Config.get_min_capability(cls)
  Fp8Config.get_config_filenames(cls)
  Fp8Config.from_config(cls,config)
  Fp8Config.get_quant_method(self,layer,prefix)
  Fp8Config.get_scaled_act_names(self)
  Fp8LinearMethod.__init__(self,quant_config)
  Fp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  Fp8LinearMethod.process_weights_after_loading(self,layer)
  Fp8LinearMethod.apply(self,layer,x,bias)
  get_tile_tokens_dim(num_tokens,top_k,num_experts)
  Fp8MoEMethod.__init__(self,quant_config)
  Fp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  Fp8MoEMethod.process_weights_after_loading(self,layer)
  Fp8MoEMethod.process_weights_hip_int4(self,layer)
  Fp8MoEMethod.process_weights_hip_scale_padding(self,layer)
  Fp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  Fp8MoEMethod.apply_with_router_logits(self,layer,x,topk_output,moe_runner_config)
  Fp8MoEMethod.maybe_apply_hip_fused_experts(self,layer,x,topk_output,activation,no_combine)
  Fp8KVCacheMethod.__init__(self,quant_config)
layers/quantization/fp8_kernel.py:
  is_fp8_fnuz()
  deep_gemm_fp8_fp8_bf16_nt(A,As,B,Bs,C)
  deep_gemm_fp8_fp8_bf16_nt_fake(A,As,B,Bs,C)
  _per_token_group_quant_8bit(y_ptr,y_q_ptr,y_s_ptr,y_stride,N,eps,bit8_min,bit8_max,BLOCK)
  _per_token_group_quant_8bit_colmajor(y_ptr,y_q_ptr,y_s_ptr,group_size,y_num_columns,y_s_col_stride,eps,bit8_min,bit8_max,BLOCK,SCALE_UE8M0)
  _per_token_group_quant_8bit_raw(x,group_size,eps,dtype,column_major_scales,scale_tma_aligned,scale_ue8m0)
  _per_token_group_quant_8bit_fuse_silu_and_mul(x,group_size,dst_dtype,column_major_scales,scale_tma_aligned,scale_ue8m0,masked_m)
  per_token_group_quant_8bit(x,group_size,dst_dtype,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  create_per_token_group_quant_fp8_output_scale(x_shape,device,group_size,column_major_scales,scale_tma_aligned,scale_ue8m0)
  sglang_per_token_group_quant_fp8(x,group_size,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  sglang_per_token_group_quant_8bit(x,group_size,dst_dtype,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  sglang_per_token_quant_fp8(x,dtype)
  _static_quant_fp8(y_ptr,y_q_ptr,y_s_ptr,y_s_repeat_ptr,y_stride,N,fp8_min,fp8_max,BLOCK,REPEAT_SCALE)
  static_quant_fp8(x,x_s,repeat_scale)
  _w8a8_block_fp8_matmul(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  _w8a8_block_fp8_matmul_unrolledx4(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  get_w8a8_block_fp8_configs(N,K,block_n,block_k)
  select_w8a8_block_fp8_matmul_kernel(M,N,META)
  use_w8a8_block_fp8_matmul_unrolledx4(M,N,META)
  select_w8a8_block_fp8_matmul_kernel(M,N,META)
  prepare_block_fp8_matmul_inputs(A,B,As,Bs,block_size,output_dtype)
  w8a8_block_fp8_matmul_deepgemm(A,B,As,Bs,block_size,output_dtype)
  w8a8_block_fp8_matmul_triton(A,B,As,Bs,block_size,output_dtype)
  grid(META)
  w8a8_block_fp8_matmul(A,B,As,Bs,block_size,output_dtype)
  _per_tensor_quant_mla_fp8_stage1(x_ptr,x_s_ptr,head_size,x_stride_h,x_stride_s,eps,fp8_max,BLOCK_SIZE)
  _per_tensor_quant_mla_fp8_stage2(x_ptr,x_s_ptr,x_q_ptr,num_seq,head_size,x_stride_h,x_stride_s,fp8_min,fp8_max,BLOCK_SIZE)
  per_tensor_quant_mla_fp8(x,x_s_out,eps)
  _per_token_group_quant_mla_deep_gemm_masked_fp8(y_ptr,y_q_ptr,y_s_ptr,masked_m_ptr,group_size,y_stride_b,y_stride_t,y_q_stride_b,y_q_stride_t,y_s_stride_b,y_s_stride_g,eps,fp8_min,fp8_max,NUM_GROUP,BLOCK)
  per_token_group_quant_mla_deep_gemm_masked_fp8(x,group_size,eps,dtype)
  scaled_fp8_quant(input,scale,num_token_padding,use_per_token_if_dynamic)
  scaled_fp8_quant(input,scale,num_token_padding,use_per_token_if_dynamic)
  _per_token_group_quant_fp8_hopper_moe_mn_major(a,expert_offsets,problem_sizes,a_fp8,sfa,K,BLOCK_K,M_ALIGNMENT,BLOCK_M)
  per_token_group_quant_fp8_hopper_moe_mn_major(A,expert_offsets,problem_sizes,group_size,expert_tokens_alignment)
  _per_group_transpose(data_ptr,trans_data_ptr,expert_offsets,k,M_ALIGNMENT,BLOCK_SIZE_M,BLOCK_SIZE_K)
  per_group_transpose(a,expert_offsets,M_ALIGNMENT)
  is_weak_contiguous(x)
  scaled_mm_kernel(a_ptr,b_ptr,scale_a_ptr,scale_b_ptr,c_ptr,bias_ptr,M,N,K,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,ACCUMULATOR_DTYPE,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,BLOCK_SIZE_SCALE_A,BLOCK_SIZE_SCALE_B)
  triton_scaled_mm(input,weight,scale_a,scale_b,out_dtype,bias,block_size_m,block_size_n,block_size_k,use_heuristic)
layers/quantization/fp8_utils.py:
  use_rowwise_torch_scaled_mm()
  cutlass_fp8_supported()
  normalize_e4m3fn_to_e4m3fnuz(weight,weight_scale,input_scale)
  cutlass_block_fp8_supported()
  dispatch_w8a8_block_fp8_linear()
  flashinfer_gemm_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  cutlass_w8a8_block_fp8_linear_with_fallback(input,weight,block_size,weight_scale,input_scale,bias)
  deepgemm_w8a8_block_fp8_linear_with_fallback(input,weight,block_size,weight_scale,input_scale,bias)
  _check_ue8m0(name,x)
  aiter_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  triton_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  dequant_mxfp4(w_block,w_scale,out_dtype)
  input_to_float8(x,dtype)
  block_quant_to_tensor_quant(x_q_block,x_s,block_size)
  block_quant_dequant(x_q_block,x_s,block_size,dtype)
  requant_weight_ue8m0_inplace(weight,weight_scale_inv,weight_block_size)
  _requant_weight_ue8m0(weight,weight_scale_inv,weight_block_size)
  _transform_scale(sf,mn)
  per_block_cast_to_fp8(x)
  ceil_to_ue8m0(x)
  channel_quant_to_tensor_quant(x_q_channel,x_s)
  _process_scaled_mm_output(output,input_2d_shape,output_shape)
  _apply_fallback_scaled_mm(qinput,weight,x_scale,weight_scale,input_2d_shape,output_shape,bias,input_dtype)
  apply_fp8_linear(input,weight,weight_scale,input_scale,input_scale_ub,bias,cutlass_fp8_supported,use_per_token_if_dynamic,pad_output,compressed_tensor_quant)
  can_auto_enable_marlin_fp8()
layers/quantization/fpgemm_fp8.py:
  FBGEMMFp8Config.__init__(self,ignore_list,input_scale_ub)
  FBGEMMFp8Config.get_name(cls)
  FBGEMMFp8Config.get_supported_act_dtypes(cls)
  FBGEMMFp8Config.get_min_capability(cls)
  FBGEMMFp8Config.get_config_filenames(cls)
  FBGEMMFp8Config.from_config(cls,config)
  FBGEMMFp8Config.get_quant_method(self,layer,prefix)
  FBGEMMFp8Config.get_scaled_act_names(self)
  FBGEMMFp8LinearMethod.__init__(self,quant_config)
  FBGEMMFp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  FBGEMMFp8LinearMethod.process_weights_after_loading(self,layer)
  FBGEMMFp8LinearMethod.apply(self,layer,x,bias)
layers/quantization/gptq.py:
  check_marlin_format(hf_quant_cfg)
  gptq_marlin_moe_repack(b_q_weight,perm,size_k,size_n,num_bits)
  GPTQConfig.__init__(self,weight_bits,group_size,desc_act,lm_head_quantized,dynamic)
  GPTQConfig.__repr__(self)
  GPTQConfig.get_scaled_act_names(self)
  GPTQConfig.get_name(cls)
  GPTQConfig.get_supported_act_dtypes(cls)
  GPTQConfig.get_min_capability(cls)
  GPTQConfig.get_config_filenames(cls)
  GPTQConfig.from_config(cls,config)
  GPTQConfig.get_quant_method(self,layer,prefix)
  GPTQMarlinConfig.__init__(self,weight_bits,group_size,desc_act,is_sym,lm_head_quantized,dynamic,full_config)
  GPTQMarlinConfig.__repr__(self)
  GPTQMarlinConfig.get_scaled_act_names(self)
  GPTQMarlinConfig.get_name(cls)
  GPTQMarlinConfig.get_supported_act_dtypes(cls)
  GPTQMarlinConfig.get_min_capability(cls)
  GPTQMarlinConfig.get_config_filenames(cls)
  GPTQMarlinConfig.from_config(cls,config)
  GPTQMarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  GPTQMarlinConfig.get_quant_method(self,layer,prefix)
  GPTQMarlinConfig.is_gptq_marlin_compatible(cls,quant_config)
  GPTQLinearMethod.__init__(self,quant_config)
  GPTQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  GPTQLinearMethod.process_weights_after_loading(self,layer)
  GPTQLinearMethod.apply(self,layer,x,bias)
  GPTQMarlinLinearMethod.__init__(self,quant_config)
  GPTQMarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  GPTQMarlinLinearMethod.process_weights_after_loading(self,layer)
  GPTQMarlinLinearMethod._transform_param(layer,name,fn)
  GPTQMarlinLinearMethod.transform_w_q(x)
  GPTQMarlinLinearMethod.transform_w_s(x)
  GPTQMarlinLinearMethod.apply(self,layer,x,bias)
  GPTQMarlinLinearMethod._get_weight_params(layer)
  GPTQMarlinMoEMethod.__init__(self,quant_config)
  GPTQMarlinMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  GPTQMarlinMoEMethod.process_weights_after_loading(self,layer)
  GPTQMarlinMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/int8_kernel.py:
  _per_token_quant_int8(x_ptr,xq_ptr,scale_ptr,x_sum_ptr,stride_x,stride_xq,N,CAL_SUM,BLOCK)
  per_token_quant_int8(x,scale_dtype,cal_sum)
  _per_token_group_quant_int8(y_ptr,y_q_ptr,y_s_ptr,y_stride,N,eps,int8_min,int8_max,BLOCK)
  per_token_group_quant_int8(x,group_size,eps,dtype)
  sglang_per_token_group_quant_int8(x,group_size,eps,dtype)
  _w8a8_block_int8_matmul(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  get_w8a8_block_int8_configs(N,K,block_n,block_k)
  w8a8_block_int8_matmul(A,B,As,Bs,block_size,output_dtype)
  grid(META)
layers/quantization/int8_utils.py:
  apply_w8a8_block_int8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  input_to_int8(x,dtype)
  block_dequant(x_q_block,x_s,block_size)
layers/quantization/kv_cache.py:
  BaseKVCacheMethod.__init__(self,quant_config)
  BaseKVCacheMethod.create_weights(self,layer)
  BaseKVCacheMethod.apply(self,layer)
  BaseKVCacheMethod.process_weights_after_loading(self,layer)
layers/quantization/marlin_utils.py:
  query_marlin_supported_quant_types(has_zp,include_fp_type,device_capability)
  _check_marlin_supported(quant_type,group_size,has_zp,device_capability)
  check_marlin_supported(quant_type,group_size,has_zp,device_capability)
  verify_marlin_supported(quant_type,group_size,has_zp)
  verify_marlin_supports_shape(output_size_per_partition,input_size_per_partition,input_size,group_size)
  check_marlin_supports_shape(output_size_per_partition,input_size_per_partition,input_size,group_size)
  check_marlin_supports_layer(layer,group_size)
  check_moe_marlin_supports_layer(layer,group_size)
  marlin_make_workspace(device,max_blocks_per_sm)
  marlin_is_k_full(act_order,is_row_parallel)
  marlin_repeat_scales_on_all_ranks(act_order,group_size,is_row_parallel)
  marlin_make_empty_g_idx(device)
  marlin_make_empty_zp(device)
  marlin_sort_g_idx(g_idx)
  get_scale_perms()
  marlin_permute_scales(s,size_k,size_n,group_size)
  marlin_permute_bias(s)
  marlin_moe_permute_scales(s,size_k,size_n,group_size)
  marlin_zero_points(zp,size_k,size_n,num_bits)
  awq_to_marlin_zero_points(q_zp_packed,size_k,size_n,num_bits)
  moe_awq_to_marlin_zero_points(q_zp_packed,size_k,size_n,num_bits)
  maybe_warn_marlin_atomic_add(device,dtype)
  maybe_warn_marlin_atomic_add_env()
  should_use_atomic_add_reduce(m,n,k,device,dtype)
  apply_gptq_marlin_linear(input,weight,weight_scale,weight_zp,g_idx,g_idx_sort_indices,workspace,wtype,output_size_per_partition,input_size_per_partition,is_k_full,bias,use_fp32_reduce)
  apply_awq_marlin_linear(input,weight,weight_scale,weight_zp,g_idx,g_idx_sort_indices,workspace,quant_type,output_size_per_partition,input_size_per_partition,bias,use_fp32_reduce)
  MarlinConfig.__init__(self,group_size,lm_head_quantized)
  MarlinConfig.__repr__(self)
  MarlinConfig.get_name(cls)
  MarlinConfig.get_supported_act_dtypes(cls)
  MarlinConfig.get_min_capability(cls)
  MarlinConfig.get_config_filenames(cls)
  MarlinConfig.from_config(cls,config)
  MarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  MarlinConfig.get_quant_method(self,layer,prefix)
  MarlinLinearMethod.__init__(self,quant_config)
  MarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  MarlinLinearMethod.process_weights_after_loading(self,layer)
  MarlinLinearMethod.apply(self,layer,x,bias)
layers/quantization/marlin_utils_fp8.py:
  fp8_fused_exponent_bias_into_scales(scales)
  apply_fp8_marlin_linear(input,weight,weight_scale,workspace,size_n,size_k,bias,use_fp32_reduce)
  prepare_fp8_layer_for_marlin(layer,size_k_first)
  prepare_moe_fp8_layer_for_marlin(layer,size_k_first)
  pack_fp8_to_int32(fp8_tensor,size_k_first)
  marlin_quant_fp8_torch(weight,group_size)
layers/quantization/modelopt_quant.py:
  ModelOptFp8Config.__init__(self,is_checkpoint_fp8_serialized,kv_cache_quant_method,exclude_modules)
  ModelOptFp8Config.get_name(cls)
  ModelOptFp8Config.get_supported_act_dtypes(cls)
  ModelOptFp8Config.get_min_capability(cls)
  ModelOptFp8Config.get_config_filenames(cls)
  ModelOptFp8Config.from_config(cls,config)
  ModelOptFp8Config.get_quant_method(self,layer,prefix)
  ModelOptFp8Config.get_scaled_act_names(self)
  ModelOptFp8LinearMethod.__init__(self,quant_config)
  ModelOptFp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,params_dtype,**extra_weight_attrs)
  ModelOptFp8LinearMethod.process_weights_after_loading(self,layer)
  ModelOptFp8LinearMethod.apply(self,layer,x,bias)
  ModelOptFp8KVCacheMethod.__init__(self,quant_config)
  ModelOptFp8MoEMethod.__init__(self,quant_config)
  ModelOptFp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  ModelOptFp8MoEMethod.process_weights_after_loading(self,layer)
  ModelOptFp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  ModelOptFp4Config.__init__(self,is_checkpoint_nvfp4_serialized,kv_cache_quant_algo,group_size,exclude_modules)
  ModelOptFp4Config.get_name(cls)
  ModelOptFp4Config.get_supported_act_dtypes(cls)
  ModelOptFp4Config.get_min_capability(cls)
  ModelOptFp4Config.get_config_filenames(cls)
  ModelOptFp4Config.from_config(cls,config)
  ModelOptFp4Config.is_layer_excluded(self,prefix,exclude_modules)
  ModelOptFp4Config.get_quant_method(self,layer,prefix)
  ModelOptFp4Config.get_scaled_act_names(self)
  ModelOptFp4LinearMethod.__init__(self,quant_config)
  ModelOptFp4LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  ModelOptFp4LinearMethod.process_weights_after_loading(self,layer)
  ModelOptFp4LinearMethod.apply(self,layer,x,bias)
  ModelOptNvFp4FusedMoEMethod.__init__(self,quant_config)
  ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe(self)
  ModelOptNvFp4FusedMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  ModelOptNvFp4FusedMoEMethod.swizzle_blockscale(self,scale)
  ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel(self,gemm1_weights,gemm2_weights,gemm1_scales_linear_fp4_bytes,gemm2_scales_linear_fp4_bytes,hidden_size,intermediate_size,num_experts)
  ModelOptNvFp4FusedMoEMethod.process_weights_after_loading(self,layer)
  ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first(self)
  ModelOptNvFp4FusedMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/moe_wna16.py:
  get_weight_perm(num_bits)
  MoeWNA16Config.__init__(self,linear_quant_method,weight_bits,group_size,has_zp,lm_head_quantized,modules_to_not_convert,full_config)
  MoeWNA16Config.get_name(cls)
  MoeWNA16Config.get_supported_act_dtypes(cls)
  MoeWNA16Config.get_min_capability(cls)
  MoeWNA16Config.get_config_filenames(cls)
  MoeWNA16Config.get_scaled_act_names(self)
  MoeWNA16Config.from_config(cls,config)
  MoeWNA16Config.override_quantization_method(cls,hf_quant_cfg,user_quant)
  MoeWNA16Config.is_moe_wna16_compatible(cls,quant_config)
  MoeWNA16Config.get_quant_method(self,layer,prefix)
  is_layer_skipped_quant(prefix,modules_to_not_convert)
  MoeWNA16Method.__init__(self,quant_config)
  MoeWNA16Method.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  MoeWNA16Method.apply(self,layer,x,topk_output,moe_runner_config)
  MoeWNA16Method.get_weight_loader(layer,weight_loader)
  MoeWNA16Method.convert_awq_tensor(tensor,tensor_type)
  MoeWNA16Method.convert_gptq_int4_qzeros(tensor)
  MoeWNA16Method.moe_wna16_weight_loader(param,loaded_weight,weight_name,shard_id,expert_id)
layers/quantization/mxfp4.py:
  _swizzle_mxfp4(quant_tensor,scale,num_warps)
  _dequant_mxfp4(x,scale,float_dtype)
  _dequant_mxfp4_fake(x,scale,float_dtype)
  _quant_dequant_mxfp4(x,scale_calculation_mode)
  _quant_dequant_mxfp4_fake(x,scale_calculation_mode)
  Mxfp4Config.__init__(self,ignored_layers,is_checkpoint_mxfp4_serialized)
  Mxfp4Config.from_config(cls,config)
  Mxfp4Config.get_min_capability(cls)
  Mxfp4Config.get_name(cls)
  Mxfp4Config.get_supported_act_dtypes(cls)
  Mxfp4Config.get_config_filenames(cls)
  Mxfp4Config.is_static_cfg(self)
  Mxfp4Config.get_quant_method(self,layer,prefix)
  Mxfp4Config.get_scaled_act_names(self)
  Mxfp4MoEMethod.__init__(self,prefix)
  Mxfp4MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,with_bias,**extra_weight_attrs)
  Mxfp4MoEMethod.process_weights_after_loading(self,layer)
  Mxfp4MoEMethod.swap_every_two_rows(x,axis)
  Mxfp4MoEMethod._get_tile_tokens_dim(self,x,top_k)
  Mxfp4MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  Mxfp4DynamicQuantMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  Mxfp4DynamicQuantMoEMethod.mxfp4_quantize(self,w)
  Mxfp4DynamicQuantMoEMethod.process_weights_after_loading(self,layer)
  Mxfp4DynamicQuantMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/mxfp4_tensor.py:
  MXFP4QuantizeUtil.quantize(cls,input,block_size)
  MXFP4QuantizeUtil.cast_fp4(x)
  MXFP4QuantizeUtil.fuse_uint4_to_uint8(x)
  MXFP4QuantizeUtil.dequantize(cls,quantized_data,dtype,scale,block_sizes)
  MXFP4QuantizeUtil.unfuse_uint8_to_uint4(x)
layers/quantization/petit.py:
  PetitNvFp4Config.__init__(self,is_checkpoint_nvfp4_serialized,kv_cache_quant_algo,group_size,exclude_modules)
  PetitNvFp4Config.get_name(cls)
  PetitNvFp4Config.get_supported_act_dtypes(cls)
  PetitNvFp4Config.get_min_capability(cls)
  PetitNvFp4Config.get_config_filenames(cls)
  PetitNvFp4Config.from_config(cls,config)
  PetitNvFp4Config.override_quantization_method(cls,hf_quant_cfg,user_quant)
  PetitNvFp4Config.is_petit_nvfp4_compatible(cls,quant_config)
  PetitNvFp4Config.is_layer_excluded(self,prefix,exclude_modules)
  PetitNvFp4Config.get_quant_method(self,layer,prefix)
  PetitNvFp4Config.get_scaled_act_names(self)
  PetitNvFp4LinearMethod.__init__(self,quant_config)
  PetitNvFp4LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  PetitNvFp4LinearMethod.process_weights_after_loading(self,layer)
  PetitNvFp4LinearMethod.apply(self,layer,x,bias)
layers/quantization/petit_utils.py:
  _check_petit_nvfp4_supported(quant_method,group_size)
  prepare_nvfp4_layer_for_petit(layer)
  apply_petit_nvfp4_linear(input,weight,weight_scale,weight_scale_2,size_n,size_k,bias)
  _check_petit_nvfp4_supported(quant_method,group_size)
  verify_petit_nvfp4_supported(quant_method,group_size)
  prepare_nvfp4_layer_for_petit(layer)
  apply_petit_nvfp4_linear(input,weight,weight_scale,weight_scale_2,size_n,size_k,bias)
layers/quantization/qoq.py:
  QoQConfig.__init__(self,weight_bits,group_size)
  QoQConfig.__repr__(self)
  QoQConfig.get_supported_act_dtypes(cls)
  QoQConfig.get_min_capability(cls)
  QoQConfig.get_name(cls)
  QoQConfig.get_config_filenames(cls)
  QoQConfig.from_config(cls,config)
  QoQConfig.get_quant_method(self,layer,prefix)
  QoQConfig.get_scaled_act_names(self)
  QoQLinearMethod.__init__(self,quant_config)
  QoQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  QoQLinearMethod.process_weights_after_loading(self,layer)
  QoQLinearMethod.apply(self,layer,x,bias)
layers/quantization/quark/quark.py:
  QuarkConfig.__init__(self,quant_config,kv_cache_group,kv_cache_config,pack_method)
  QuarkConfig.get_linear_method(self)
  QuarkConfig.get_supported_act_dtypes(cls)
  QuarkConfig.get_min_capability(cls)
  QuarkConfig.get_name(self)
  QuarkConfig.get_quant_method(self,layer,prefix)
  QuarkConfig.from_config(cls,config)
  QuarkConfig.get_config_filenames(cls)
  QuarkConfig._check_scheme_supported(self,min_capability,error)
  QuarkConfig._is_mx_fp4(self,weight_quant,input_quant)
  QuarkConfig._find_matched_config(self,layer_name,module)
  QuarkConfig._get_scheme_from_config(self,config)
  QuarkConfig.get_scheme(self,layer,layer_name)
  QuarkConfig.get_scaled_act_names(self)
  QuarkLinearMethod.__init__(self,quantization_config)
  QuarkLinearMethod.process_weights_after_loading(self,layer)
  QuarkLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  QuarkLinearMethod.apply(self,layer,x,bias)
  QuarkKVCacheMethod.__init__(self,quant_config)
  QuarkKVCacheMethod.validate_kv_cache_config(kv_cache_config)
layers/quantization/quark/quark_moe.py:
  QuarkMoEMethod.__new__(cls,*args,**kwargs)
  QuarkMoEMethod.get_moe_method(quant_config,module,layer_name)
  QuarkW4A4MXFp4MoEMethod.__init__(self,weight_config,input_config)
  QuarkW4A4MXFp4MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  QuarkW4A4MXFp4MoEMethod.process_weights_after_loading(self,layer)
  QuarkW4A4MXFp4MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/quark/schemes/quark_scheme.py:
  QuarkScheme.get_min_capability(cls)
  QuarkScheme.create_weights(self,*args,**kwargs)
  QuarkScheme.apply_weights(self,layer,x,bias)
  QuarkScheme.process_weights_after_loading(self,layer)
layers/quantization/quark/schemes/quark_w4a4_mxfp4.py:
  QuarkW4A4MXFP4.__init__(self,weight_quant_spec,input_quant_spec)
  QuarkW4A4MXFP4.get_min_capability(cls)
  QuarkW4A4MXFP4.process_weights_after_loading(self,layer)
  QuarkW4A4MXFP4.create_weights(self,layer,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  QuarkW4A4MXFP4.apply_weights(self,layer,x,bias)
layers/quantization/quark/utils.py:
  deep_compare(dict1,dict2)
  should_ignore_layer(layer_name,ignore,fused_mapping)
  check_equal_or_regex_match(layer_name,targets)
  _is_equal_or_regex_match(value,target,check_contains)
layers/quantization/unquant.py:
  UnquantizedEmbeddingMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  UnquantizedEmbeddingMethod.apply(self,layer,x,bias)
  UnquantizedEmbeddingMethod.embedding(self,layer,input_)
  UnquantizedLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  UnquantizedLinearMethod.process_weights_after_loading(self,layer)
  UnquantizedLinearMethod.apply(self,layer,x,bias)
  UnquantizedFusedMoEMethod.__init__(self,use_triton_kernels)
  UnquantizedFusedMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,with_bias,**extra_weight_attrs)
  UnquantizedFusedMoEMethod.process_weights_after_loading(self,layer)
  UnquantizedFusedMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_cuda(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_cpu(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_npu(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_tpu(self,*args,**kwargs)
layers/quantization/utils.py:
  get_scalar_types()
  MockScalarTypes.__getattr__(self,name)
  is_layer_skipped(prefix,ignored_layers,fused_mapping)
  per_tensor_dequantize(tensor,inv_scale)
  all_close_1d(x)
  convert_to_channelwise(weight_scale,logical_widths)
  requantize_with_max_scale(weight,weight_scale,logical_widths)
  update_tensor_inplace(old,new)
  replace_parameter(mod,name,new)
  assert_fp8_all_close(a,b)
  override_config(config,prefix)
  get_dynamic_override(config,layer_name,key,default_value)
  get_linear_quant_method(config,layer,prefix,linear_method_cls)
  get_pack_factor(num_bits)
  permute_rows(q_w,w_ref,group_size,test_perm)
  pack_cols(q_w,num_bits,size_k,size_n)
  pack_rows(q_w,num_bits,size_k,size_n)
  unpack_cols(packed_q_w,num_bits,size_k,size_n)
  quantize_weights(w,quant_type,group_size,zero_points,ref_zero_points_after_scales)
  reshape_w(w)
  gptq_quantize_weights(w,quant_type,group_size,act_order,test_perm)
  sort_weights(q_w,g_idx)
layers/quantization/w4afp8.py:
  W4AFp8Config.__init__(self,is_checkpoint_fp8_serialized,is_checkpoint_w4afp8_serialized,linear_activation_scheme,moe_activation_scheme,ignored_layers,weight_block_size,group_size)
  W4AFp8Config.get_name(cls)
  W4AFp8Config.get_supported_act_dtypes(cls)
  W4AFp8Config.get_min_capability(cls)
  W4AFp8Config.get_config_filenames(cls)
  W4AFp8Config.from_config(cls,config)
  W4AFp8Config.get_quant_method(self,layer,prefix)
  W4AFp8Config.get_scaled_act_names(self)
  W4AFp8MoEMethod.__init__(self,quant_config)
  W4AFp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W4AFp8MoEMethod._interleave_scales(self,scales)
  W4AFp8MoEMethod.process_weights_after_loading(self,layer)
  W4AFp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/w8a8_fp8.py:
  W8A8Fp8Config.__init__(self,is_checkpoint_fp8_serialized)
  W8A8Fp8Config.get_supported_act_dtypes(cls)
  W8A8Fp8Config.get_min_capability(cls)
  W8A8Fp8Config.get_name(self)
  W8A8Fp8Config.get_config_filenames(cls)
  W8A8Fp8Config.from_config(cls,config)
  W8A8Fp8Config.get_quant_method(self,layer,prefix)
  W8A8Fp8Config.get_scaled_act_names(self)
  W8A8Fp8LinearMethod.__init__(self,quantization_config)
  W8A8Fp8LinearMethod.process_weights_after_loading(self,layer)
  W8A8Fp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  W8A8Fp8LinearMethod.apply(self,layer,x,bias)
  W8A8FP8MoEMethod.__init__(self,quant_config)
  W8A8FP8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W8A8FP8MoEMethod.process_weights_after_loading(self,layer)
  W8A8FP8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/quantization/w8a8_int8.py:
  npu_wrapper_rmsnorm_init(func)
  init(self,hidden_size,**extra_args)
  npu_wrapper_rmsnorm_forward(func)
  _rmsnorm_forward_oot(self,x,residual)
  npu_fused_experts(hidden_states,w13,w13_scale,w2,w2_scale,topk_weights,topk_ids,top_k)
  W8A8Int8Config.__init__(self,quant_config)
  W8A8Int8Config.get_supported_act_dtypes(cls)
  W8A8Int8Config.get_min_capability(cls)
  W8A8Int8Config.get_name(self)
  W8A8Int8Config.get_config_filenames(cls)
  W8A8Int8Config.from_config(cls,config)
  W8A8Int8Config.get_quant_method(self,layer,prefix)
  W8A8Int8Config.is_layer_skipped(self,prefix,fused_mapping)
  W8A8Int8Config.get_scaled_act_names(self)
  W8A8Int8LinearMethod.__init__(self,quantization_config)
  W8A8Int8LinearMethod.process_weights_after_loading(self,layer)
  W8A8Int8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  W8A8Int8LinearMethod.apply(self,layer,x,bias)
  W8A8Int8MoEMethod.__init__(self,quant_config)
  W8A8Int8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W8A8Int8MoEMethod.process_weights_after_loading(self,layer)
  W8A8Int8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  NPU_W8A8LinearMethodImpl.__init__(self)
  NPU_W8A8LinearMethodImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8LinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8LinearMethodImpl.apply(layer,x,bias)
  NPU_W8A8LinearMethodImpl.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethodMTImpl.__init__(self)
  NPU_W8A8LinearMethodMTImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8LinearMethodMTImpl.apply(layer,x,bias)
  NPU_W8A8LinearMethodMTImpl.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethod.__init__(self,quantization_config)
  NPU_W8A8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8LinearMethod.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethod.apply(self,layer,x,bias)
  NPU_W8A8DynamicLinearMethodImpl.__init__(self)
  NPU_W8A8DynamicLinearMethodImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.apply(layer,x,bias,tp_rank)
  NPU_W8A8DynamicLinearMethodImpl.process_weights_after_loading(self,layer)
  NPU_W8A8DynamicLinearMethod.__init__(self,quantization_config)
  NPU_W8A8DynamicLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8DynamicLinearMethod.process_weights_after_loading(self,layer)
  NPU_W8A8DynamicLinearMethod.apply(self,layer,x,bias)
  NPU_W8A8MoEMethod.__init__(self,quantization_config)
  NPU_W8A8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8MoEMethod.process_weights_after_loading(self,layer)
  NPU_W8A8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
layers/radix_attention.py:
  RadixAttention.__init__(self,num_heads,head_dim,scaling,num_kv_heads,layer_id,logit_cap,v_head_dim,sliding_window_size,is_cross_attention,pos_encoding_mode,logit_capping_method,quant_config,attn_type,use_irope,prefix)
  RadixAttention.forward(self,q,k,v,forward_batch,save_kv_cache,**kwargs)
layers/rotary_embedding.py:
  _rotate_neox(x)
  _rotate_gptj(x)
  _apply_rotary_emb(x,cos,sin,is_neox_style)
  RotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype)
  RotaryEmbedding._compute_inv_freq(self,base)
  RotaryEmbedding._compute_cos_sin_cache(self)
  RotaryEmbedding.forward_native(self,positions,query,key,offsets)
  RotaryEmbedding.forward_npu(self,positions,query,key,offsets)
  RotaryEmbedding.forward_cpu(self,positions,query,key,offsets)
  RotaryEmbedding.forward_cuda(self,positions,query,key,offsets,fused_set_kv_buffer_arg)
  RotaryEmbedding.extra_repr(self)
  LinearScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factors,dtype)
  LinearScalingRotaryEmbedding._compute_cos_sin_cache(self)
  LinearScalingRotaryEmbedding.scaling_factor_to_offset(self)
  DynamicNTKScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype)
  DynamicNTKScalingRotaryEmbedding._compute_cos_sin_cache(self)
  _yarn_find_correction_dim(num_rotations,dim,base,max_position_embeddings)
  _yarn_find_correction_range(low_rot,high_rot,dim,base,max_position_embeddings)
  _yarn_linear_ramp_mask(low,high,dim,dtype,device)
  _yarn_get_mscale(scale)
  YaRNScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype,extrapolation_factor,attn_factor,beta_fast,beta_slow)
  YaRNScalingRotaryEmbedding._compute_inv_freq(self,scaling_factor)
  YaRNScalingRotaryEmbedding._compute_cos_sin_cache(self)
  Phi3LongRoPEScaledRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,original_max_position_embeddings,base,is_neox_style,dtype,short_factor,long_factor,short_mscale,long_mscale)
  Phi3LongRoPEScaledRotaryEmbedding._compute_inv_freq(self,rescale_factors)
  Phi3LongRoPEScaledRotaryEmbedding._compute_cos_sin_cache(self,max_position_embeddings,rescale_factors,mscale)
  Phi3LongRoPEScaledRotaryEmbedding.forward(self,positions,query,key,offsets)
  yarn_get_mscale(scale,mscale)
  DeepseekScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype,extrapolation_factor,attn_factor,beta_fast,beta_slow,mscale,mscale_all_dim,device)
  DeepseekScalingRotaryEmbedding._compute_inv_freq(self,scaling_factor)
  DeepseekScalingRotaryEmbedding._compute_cos_sin_cache(self)
  DeepseekScalingRotaryEmbedding.forward_native(self,positions,query,key,offsets)
  DeepseekScalingRotaryEmbedding.forward_npu(self,positions,query,key,offsets)
  DeepseekScalingRotaryEmbedding.forward_cpu(self,positions,query,key,offsets)
  Llama3RotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,scaling_factor,low_freq_factor,high_freq_factor,orig_max_position)
  Llama3RotaryEmbedding._compute_inv_freq(self,base)
  Llama4VisionRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype)
  Llama4VisionRotaryEmbedding._compute_inv_freq(self,base)
  Llama4VisionRotaryEmbedding._compute_cos_sin_cache(self)
  Llama4VisionRotaryEmbedding.forward(self,query,key)
  DynamicNTKAlphaRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_alpha,dtype)
  DynamicNTKAlphaRotaryEmbedding._compute_cos_sin_cache(self)
  MRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,mrope_section)
  MRotaryEmbedding.forward(self,positions,query,key)
  MRotaryEmbedding.get_rope_index(spatial_merge_size,image_token_id,video_token_id,vision_start_token_id,model_type,tokens_per_second,input_ids,image_grid_thw,video_grid_thw,second_per_grid_ts,**kwargs)
  MRotaryEmbedding.get_rope_index_glm4v(input_ids,hf_config,image_grid_thw,video_grid_thw,attention_mask,**kwargs)
  MRotaryEmbedding.get_next_input_positions(mrope_position_delta,context_len,seq_len)
  DualChunkRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,chunk_size,local_size)
  DualChunkRotaryEmbedding._compute_inv_freq(self,base)
  DualChunkRotaryEmbedding._compute_cos_sin_cache(self)
  DualChunkRotaryEmbedding.forward(self,positions,query,key,offsets)
  DualChunkRotaryEmbedding._apply_rotary_embedding(self,cos_sin,hidden_rot,hidden_pass)
  DualChunkRotaryEmbedding.extra_repr(self)
  get_rope(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,dual_chunk_attention_config)
  rotate_half(x)
  apply_rotary_pos_emb_native(q,k,cos,sin,unsqueeze_dim)
  apply_rotary_pos_emb_npu(q,k,cos,sin,unsqueeze_dim)
  get_rope_cpu(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,device)
  get_rope_wrapper(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,device)
layers/sampler.py:
  Sampler.__init__(self)
  Sampler.forward(self,logits_output,sampling_info,return_logprob,top_logprobs_nums,token_ids_logprobs)
  top_k_top_p_min_p_sampling_from_probs_torch(probs,top_ks,top_ps,min_ps,need_min_p_sampling)
  sampling_from_probs_torch(probs)
  top_p_normalize_probs_torch(probs,top_ps)
  get_top_logprobs(logprobs,top_logprobs_nums)
  get_token_ids_logprobs(logprobs,token_ids_logprobs)
  apply_custom_logit_processor(logits,sampling_batch_info,num_tokens_in_batch)
layers/torchao_utils.py:
  get_gemlite_cache_path()
  save_gemlite_cache(print_error)
  proj_filter(module,fqn)
  apply_torchao_config_to_model(model,torchao_config,filter_fn)
layers/utils.py:
  get_layer_id(weight_name)
  PPMissingLayer.__init__(self,*args,**kwargs)
  PPMissingLayer.forward(self,*args,**kwargs)
layers/vocab_parallel_embedding.py:
  pad_vocab_size(vocab_size,pad_to)
  vocab_range_from_per_partition_vocab_size(per_partition_vocab_size,rank,offset)
  vocab_range_from_global_vocab_size(global_vocab_size,rank,world_size,offset)
  VocabParallelEmbeddingShardIndices.num_org_elements(self)
  VocabParallelEmbeddingShardIndices.num_added_elements(self)
  VocabParallelEmbeddingShardIndices.num_org_elements_padded(self)
  VocabParallelEmbeddingShardIndices.num_added_elements_padded(self)
  VocabParallelEmbeddingShardIndices.num_org_vocab_padding(self)
  VocabParallelEmbeddingShardIndices.num_added_vocab_padding(self)
  VocabParallelEmbeddingShardIndices.num_elements_padded(self)
  VocabParallelEmbeddingShardIndices.__post_init__(self)
  get_masked_input_and_mask(input_,org_vocab_start_index,org_vocab_end_index,num_org_vocab_padding,added_vocab_start_index,added_vocab_end_index)
  VocabParallelEmbedding.__init__(self,num_embeddings,embedding_dim,params_dtype,org_num_embeddings,padding_size,quant_config,prefix,enable_tp,use_attn_tp_group,use_presharded_weights)
  VocabParallelEmbedding._get_indices(cls,vocab_size_padded,org_vocab_size_padded,vocab_size,org_vocab_size,tp_rank,tp_size)
  VocabParallelEmbedding.get_sharded_to_full_mapping(self)
  VocabParallelEmbedding.weight_loader(self,param,loaded_weight)
  VocabParallelEmbedding.forward(self,input_)
  VocabParallelEmbedding.extra_repr(self)
  ParallelLMHead.__init__(self,num_embeddings,embedding_dim,bias,params_dtype,org_num_embeddings,padding_size,quant_config,prefix,use_attn_tp_group,use_presharded_weights)
  ParallelLMHead.tie_weights(self,embed_tokens)
  ParallelLMHead.forward(self,input_)
managers/cache_controller.py:
  LayerDoneCounter.__init__(self,num_layers)
  LayerDoneCounter.next_producer(self)
  LayerDoneCounter.update_producer(self)
  LayerDoneCounter.set_consumer(self,index)
  LayerDoneCounter.increment(self)
  LayerDoneCounter.wait_until(self,threshold)
  LayerDoneCounter.reset(self)
  CacheOperation.__init__(self,host_indices,device_indices,node_id,priority)
  CacheOperation.merge(self,other)
  CacheOperation.split(self,factor)
  CacheOperation.__lt__(self,other)
  TransferBuffer.__init__(self,stop_event,buffer_count,max_buffer_size)
  TransferBuffer.full(self)
  TransferBuffer.empty(self)
  TransferBuffer.put(self,item,block,timeout)
  TransferBuffer.get(self,block,timeout)
  TransferBuffer.clear(self)
  StorageOperation.__init__(self,host_indices,token_ids,last_hash,hash_value)
  StorageOperation.__lt__(self,other)
  PrefetchOperation.__init__(self,request_id,host_indices,token_ids,last_hash)
  PrefetchOperation.increment(self,num_tokens)
  PrefetchOperation.mark_done(self)
  PrefetchOperation.is_done(self)
  HiCacheController.__init__(self,token_to_kv_pool_allocator,mem_pool_host,page_size,tp_group,load_cache_event,write_policy,io_backend,storage_backend,prefetch_threshold,model_name,storage_backend_extra_config)
  HiCacheController._generate_storage_config(self,model_name,storage_backend_extra_config)
  HiCacheController.reset(self)
  HiCacheController.write(self,device_indices,priority,node_id)
  HiCacheController.load(self,host_indices,priority,node_id)
  HiCacheController.move_indices(self,host_indices,device_indices)
  HiCacheController.write_thread_func_direct(self)
  HiCacheController.load_thread_func_layer_by_layer(self)
  HiCacheController.evict_device(self,device_indices,host_indices)
  HiCacheController.evict_host(self,host_indices,backup_only)
  HiCacheController.prefetch(self,request_id,host_indices,new_input_tokens,last_hash)
  HiCacheController.terminate_prefetch(self,operation)
  HiCacheController._3fs_zero_copy_page_get(self,operation,hash_values,host_indices)
  HiCacheController._mooncake_page_get(self,operation,hash_values,host_indices)
  HiCacheController._generic_page_get(self,operation,hash_values,host_indices)
  HiCacheController._page_transfer(self,operation)
  HiCacheController.is_mooncake_backend(self)
  HiCacheController.prefetch_io_aux_func(self)
  HiCacheController.prefetch_rate_limit_check(self)
  HiCacheController._generic_storage_hit_query(self,operation)
  HiCacheController.prefetch_thread_func(self)
  HiCacheController.write_storage(self,host_indices,token_ids,hash_value)
  HiCacheController._generic_page_set(self,hash_values,host_indices)
  HiCacheController._mooncake_page_set(self,hash_values,host_indices)
  HiCacheController._3fs_zero_copy_page_set(self,hash_values,host_indices)
  HiCacheController._page_backup(self,operation)
  HiCacheController.backup_thread_func(self)
managers/data_parallel_controller.py:
  LoadBalanceMethod.from_str(cls,method)
  DataParallelController.__init__(self,server_args,port_args,dp_balance_meta)
  DataParallelController.launch_dp_schedulers(self,server_args,port_args)
  DataParallelController.launch_tensor_parallel_group_thread(self,server_args,port_args,base_gpu_id,dp_rank,ready_event)
  DataParallelController.launch_dp_attention_schedulers(self,server_args,port_args)
  DataParallelController.launch_tensor_parallel_group(self,server_args,port_args,base_gpu_id,dp_rank)
  DataParallelController.round_robin_scheduler(self,req)
  DataParallelController.shortest_queue_scheduler(self,input_requests)
  DataParallelController.minimum_tokens_scheduler(self,req)
  DataParallelController.get_next_global_balance_id()
  DataParallelController.event_loop(self)
  run_data_parallel_controller_process(server_args,port_args,pipe_writer)
managers/detokenizer_manager.py:
  DetokenizerManager.__init__(self,server_args,port_args)
  DetokenizerManager.event_loop(self)
  DetokenizerManager.trim_matched_stop(self,output,finished_reason,no_stop_trim)
  DetokenizerManager.handle_batch_embedding_out(self,recv_obj)
  DetokenizerManager.handle_batch_token_id_out(self,recv_obj)
  DetokenizerManager.handle_multimodal_decode_req(self,recv_obj)
  DetokenizerManager.handle_freeze_gc_req(self,recv_req)
  LimitedCapacityDict.__init__(self,capacity,*args,**kwargs)
  LimitedCapacityDict.__setitem__(self,key,value)
  run_detokenizer_process(server_args,port_args)
managers/io_struct.py:
  GenerateReqInput.contains_mm_input(self)
  GenerateReqInput.normalize_batch_and_arguments(self)
  GenerateReqInput._validate_inputs(self)
  GenerateReqInput._determine_batch_size(self)
  GenerateReqInput._handle_parallel_sampling(self)
  GenerateReqInput._normalize_single_inputs(self)
  GenerateReqInput._normalize_batch_inputs(self)
  GenerateReqInput._expand_inputs(self,num)
  GenerateReqInput._normalize_lora_paths(self,num)
  GenerateReqInput._normalize_image_data(self,num)
  GenerateReqInput._normalize_video_data(self,num)
  GenerateReqInput._normalize_audio_data(self,num)
  GenerateReqInput._normalize_sampling_params(self,num)
  GenerateReqInput._normalize_rid(self,num)
  GenerateReqInput._normalize_logprob_params(self,num)
  GenerateReqInput.normalize_param(param,default_value,param_name)
  GenerateReqInput._normalize_custom_logit_processor(self,num)
  GenerateReqInput._validate_session_params(self)
  GenerateReqInput.regenerate_rid(self)
  GenerateReqInput.__getitem__(self,i)
  BatchTokenizedGenerateReqInput.__len__(self)
  BatchTokenizedGenerateReqInput.__getitem__(self,i)
  BatchTokenizedGenerateReqInput.__iter__(self)
  EmbeddingReqInput.normalize_batch_and_arguments(self)
  EmbeddingReqInput.regenerate_rid(self)
  EmbeddingReqInput.contains_mm_input(self)
  EmbeddingReqInput.__getitem__(self,i)
  BatchTokenizedEmbeddingReqInput.__len__(self)
  BatchTokenizedEmbeddingReqInput.__getitem__(self,i)
  BatchTokenizedEmbeddingReqInput.__iter__(self)
  LoadLoRAAdapterReqInput.to_ref(self)
  UnloadLoRAAdapterReqInput.to_ref(self)
managers/mm_utils.py:
  TransportProxyTensor.__new__(cls,data,name,fields,transport_mode,*args,**kwargs)
  TransportProxyTensor.__getstate__(self)
  TransportProxyTensor.__setstate__(self,state)
  TransportProxyTensor.name(self)
  TransportProxyTensor.fields(self)
  TransportProxyTensor.transport_mode(self)
  MultiModalityDataPaddingPattern.pad_input_tokens(self,input_ids,mm_inputs)
  MultiModalityDataPaddingPatternTokenPairs.__init__(self,data_token_pairs,data_start_token_ids)
  MultiModalityDataPaddingPatternTokenPairs.pad_input_tokens(self,input_ids,mm_inputs)
  MultiModalityDataPaddingPatternMultimodalTokens.pad_input_tokens(self,input_ids,mm_inputs)
  init_embedding_cache(max_size)
  get_embedding_hash(embedding_items)
  get_embedding_chunk(embedding,extend_prefix_len,extend_seq_len,items_offset)
  _get_precomputed_embedding(items)
  _get_chunked_prefill_embedding(data_embedding_func,embedding_items,items_size,prefix_length,extend_length,items_offset_list)
  _get_multimodal_mask(input_ids,placeholder_tensor)
  _adjust_embedding_length(embedding,mask,logger)
  get_embedding_and_mask(data_embedding_func,embedding_items,placeholder_tensor,input_ids,items_size,prefix_length,extend_length,items_offset_list)
  embed_mm_inputs(mm_inputs_list,extend_prefix_lens,extend_seq_lens,input_ids,input_embedding,multimodal_model,data_embedding_func_mapping,placeholder_tokens)
  general_mm_embed_routine(input_ids,forward_batch,language_model,multimodal_model,data_embedding_funcs,placeholder_tokens,**kwargs)
  get_multimodal_data_bounds(input_ids,pad_values,token_pairs)
  data_hash(data)
  tensor_hash(tensor_list)
  hash_feature(f)
managers/multimodal_processor.py:
  import_processors()
  get_mm_processor(hf_config,server_args,processor,transport_mode)
managers/schedule_batch.py:
  BaseFinishReason.__init__(self,is_error)
  BaseFinishReason.to_json(self)
  FINISH_MATCHED_TOKEN.__init__(self,matched)
  FINISH_MATCHED_TOKEN.to_json(self)
  FINISH_MATCHED_STR.__init__(self,matched)
  FINISH_MATCHED_STR.to_json(self)
  FINISH_LENGTH.__init__(self,length)
  FINISH_LENGTH.to_json(self)
  FINISH_ABORT.__init__(self,message,status_code,err_type)
  FINISH_ABORT.to_json(self)
  Modality.from_str(modality_str)
  Modality.all()
  MultimodalDataItem.__getattr__(self,name)
  MultimodalDataItem.__setitem__(self,key,value)
  MultimodalDataItem.set(self,key,value)
  MultimodalDataItem.is_empty_list(l)
  MultimodalDataItem.set_pad_value(self)
  MultimodalDataItem.is_modality(self,modality)
  MultimodalDataItem.is_audio(self)
  MultimodalDataItem.is_image(self)
  MultimodalDataItem.is_video(self)
  MultimodalDataItem.is_valid(self)
  MultimodalDataItem.validate(self)
  MultimodalDataItem.from_dict(obj)
  MultimodalDataItem.merge(self,other)
  MultimodalInputs.from_dict(obj)
  MultimodalInputs.contains_image_inputs(self)
  MultimodalInputs.contains_video_inputs(self)
  MultimodalInputs.contains_audio_inputs(self)
  MultimodalInputs.contains_mm_input(self)
  MultimodalInputs.merge(self,other)
  Req.__init__(self,rid,origin_input_text,origin_input_ids,sampling_params,return_logprob,top_logprobs_num,token_ids_logprob,stream,origin_input_ids_unpadded,lora_id,input_embeds,token_type_ids,session_id,custom_logit_processor,return_hidden_states,eos_token_ids,bootstrap_host,bootstrap_port,bootstrap_room,data_parallel_rank,vocab_size)
  Req.seqlen(self)
  Req.extend_image_inputs(self,image_inputs)
  Req.finished(self)
  Req.init_next_round_input(self,tree_cache)
  Req.adjust_max_prefix_ids(self)
  Req.init_incremental_detokenize(self)
  Req.check_finished(self)
  Req.reset_for_retract(self)
  Req.offload_kv_cache(self,req_to_token_pool,token_to_kv_pool_allocator)
  Req.load_kv_cache(self,req_to_token_pool,token_to_kv_pool_allocator)
  Req.log_time_stats(self)
  Req.set_finish_with_abort(self,error_msg)
  Req.__repr__(self)
  ScheduleBatch.init_new(cls,reqs,req_to_token_pool,token_to_kv_pool_allocator,tree_cache,model_config,enable_overlap,spec_algorithm,chunked_req)
  ScheduleBatch.batch_size(self)
  ScheduleBatch.is_empty(self)
  ScheduleBatch.alloc_req_slots(self,num_reqs)
  ScheduleBatch.alloc_token_slots(self,num_tokens,backup_state)
  ScheduleBatch.alloc_paged_token_slots_extend(self,prefix_lens,seq_lens,last_loc,extend_num_tokens,backup_state)
  ScheduleBatch.alloc_paged_token_slots_decode(self,seq_lens,last_loc,backup_state)
  ScheduleBatch.prepare_encoder_info_extend(self,input_ids,seq_lens)
  ScheduleBatch.prepare_for_extend(self)
  ScheduleBatch.prepare_for_split_prefill(self)
  ScheduleBatch.mix_with_running(self,running_batch)
  ScheduleBatch.new_page_count_next_decode(self)
  ScheduleBatch.check_decode_mem(self,buf_multiplier)
  ScheduleBatch.retract_decode(self,server_args)
  ScheduleBatch.get_required_tokens(num_reqs)
  ScheduleBatch._get_available_size()
  ScheduleBatch.prepare_encoder_info_decode(self)
  ScheduleBatch.prepare_for_idle(self)
  ScheduleBatch.prepare_for_decode(self)
  ScheduleBatch.filter_batch(self,chunked_req_to_exclude,keep_indices)
  ScheduleBatch.merge_batch(self,other)
  ScheduleBatch.get_model_worker_batch(self,seq_lens_cpu_cache)
  ScheduleBatch.copy(self)
  ScheduleBatch._evict_tree_cache_if_needed(self,num_tokens)
  ScheduleBatch._is_available_size_sufficient(self,num_tokens)
  ScheduleBatch._available_and_evictable_str(self)
  ScheduleBatch.__str__(self)
  write_req_to_token_pool_triton(req_to_token_ptr,req_pool_indices,pre_lens,seq_lens,extend_lens,out_cache_loc,req_to_token_ptr_stride)
  get_last_loc(req_to_token,req_pool_indices_tensor,prefix_lens_tensor)
  get_last_loc_torch(req_to_token,req_pool_indices_tensor,prefix_lens_tensor)
  get_last_loc_kernel(req_to_token,req_pool_indices_tensor,prefix_lens_tensor,result,num_tokens,req_to_token_stride,BLOCK_SIZE)
  get_last_loc_triton(req_to_token,req_pool_indices_tensor,prefix_lens_tensor)
managers/schedule_policy.py:
  SchedulePolicy.__init__(self,policy,tree_cache,enable_hierarchical_cache)
  SchedulePolicy.calc_priority(self,waiting_queue)
  SchedulePolicy._determine_active_policy(self,waiting_queue)
  SchedulePolicy._validate_and_adjust_policy(self,policy,tree_cache)
  SchedulePolicy._compute_prefix_matches(self,waiting_queue,policy)
  SchedulePolicy._sort_by_longest_prefix(waiting_queue,temporary_deprioritized)
  SchedulePolicy._sort_by_dfs_weight(waiting_queue,tree_cache)
  SchedulePolicy._sort_by_longest_output(waiting_queue)
  SchedulePolicy._sort_randomly(waiting_queue)
  SchedulePolicy._calc_weight(cur_node,node_to_weight)
  SchedulePolicy._get_dfs_priority(cur_node,node_to_priority,last_node_to_reqs,q)
  PrefillAdder.__init__(self,page_size,tree_cache,token_to_kv_pool_allocator,running_batch,new_token_ratio,rem_input_tokens,rem_chunk_tokens,mixed_with_decode_tokens)
  PrefillAdder.rem_total_tokens(self)
  PrefillAdder.cur_rem_tokens(self)
  PrefillAdder.ceil_paged_tokens(self,tokens)
  PrefillAdder.budget_state(self)
  PrefillAdder._update_prefill_budget(self,prefix_len,extend_input_len,max_new_tokens)
  PrefillAdder.add_chunked_req(self,req)
  PrefillAdder._lock_node(self,last_node)
  PrefillAdder.add_one_req_ignore_eos(self,req,has_chunked_req)
  PrefillAdder.add_req_state(r,insert_sort)
  PrefillAdder.add_one_req(self,req,has_chunked_req)
managers/scheduler.py:
  Scheduler.__init__(self,server_args,port_args,gpu_id,tp_rank,moe_ep_rank,pp_rank,dp_rank,dp_balance_meta)
  Scheduler.init_tokenizer(self)
  Scheduler.init_memory_pool_and_cache(self)
  Scheduler.init_disaggregation(self)
  Scheduler.init_moe_config(self)
  Scheduler.event_loop_normal(self)
  Scheduler.event_loop_overlap(self)
  Scheduler.event_loop_pp(self)
  Scheduler.recv_requests(self)
  Scheduler.process_input_requests(self,recv_reqs)
  Scheduler.handle_generate_request(self,recv_req)
  Scheduler.handle_batch_generate_request(self,recv_req)
  Scheduler._add_request_to_queue(self,req)
  Scheduler._prefetch_kvcache(self,req)
  Scheduler._extend_requests_to_queue(self,reqs,is_retracted)
  Scheduler.handle_embedding_request(self,recv_req)
  Scheduler.handle_batch_embedding_request(self,recv_req)
  Scheduler.self_check_during_idle(self)
  Scheduler.check_memory(self)
  Scheduler.check_tree_cache(self)
  Scheduler._get_token_info(self)
  Scheduler._get_swa_token_info(self)
  Scheduler.get_next_batch_to_run(self)
  Scheduler.get_num_allocatable_reqs(self,running_bs)
  Scheduler.get_new_batch_prefill(self)
  Scheduler.update_running_batch(self,batch)
  Scheduler.run_batch(self,batch)
  Scheduler.process_batch_result(self,batch,result,launch_done)
  Scheduler.maybe_send_health_check_signal(self)
  Scheduler.prepare_mlp_sync_batch(self,local_batch)
  Scheduler.handle_dp_balance_data(self,local_batch)
  Scheduler.gather_dp_balance_info(holding_tokens_list)
  Scheduler.write_shared_dp_balance_info(new_recv_rid_lists,local_tokens)
  Scheduler.prepare_mlp_sync_batch_raw(local_batch,dp_size,attn_tp_size,tp_group,get_idle_batch,disable_cuda_graph,spec_algorithm,speculative_num_draft_tokens,require_mlp_tp_gather,disable_overlap_schedule)
  Scheduler.get_idle_batch(self)
  Scheduler.move_ready_grammar_requests(self)
  Scheduler.set_next_batch_sampling_info_done(self,batch)
  Scheduler.watchdog_thread(self)
  Scheduler.flush_cache_wrapped(self,recv_req)
  Scheduler.clear_hicache_storage_wrapped(self,recv_req)
  Scheduler.flush_cache(self)
  Scheduler.get_load(self)
  Scheduler.get_internal_state(self,recv_req)
  Scheduler.set_internal_state(self,recv_req)
  Scheduler.handle_rpc_request(self,recv_req)
  Scheduler.abort_request(self,recv_req)
  Scheduler._pause_engine(self)
  Scheduler.load_lora_adapter(self,recv_req)
  Scheduler.unload_lora_adapter(self,recv_req)
  Scheduler.slow_down(self,recv_req)
  Scheduler.expert_distribution_handle(self,recv_req)
  Scheduler.open_session(self,recv_req)
  Scheduler.close_session(self,recv_req)
  Scheduler.get_print_prefix(self)
  Scheduler.current_scheduler_metrics_enabled(self)
  Scheduler.maybe_sleep_on_idle(self)
  Scheduler.handle_freeze_gc(self,recv_req)
  IdleSleeper.__init__(self,sockets)
  IdleSleeper.maybe_sleep(self)
  is_health_check_generate_req(recv_req)
  is_work_request(recv_req)
  run_scheduler_process(server_args,port_args,gpu_id,tp_rank,moe_ep_rank,pp_rank,dp_rank,pipe_writer,balance_meta)
managers/scheduler_input_blocker.py:
  SchedulerInputBlocker.__init__(self,noop)
  SchedulerInputBlocker.handle(self,recv_reqs)
  SchedulerInputBlocker._handle_recv_req(self,recv_req)
  SchedulerInputBlocker._execute_block_req(self)
  SchedulerInputBlocker._execute_unblock_req(self)
  SchedulerInputBlocker._handle_arrive_unblock_barrier(self)
  SchedulerInputBlocker._change_state(self,original,target)
  input_blocker_guard_region(send_to_scheduler)
managers/scheduler_metrics_mixin.py:
  KvMetrics.__init__(self)
  SchedulerMetricsMixin.init_metrics(self,tp_rank,pp_rank,dp_rank)
  SchedulerMetricsMixin.init_kv_events(self,kv_events_config)
  SchedulerMetricsMixin.log_prefill_stats(self,adder,can_run_list,running_bs)
  SchedulerMetricsMixin.log_decode_stats(self,can_run_cuda_graph,running_batch)
  SchedulerMetricsMixin._emit_kv_metrics(self)
  SchedulerMetricsMixin._publish_kv_events(self)
managers/scheduler_output_processor_mixin.py:
  SchedulerOutputProcessorMixin.process_batch_result_prefill(self,batch,result,launch_done)
  SchedulerOutputProcessorMixin.process_batch_result_decode(self,batch,result,launch_done)
  SchedulerOutputProcessorMixin.add_input_logprob_return_values(self,i,req,output,logprob_pt,num_input_logprobs,last_prefill_chunk)
  SchedulerOutputProcessorMixin.add_logprob_return_values(self,i,req,pt,next_token_ids,num_input_logprobs,output)
  SchedulerOutputProcessorMixin.stream_output(self,reqs,return_logprob,skip_req)
  SchedulerOutputProcessorMixin.stream_output_generation(self,reqs,return_logprob,skip_req)
  SchedulerOutputProcessorMixin.stream_output_embedding(self,reqs)
managers/scheduler_profiler_mixin.py:
  SchedulerProfilerMixin.init_profier(self)
  SchedulerProfilerMixin.init_profile(self,output_dir,start_step,num_steps,activities,with_stack,record_shapes,profile_by_stage,profile_id)
  SchedulerProfilerMixin.start_profile(self,stage)
  SchedulerProfilerMixin.stop_profile(self,stage)
  SchedulerProfilerMixin._profile_batch_predicate(self,batch)
  SchedulerProfilerMixin.profile(self,recv_req)
managers/scheduler_recv_skipper.py:
  SchedulerRecvSkipper.maybe_create(server_args)
  SchedulerRecvSkipper.__init__(self,server_args)
  SchedulerRecvSkipper.handle(self,last_forward_mode)
managers/scheduler_update_weights_mixin.py:
  SchedulerUpdateWeightsMixin.update_weights_from_disk(self,recv_req)
  SchedulerUpdateWeightsMixin.init_weights_update_group(self,recv_req)
  SchedulerUpdateWeightsMixin.update_weights_from_distributed(self,recv_req)
  SchedulerUpdateWeightsMixin.update_weights_from_tensor(self,recv_req)
  SchedulerUpdateWeightsMixin.get_weights_by_name(self,recv_req)
  SchedulerUpdateWeightsMixin.release_memory_occupation(self,recv_req)
  SchedulerUpdateWeightsMixin.resume_memory_occupation(self,recv_req)
  SchedulerUpdateWeightsMixin.save_remote_model(self,params)
  SchedulerUpdateWeightsMixin.save_sharded_model(self,params)
  _export_static_state(model)
  _import_static_state(model,static_params)
managers/session_controller.py:
  SessionReqNode.__init__(self,req,parent,childs)
  SessionReqNode.clear_childs(self,req_dict)
  SessionReqNode.clear(self,req_dict)
  SessionReqNode.abort(self)
  SessionReqNode.__str__(self)
  SessionReqNode._str_helper(self,prefix)
  Session.__init__(self,capacity_of_str_len,session_id)
  Session.create_req(self,req,tokenizer)
managers/template_manager.py:
  TemplateManager.__init__(self)
  TemplateManager.chat_template_name(self)
  TemplateManager.completion_template_name(self)
  TemplateManager.jinja_template_content_format(self)
  TemplateManager.force_reasoning(self)
  TemplateManager._detect_reasoning_pattern(self,template)
  TemplateManager.load_chat_template(self,tokenizer_manager,chat_template_arg,model_path)
  TemplateManager._load_explicit_chat_template(self,tokenizer_manager,chat_template_arg)
  TemplateManager.guess_chat_template_from_model_path(self,model_path)
  TemplateManager.load_completion_template(self,completion_template_arg)
  TemplateManager.initialize_templates(self,tokenizer_manager,model_path,chat_template,completion_template)
  TemplateManager._load_jinja_template(self,tokenizer_manager,template_path)
  TemplateManager._load_json_chat_template(self,template_path)
  TemplateManager._load_json_completion_template(self,template_path)
  TemplateManager._resolve_hf_chat_template(self,tokenizer_manager)
managers/tokenizer_manager.py:
  TokenizerManager.__init__(self,server_args,port_args)
  async TokenizerManager.generate_request(self,obj,request)
  async TokenizerManager._tokenize_one_request(self,obj)
  TokenizerManager._validate_one_request(self,obj,input_ids)
  TokenizerManager._validate_input_ids_in_vocab(self,input_ids,vocab_size)
  TokenizerManager._create_tokenized_object(self,obj,input_text,input_ids,input_embeds,mm_inputs,token_type_ids)
  async TokenizerManager._batch_tokenize_and_process(self,batch_size,obj)
  TokenizerManager._validate_batch_tokenization_constraints(self,batch_size,obj)
  TokenizerManager._send_one_request(self,obj,tokenized_obj,created_time)
  TokenizerManager._send_batch_request(self,obj,tokenized_objs,created_time)
  async TokenizerManager._wait_one_response(self,obj,state,request)
  async TokenizerManager._handle_batch_request(self,obj,request,created_time)
  async TokenizerManager.flush_cache(self)
  async TokenizerManager.clear_hicache_storage(self)
  TokenizerManager.abort_request(self,rid,abort_all)
  async TokenizerManager.start_profile(self,output_dir,start_step,num_steps,activities,with_stack,record_shapes,profile_by_stage)
  async TokenizerManager.stop_profile(self)
  async TokenizerManager._execute_profile(self,req)
  async TokenizerManager.start_expert_distribution_record(self)
  async TokenizerManager.stop_expert_distribution_record(self)
  async TokenizerManager.dump_expert_distribution_record(self)
  async TokenizerManager.pause_generation(self)
  async TokenizerManager.continue_generation(self)
  async TokenizerManager.update_weights_from_disk(self,obj,request)
  async TokenizerManager._wait_for_model_update_from_disk(self,obj)
  async TokenizerManager.init_weights_update_group(self,obj,request)
  async TokenizerManager.update_weights_from_distributed(self,obj,request)
  async TokenizerManager.update_weights_from_tensor(self,obj,request)
  async TokenizerManager.load_lora_adapter(self,obj,_)
  async TokenizerManager.unload_lora_adapter(self,obj,_)
  async TokenizerManager.get_weights_by_name(self,obj,request)
  async TokenizerManager.release_memory_occupation(self,obj,request)
  async TokenizerManager.resume_memory_occupation(self,obj,request)
  async TokenizerManager.slow_down(self,obj,request)
  async TokenizerManager.open_session(self,obj,request)
  async TokenizerManager.close_session(self,obj,request)
  async TokenizerManager.get_internal_state(self)
  async TokenizerManager.set_internal_state(self,obj)
  async TokenizerManager.get_load(self)
  TokenizerManager.get_log_request_metadata(self)
  TokenizerManager.configure_logging(self,obj)
  async TokenizerManager.freeze_gc(self)
  TokenizerManager.create_abort_task(self,obj)
  async TokenizerManager.abort_request()
  TokenizerManager.auto_create_handle_loop(self)
  TokenizerManager.dump_requests_before_crash(self)
  TokenizerManager._upload_file_to_gcs(bucket_name,source_file_path,object_name)
  async TokenizerManager.sigterm_watchdog(self)
  async TokenizerManager.handle_loop(self)
  TokenizerManager._handle_batch_output(self,recv_obj)
  TokenizerManager.convert_logprob_style(self,meta_info,state,top_logprobs_num,token_ids_logprob,return_text_in_logprobs,recv_obj,recv_obj_index)
  TokenizerManager.detokenize_logprob_tokens(self,token_logprobs_val,token_logprobs_idx,decode_to_text)
  TokenizerManager.detokenize_top_logprobs_tokens(self,token_logprobs_val,token_logprobs_idx,decode_to_text)
  TokenizerManager.collect_metrics(self,state,recv_obj,i)
  TokenizerManager.dump_requests(self,state,out_dict)
  TokenizerManager.record_request_for_crash_dump(self,state,out_dict)
  TokenizerManager._dump_data_to_file(self,data_list,filename,log_message)
  TokenizerManager.background_task()
  TokenizerManager._handle_abort_req(self,recv_obj)
  TokenizerManager._handle_open_session_req_output(self,recv_obj)
  TokenizerManager._handle_update_weights_from_disk_req_output(self,recv_obj)
  async TokenizerManager.score_request(self,query,items,label_token_ids,apply_softmax,item_first,request)
  _determine_tensor_transport_mode(server_args)
  async print_exception_wrapper(func)
  SignalHandler.__init__(self,tokenizer_manager)
  SignalHandler.sigterm_handler(self,signum,frame)
  SignalHandler.running_phase_sigquit_handler(self,signum,frame)
  _Communicator.__init__(self,sender,fan_out)
  async _Communicator.__call__(self,obj)
  _Communicator.handle_recv(self,recv_obj)
managers/tp_worker.py:
  TpModelWorker.__init__(self,server_args,gpu_id,tp_rank,moe_ep_rank,pp_rank,dp_rank,nccl_port,is_draft_worker,req_to_token_pool,token_to_kv_pool_allocator)
  TpModelWorker.register_hicache_layer_transfer_counter(self,counter)
  TpModelWorker.set_hicache_consumer(self,consumer_index)
  TpModelWorker.get_worker_info(self)
  TpModelWorker.sliding_window_size(self)
  TpModelWorker.is_hybrid(self)
  TpModelWorker.get_tokens_per_layer_info(self)
  TpModelWorker.get_pad_input_ids_func(self)
  TpModelWorker.get_tp_group(self)
  TpModelWorker.get_attention_tp_group(self)
  TpModelWorker.get_attention_tp_cpu_group(self)
  TpModelWorker.get_memory_pool(self)
  TpModelWorker.forward_batch_generation(self,model_worker_batch,launch_done,skip_sample)
  TpModelWorker.forward_batch_embedding(self,model_worker_batch)
  TpModelWorker.update_weights_from_disk(self,recv_req)
  TpModelWorker.init_weights_update_group(self,recv_req)
  TpModelWorker.update_weights_from_distributed(self,recv_req)
  TpModelWorker.update_weights_from_tensor(self,recv_req)
  TpModelWorker.get_weights_by_name(self,recv_req)
  TpModelWorker.load_lora_adapter(self,recv_req)
  TpModelWorker.unload_lora_adapter(self,recv_req)
  TpModelWorker.can_run_lora_batch(self,lora_ids)
managers/tp_worker_overlap_thread.py:
  resolve_future_token_ids(input_ids,future_token_ids_map)
  TpModelWorkerClient.__init__(self,server_args,gpu_id,tp_rank,moe_ep_rank,pp_rank,dp_rank,nccl_port)
  TpModelWorkerClient.register_hicache_layer_transfer_counter(self,counter)
  TpModelWorkerClient.set_hicache_consumer(self,consumer_index)
  TpModelWorkerClient.get_worker_info(self)
  TpModelWorkerClient.get_tokens_per_layer_info(self)
  TpModelWorkerClient.sliding_window_size(self)
  TpModelWorkerClient.is_hybrid(self)
  TpModelWorkerClient.get_pad_input_ids_func(self)
  TpModelWorkerClient.get_tp_group(self)
  TpModelWorkerClient.get_attention_tp_group(self)
  TpModelWorkerClient.get_attention_tp_cpu_group(self)
  TpModelWorkerClient.get_memory_pool(self)
  TpModelWorkerClient.get_kv_cache(self)
  TpModelWorkerClient.forward_thread_func(self)
  TpModelWorkerClient.forward_thread_func_(self)
  TpModelWorkerClient.resolve_last_batch_result(self,launch_done)
  TpModelWorkerClient.forward_batch_generation(self,model_worker_batch)
  TpModelWorkerClient.update_weights_from_disk(self,recv_req)
  TpModelWorkerClient.init_weights_update_group(self,recv_req)
  TpModelWorkerClient.update_weights_from_distributed(self,recv_req)
  TpModelWorkerClient.update_weights_from_tensor(self,recv_req)
  TpModelWorkerClient.get_weights_by_name(self,recv_req)
  TpModelWorkerClient.load_lora_adapter(self,recv_req)
  TpModelWorkerClient.unload_lora_adapter(self,recv_req)
  TpModelWorkerClient.can_run_lora_batch(self,lora_ids)
  TpModelWorkerClient.__delete__(self)
managers/utils.py:
  validate_input_length(req,max_req_input_len,allow_auto_truncate)
  get_logprob_dict_from_result(result)
  get_logprob_from_pp_outputs(next_pp_outputs)
  DPBalanceMeta.__init__(self,num_workers)
  DPBalanceMeta.destructor(self)
  DPBalanceMeta.get_shared_onfly(self)
  DPBalanceMeta.set_shared_onfly_info(self,data)
  DPBalanceMeta.get_shared_local_tokens(self)
  DPBalanceMeta.set_shared_local_tokens(self,data)
  DPBalanceMeta.__getstate__(self)
  DPBalanceMeta.__setstate__(self,state)
model_executor/cuda_graph_runner.py:
  get_is_capture_mode()
  model_capture_mode()
  freeze_gc(enable_cudagraph_gc)
  _to_torch(model,reverse,num_tokens)
  patch_model(model,enable_compile,num_tokens,tp_group)
  set_torch_compile_config()
  get_batch_sizes_to_capture(model_runner)
  get_global_graph_memory_pool()
  set_global_graph_memory_pool(val)
  CudaGraphRunner.__init__(self,model_runner)
  CudaGraphRunner._cache_loc_dtype(self)
  CudaGraphRunner.can_run(self,forward_batch)
  CudaGraphRunner.capture(self)
  CudaGraphRunner._capture_graph(self,graph,pool,stream,run_once_fn)
  CudaGraphRunner._create_device_graph(self)
  CudaGraphRunner.capture_one_batch_size(self,bs,forward)
  CudaGraphRunner.run_once()
  CudaGraphRunner.recapture_if_needed(self,forward_batch)
  CudaGraphRunner.replay_prepare(self,forward_batch,pp_proxy_tensors)
  CudaGraphRunner.replay(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors)
  CudaGraphRunner.get_spec_info(self,num_tokens)
model_executor/forward_batch_info.py:
  ForwardMode.is_prefill(self)
  ForwardMode.is_extend(self)
  ForwardMode.is_decode(self)
  ForwardMode.is_mixed(self)
  ForwardMode.is_idle(self)
  ForwardMode.is_decode_or_idle(self)
  ForwardMode.is_target_verify(self)
  ForwardMode.is_draft_extend(self)
  ForwardMode.is_extend_or_draft_extend_or_mixed(self)
  ForwardMode.is_cuda_graph(self)
  ForwardMode.is_dummy_first(self)
  ForwardMode.is_split_prefill(self)
  CaptureHiddenMode.need_capture(self)
  CaptureHiddenMode.is_full(self)
  CaptureHiddenMode.is_last(self)
  CaptureHiddenMode.__lt__(self,other)
  ForwardBatch.init_new(cls,batch,model_runner)
  ForwardBatch.merge_mm_inputs(self)
  ForwardBatch.contains_image_inputs(self)
  ForwardBatch.contains_audio_inputs(self)
  ForwardBatch.contains_video_inputs(self)
  ForwardBatch.contains_mm_inputs(self)
  ForwardBatch._compute_mrope_positions(self,model_runner,batch)
  ForwardBatch.get_max_chunk_capacity(self)
  ForwardBatch.set_prefix_chunk_idx(self,idx)
  ForwardBatch.set_attn_attend_prefix_cache(self,attn_attend_prefix_cache)
  ForwardBatch.prepare_chunked_kv_indices(self,device)
  ForwardBatch._pad_tensor_to_size(self,tensor,size,value)
  ForwardBatch.prepare_mlp_sync_batch(self,model_runner)
  ForwardBatch.post_forward_mlp_sync_batch(self,logits_output)
  ForwardBatch.get_prefix_chunk_seq_lens(self,prefix_lens,num_prefix_chunks,prefix_chunk_len)
  ForwardBatch.prepare_chunked_prefix_cache_info(self,device)
  ForwardBatch.can_run_tbo(self)
  enable_num_token_non_padded(server_args)
  PPProxyTensors.__init__(self,tensors)
  PPProxyTensors.__getitem__(self,key)
  PPProxyTensors.__setitem__(self,key,value)
  PPProxyTensors.__len__(self)
  PPProxyTensors.__eq__(self,other)
  PPProxyTensors.__repr__(self)
  compute_position(attn_backend,extend_prefix_lens,extend_seq_lens,extend_seq_lens_sum)
  compute_position_triton(extend_prefix_lens,extend_seq_lens,extend_seq_lens_sum)
  compute_position_kernel(positions,extend_start_loc,extend_prefix_lens,extend_seq_lens,has_prefix)
  compute_position_torch(extend_prefix_lens,extend_seq_lens)
  clamp_position(seq_lens)
  create_chunked_prefix_cache_kv_indices(req_to_token_ptr,req_pool_indices_ptr,chunk_start_idx_ptr,chunk_seq_lens_ptr,chunk_cu_seq_lens_ptr,chunk_kv_indices_ptr,req_to_token_ptr_stride)
model_executor/model_runner.py:
  RankZeroFilter.__init__(self,is_rank_zero)
  RankZeroFilter.filter(self,record)
  ModelRunner.__init__(self,model_config,mem_fraction_static,gpu_id,tp_rank,tp_size,moe_ep_rank,moe_ep_size,pp_rank,pp_size,nccl_port,server_args,dp_rank,is_draft_worker,req_to_token_pool,token_to_kv_pool_allocator)
  ModelRunner.initialize(self,min_per_gpu_memory)
  ModelRunner.model_specific_adjustment(self)
  ModelRunner.init_torch_distributed(self)
  ModelRunner.load_model(self)
  ModelRunner.update_expert_location(self,new_expert_location_metadata,update_layer_ids)
  ModelRunner.update_weights_from_disk(self,model_path,load_format)
  ModelRunner.get_weight_iter(config)
  ModelRunner.model_load_weights(model,iter)
  ModelRunner.init_weights_update_group(self,master_address,master_port,rank_offset,world_size,group_name,backend)
  ModelRunner.update_weights_from_distributed(self,names,dtypes,shapes,group_name)
  ModelRunner.update_weights_from_tensor(self,named_tensors,load_format)
  ModelRunner._update_weights_from_flattened_bucket(self,flattened_tensor_bucket_dict)
  ModelRunner.get_weights_by_name(self,name,truncate_size)
  ModelRunner.init_lora_manager(self)
  ModelRunner.load_lora_adapter(self,lora_ref)
  ModelRunner.unload_lora_adapter(self,lora_ref)
  ModelRunner.profile_max_num_token(self,total_gpu_memory)
  ModelRunner.set_num_token_hybrid(self)
  ModelRunner.init_memory_pool(self,total_gpu_memory,max_num_reqs,max_total_tokens)
  ModelRunner.init_cublas(self)
  ModelRunner.init_attention_backend(self)
  ModelRunner._get_attention_backend(self)
  ModelRunner._get_attention_backend_from_str(self,backend_str)
  ModelRunner.init_double_sparsity_channel_config(self,selected_channel)
  ModelRunner.init_device_graphs(self)
  ModelRunner.init_threads_binding(self)
  ModelRunner.apply_torch_tp(self)
  ModelRunner.forward_decode(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors)
  ModelRunner.forward_extend(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors)
  ModelRunner.forward_idle(self,forward_batch,pp_proxy_tensors)
  ModelRunner.forward_split_prefill(self,forward_batch,reinit_attn_backend,forward_count)
  ModelRunner.forward(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors,reinit_attn_backend,split_forward_count)
  ModelRunner._forward_raw(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors,reinit_attn_backend,split_forward_count)
  ModelRunner._preprocess_logits(self,logits_output,sampling_info)
  ModelRunner.sample(self,logits_output,forward_batch)
  ModelRunner.model_is_mrope(self)
  ModelRunner.save_remote_model(self,url)
  ModelRunner.save_sharded_model(self,path,pattern,max_size)
  _model_load_weights_direct(model,named_tensors)
  _unwrap_tensor(tensor,tp_rank,device)
  LocalSerializedTensor.get(self,rank)
model_executor/npu_graph_runner.py:
  NPUGraphRunner.__init__(self,model_runner)
  NPUGraphRunner._create_device_graph(self)
  NPUGraphRunner._capture_graph(self,graph,pool,stream,run_once_fn)
  NPUGraphRunner._update_inputs(self,seq_lens)
  NPUGraphRunner._cache_loc_dtype(self)
  NPUGraphRunner.replay(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors)
model_parallel.py:
  _shard_tensor(full_tensor,device_mesh,placements)
  ColwiseParallelSharded._partition_linear_fn(self,name,module,device_mesh)
  RowwiseParallelMaybeWait._partition_linear_fn(self,name,module,device_mesh)
  RowwiseParallelMaybeWait._prepare_output_fn(output_layouts,use_local_output,mod,outputs,device_mesh)
  tensor_parallel(module,device_mesh)
  tplize(mod)
multimodal/mm_utils.py:
  has_valid_data(data)
  select_best_resolution(original_size,possible_resolutions)
  resize_and_pad_image(image,target_resolution)
  divide_to_patches(image,patch_size)
  get_anyres_image_grid_shape(image_size,grid_pinpoints,patch_size)
  process_anyres_image(image,processor,grid_pinpoints)
  load_image_from_base64(image)
  expand2square(pil_img,background_color)
  unpad_image(tensor,original_size)
  unpad_image_shape(current_height,current_width,original_size)
  process_images(images,image_processor,model_cfg)
multimodal/processors/base_processor.py:
  BaseMultiModalProcessorOutput.organize_results(self)
  MultimodalSpecialTokens.build(self,processor)
  MultimodalSpecialTokens.convert_to_str(self,token,processor)
  MultimodalSpecialTokens.convert_to_strs(self,processor)
  MultimodalSpecialTokens.get_modality_of_token(self,token)
  MultimodalSpecialTokens.get_token_id_by_modality(self,modality)
  MultimodalSpecialTokens.parse_regex(self)
  MultimodalSpecialTokens.get_combined_regex(self)
  BaseMultimodalProcessor.__init__(self,hf_config,server_args,_processor,transport_mode,*args,**kwargs)
  BaseMultimodalProcessor.process_mm_data(self,input_text,images,videos,audios,**kwargs)
  async BaseMultimodalProcessor.process_mm_data_async(self,image_data,audio_data,input_text,request_obj,**kwargs)
  BaseMultimodalProcessor.get_estimated_frames_list(self,image_data)
  BaseMultimodalProcessor._load_single_item(data,modality,frame_count_limit,audio_sample_rate,discard_alpha_channel)
  BaseMultimodalProcessor.submit_data_loading_tasks(self,text_parts,multimodal_tokens,data_iterators,discard_alpha_channel,image_estimated_frames_iter,image_scaling_factor,max_image_frames,audio_sample_rate)
  BaseMultimodalProcessor.load_mm_data(self,prompt,multimodal_tokens,image_data,video_data,audio_data,return_text,discard_alpha_channel,audio_sample_rate)
  BaseMultimodalProcessor.get_mm_items_offset(input_ids,mm_token_id)
  BaseMultimodalProcessor.get_mm_items_offset_by_pair(input_ids,mm_start_id,mm_end_id)
  BaseMultimodalProcessor.collect_mm_items_from_processor_output(self,data_dict)
  BaseMultimodalProcessor._process_and_collect_mm_items(self,input_text,images,audios,videos,**kwargs)
  BaseMultimodalProcessor.process_and_combine_mm_data(self,base_output,mm_tokens,**kwargs)
multimodal/processors/clip.py:
  ClipImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async ClipImageProcessor.process_mm_data_async(self,image_data,input_text,*args,**kwargs)
multimodal/processors/deepseek_vl_v2.py:
  DeepseekVL2ImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async DeepseekVL2ImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,max_req_input_len,*args,**kwargs)
multimodal/processors/gemma3.py:
  Gemma3SGLangImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Gemma3SGLangImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/gemma3n.py:
  Gemma3nSGLangProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Gemma3nSGLangProcessor.process_mm_data_async(self,image_data,audio_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/glm4v.py:
  Glm4vImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Glm4vImageProcessor.preprocess_video(self,vr)
  async Glm4vImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/internvl.py:
  InternVLImageProcessor.__init__(self,hf_config,server_args,_image_processor,*args,**kwargs)
  InternVLImageProcessor.build_transform(input_size)
  InternVLImageProcessor.resize_image(img,size)
  InternVLImageProcessor.to_tensor(img)
  InternVLImageProcessor.normalize(tensor,mean,std)
  InternVLImageProcessor.transform(img)
  InternVLImageProcessor.dynamic_preprocess(image,min_num,max_num,image_size,use_thumbnail)
  InternVLImageProcessor.find_closest_aspect_ratio(aspect_ratio,target_ratios,width,height,image_size)
  InternVLImageProcessor.get_index(bound,fps,max_frame,first_idx,num_segments)
  InternVLImageProcessor.load_video(video_path,bound,input_size,max_num,num_segments)
  async InternVLImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,**kwargs)
  InternVLImageProcessor.process_image_internvl(image,input_size,max_num)
multimodal/processors/janus_pro.py:
  JanusProImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async JanusProImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,**kwargs)
multimodal/processors/kimi_vl.py:
  KimiVLImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async KimiVLImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/llava.py:
  LlavaImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  LlavaImageProcessor._process_single_image_task(image_data,image_aspect_ratio,image_grid_pinpoints,processor)
  async LlavaImageProcessor._process_single_image(self,image_data,aspect_ratio,grid_pinpoints)
  async LlavaImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
  LlavaMultimodalProcessor._get_sgl_processor_cls(self,model_type)
  LlavaMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async LlavaMultimodalProcessor.process_mm_data_async(self,*args,**kwargs)
multimodal/processors/minicpm.py:
  MiniCPMMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async MiniCPMMultimodalProcessor.process_mm_data_async(self,image_data,audio_data,input_text,request_obj,**kwargs)
multimodal/processors/mlama.py:
  MllamaImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async MllamaImageProcessor.process_mm_data_async(self,image_data,input_text,*args,**kwargs)
multimodal/processors/mllama4.py:
  Mllama4ImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Mllama4ImageProcessor.process_mm_data_async(self,image_data,input_text,*args,**kwargs)
multimodal/processors/phi4mm.py:
  Phi4MMProcessorAdapter.__init__(self,_processor)
  Phi4MMProcessorAdapter.__call__(self,**kwargs)
  Phi4MMMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Phi4MMMultimodalProcessor.process_mm_data_async(self,image_data,audio_data,input_text,request_obj,**kwargs)
multimodal/processors/pixtral.py:
  PixtralProcessor.get_patch_grid_size(self,image_width,image_height)
  PixtralProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async PixtralProcessor._resize(self,image)
  async PixtralProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/qwen_audio.py:
  Qwen2AudioMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Qwen2AudioMultimodalProcessor.process_mm_data_async(self,audio_data,input_text,**kwargs)
multimodal/processors/qwen_vl.py:
  smart_resize(height,width,factor,min_pixels,max_pixels)
  resize_image(image,size_factor)
  round_by_factor(number,factor)
  ceil_by_factor(number,factor)
  floor_by_factor(number,factor)
  async resize_image_async(image)
  smart_nframes(ele,total_frames,video_fps)
  async preprocess_video(vr,image_factor)
  Qwen2_5VLImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Qwen2_5VLImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/step3_vl.py:
  GPUToTensor.forward(self,raw_image)
  Step3VisionProcessor.__init__(self,size,interpolation_mode,patch_size)
  Step3VisionProcessor.__call__(self,image,is_patch)
  ImagePatcher.determine_window_size(self,long,short)
  ImagePatcher.slide_window(self,width,height,sizes,steps,img_rate_thr)
  ImagePatcher.square_pad(self,img)
  ImagePatcher.get_image_size_for_padding(self,img_width,img_height)
  ImagePatcher.get_image_size_for_preprocess(self,img_width,img_height)
  ImagePatcher.get_image_size_for_crop(self,img_width,img_height,window_size)
  ImagePatcher.patch_crop(self,img,i,j,th,tw)
  ImagePatcher.get_num_patches(self,img_width,img_height)
  ImagePatcher.__call__(self,img)
  Step3VLProcessor.__init__(self,config,tokenizer)
  Step3VLProcessor.image_token_id(self)
  Step3VLProcessor.get_num_image_tokens(self,img_width,img_height)
  Step3VLProcessor._split_images(self,images)
  Step3VLProcessor._convert_images_to_pixel_values(self,images,is_patch)
  Step3VLProcessor._get_patch_repl(self,num_patches,patch_newline_mask)
  Step3VLProcessor._get_image_repl(self,num_images)
  Step3VLProcessor._get_image_repl_features(self,num_images,num_patches,patch_new_line_idx)
  Step3VLProcessor.replace_placeholder(self,text,placeholder,repls)
  Step3VLProcessor.__call__(self,text,images,return_tensors,*args,**kwargs)
  Step3VLImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  Step3VLImageProcessor.preprocess(self,image)
  Step3VLImageProcessor.__call__(self,image)
  async Step3VLImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/vila.py:
  VILAMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async VILAMultimodalProcessor.process_mm_data_async(self,image_data,input_text,request_obj,**kwargs)
offloader.py:
  BaseOffloader.wrap_modules(self,all_modules_generator,submodule_accessor,whitelist_param_names_creator)
  BaseOffloader.post_init(self)
  get_offloader()
  set_offloader(instance)
  create_offloader_from_server_args(server_args,dp_rank)
  OffloaderV1.__init__(self,cpu_offload_max_bytes)
  OffloaderV1.wrap_modules(self,all_modules_generator,submodule_accessor,whitelist_param_names_creator)
  OffloaderV1.maybe_offload_to_cpu(self,module)
  OffloaderV1.forward(*args,**kwargs)
  OffloaderV2.__init__(self,group_size,num_in_group,prefetch_step,mode,dp_rank,dp_size)
  OffloaderV2.wrap_modules(self,all_modules_generator,submodule_accessor,whitelist_param_names_creator)
  OffloaderV2.post_init(self)
  _hook_module_forward_for_offloader(index,module,offloaders,prefetch_step)
  _on_forward_end()
  _hook_module_forward_raw(module,on_forward_end,get_parameter_and_buffer_dicts)
  forward(*args,**kwargs)
  _ModuleOffloader.__init__(self,mode,module,alt_stream,whitelist_param_names)
  _ModuleOffloader.post_init(self)
  _ModuleOffloader.start_onload(self)
  _ModuleOffloader.offload(self)
  _ModuleOffloader.wait_and_get_device_tensors(self)
  _ModuleOffloader._create_device_tensors(self)
  _BaseParamOffloader.create(mode,**kwargs)
  _BaseParamOffloader.__init__(self,module,param_name)
  _BaseParamOffloader._param(self)
  _BaseParamOffloader.post_init(self)
  _BaseParamOffloader.create_device_tensor(self)
  _MetaParamOffloader.__init__(self,module,param_name)
  _MetaParamOffloader.create_device_tensor(self)
  _CpuParamOffloader.__init__(self,module,param_name)
  _CpuParamOffloader.create_device_tensor(self)
  _ShmCpuParamOffloader.__init__(self,module,param_name)
  _ShmCpuParamOffloader.post_init(self)
  _ShmCpuParamOffloader.create_device_tensor(self)
  _move_param_to_cpu(param,pin_memory)
  _move_param_to_meta(module,param_name)
  _empty_strided_like(x,device,pin_memory)
  _ShardedGpuParamOffloader.__init__(self,module,param_name)
  _ShardedGpuParamOffloader.post_init(self)
  _ShardedGpuParamOffloader.create_device_tensor(self)
  _even_chunk(x,chunks)
  _create_shared_buffer_tensors(local_tensor)
operations.py:
  execute_operations(inputs,operations)
  execute_overlapped_operations(inputs_arr,operations_arr,delta_stages)
  _StageExecutor.__init__(self,debug_name,stages,inputs)
  _StageExecutor.next(self)
  _StageExecutor.output(self)
  _StageExecutor.done(self)
  _StageExecutor.num_stages(self)
  _annotate_region(debug_name)
  _StateDict.__init__(self)
  _StateDict.__setattr__(self,key,value)
  _StateDict.__getattr__(self,item)
  _StateDict.__delattr__(self,item)
  _StateDict.pop(self,item)
  _StateDict.update(self,values)
  _StateDict.get(self,item)
  _StateDict.clear(self,expect_keys)
  _convert_operations_to_stages(operations)
  _chunk_by_separator(items,is_separator)
  _decorate_operations(operations,debug_name_prefix)
  _decorate_operation(operation,debug_name_prefix)
operations_strategy.py:
  OperationsStrategy.concat(cls,items)
  OperationsStrategy.init_new_tbo(layers,forward_mode)
  _assert_all_same(items)
  _compute_moe_deepseek_layer_operations_strategy_tbo(layer,forward_mode)
  _compute_moe_deepseek_blog_prefill(layer)
  _compute_moe_deepseek_blog_decode(layer)
  _compute_moe_qwen3_layer_operations_strategy_tbo(layer,forward_mode)
  _compute_moe_qwen3_prefill(layer)
  _compute_moe_qwen3_decode(layer)
patch_torch.py:
  monkey_patch_torch_reductions()
  _reduce_tensor_modified(*args,**kwargs)
  _rebuild_cuda_tensor_modified(*args)
  _device_to_uuid(device)
  _device_from_maybe_uuid(device_maybe_uuid)
  _modify_tuple(t,index,modifier)
  monkey_patch_torch_compile()
poll_based_barrier.py:
  PollBasedBarrier.__init__(self,noop)
  PollBasedBarrier.local_arrive(self)
  PollBasedBarrier.poll_global_arrived(self)
  PollBasedBarrier._compute_global_arrived(self)
reasoning_parser.py:
  StreamingParseResult.__init__(self,normal_text,reasoning_text)
  BaseReasoningFormatDetector.__init__(self,think_start_token,think_end_token,force_reasoning,stream_reasoning)
  BaseReasoningFormatDetector.detect_and_parse(self,text)
  BaseReasoningFormatDetector.parse_streaming_increment(self,new_text)
  DeepSeekR1Detector.__init__(self,stream_reasoning,force_reasoning)
  Qwen3Detector.__init__(self,stream_reasoning,force_reasoning)
  KimiDetector.__init__(self,stream_reasoning,force_reasoning)
  GptOssDetector.__init__(self,stream_reasoning,force_reasoning)
  GptOssDetector.detect_and_parse(self,text)
  GptOssDetector.parse_streaming_increment(self,new_text)
  ReasoningParser.__init__(self,model_type,stream_reasoning,force_reasoning)
  ReasoningParser.parse_non_stream(self,full_text)
  ReasoningParser.parse_stream_chunk(self,chunk_text)
sampling/custom_logit_processor.py:
  _cache_from_str(json_str)
  CustomLogitProcessor.__call__(self,logits,custom_param_list)
  CustomLogitProcessor.to_str(cls)
  CustomLogitProcessor.from_str(cls,json_str)
  DisallowedTokensLogitsProcessor.__call__(self,logits,custom_param_list)
sampling/penaltylib/frequency_penalty.py:
  BatchedFrequencyPenalizer.__init__(self,orchestrator)
  BatchedFrequencyPenalizer._is_required(self)
  BatchedFrequencyPenalizer._prepare(self)
  BatchedFrequencyPenalizer._cumulate_output_tokens(self,output_ids)
  BatchedFrequencyPenalizer._apply(self,logits)
  BatchedFrequencyPenalizer._filter(self,keep_indices)
  BatchedFrequencyPenalizer._merge(self,their)
sampling/penaltylib/min_new_tokens.py:
  BatchedMinNewTokensPenalizer.__init__(self,orchestrator)
  BatchedMinNewTokensPenalizer._is_required(self)
  BatchedMinNewTokensPenalizer._prepare(self)
  BatchedMinNewTokensPenalizer._cumulate_output_tokens(self,output_ids)
  BatchedMinNewTokensPenalizer._apply(self,logits)
  BatchedMinNewTokensPenalizer._filter(self,keep_indices)
  BatchedMinNewTokensPenalizer._merge(self,their)
sampling/penaltylib/orchestrator.py:
  BatchedPenalizerOrchestrator.__init__(self,vocab_size,batch,penalizers)
  BatchedPenalizerOrchestrator.batch(self)
  BatchedPenalizerOrchestrator.batch(self,value)
  BatchedPenalizerOrchestrator.reqs(self)
  BatchedPenalizerOrchestrator.cumulate_output_tokens(self,output_ids)
  BatchedPenalizerOrchestrator.apply(self,logits)
  BatchedPenalizerOrchestrator.filter(self,keep_indices)
  BatchedPenalizerOrchestrator.merge(self,their)
  _BatchedPenalizer.is_prepared(self)
  _BatchedPenalizer.is_required(self)
  _BatchedPenalizer.prepare(self)
  _BatchedPenalizer.prepare_if_required(self)
  _BatchedPenalizer.teardown(self)
  _BatchedPenalizer.cumulate_output_tokens(self,output_ids)
  _BatchedPenalizer.apply(self,logits)
  _BatchedPenalizer.filter(self,keep_indices)
  _BatchedPenalizer.merge(self,their)
  _BatchedPenalizer._is_required(self)
  _BatchedPenalizer._prepare(self)
  _BatchedPenalizer._cumulate_output_tokens(self,output_ids)
  _BatchedPenalizer._apply(self,logits)
  _BatchedPenalizer._filter(self,keep_indices)
  _BatchedPenalizer._merge(self,their)
sampling/penaltylib/presence_penalty.py:
  BatchedPresencePenalizer.__init__(self,orchestrator)
  BatchedPresencePenalizer._is_required(self)
  BatchedPresencePenalizer._prepare(self)
  BatchedPresencePenalizer._cumulate_output_tokens(self,output_ids)
  BatchedPresencePenalizer._apply(self,logits)
  BatchedPresencePenalizer._filter(self,keep_indices)
  BatchedPresencePenalizer._merge(self,their)
sampling/sampling_batch_info.py:
  SamplingBatchInfo.from_schedule_batch(cls,batch,vocab_size)
  SamplingBatchInfo.__len__(self)
  SamplingBatchInfo.update_regex_vocab_mask(self)
  SamplingBatchInfo.update_penalties(self)
  SamplingBatchInfo.apply_logits_bias(self,logits)
  SamplingBatchInfo.filter_batch(self,keep_indices,keep_indices_device)
  SamplingBatchInfo._filter_batch_custom_logit_processor(self,keep_indices,keep_indices_device)
  SamplingBatchInfo.merge_custom_logit_processor(lhs,rhs,bs1,bs2,device)
  SamplingBatchInfo.merge_batch(self,other)
  merge_bias_tensor(lhs,rhs,bs1,bs2,device,default)
sampling/sampling_params.py:
  SamplingParams.__init__(self,max_new_tokens,stop,stop_token_ids,temperature,top_p,top_k,min_p,frequency_penalty,presence_penalty,repetition_penalty,min_new_tokens,n,json_schema,regex,ebnf,structural_tag,ignore_eos,skip_special_tokens,spaces_between_special_tokens,no_stop_trim,custom_params,stream_interval,logit_bias)
  SamplingParams.verify(self,vocab_size)
  SamplingParams.normalize(self,tokenizer)
server_args.py:
  add_load_format_choices(choices)
  add_quantization_method_choices(choices)
  add_attention_backend_choices(choices)
  add_disagg_transfer_backend_choices(choices)
  ServerArgs.__post_init__(self)
  ServerArgs.add_cli_args(parser)
  ServerArgs.from_cli_args(cls,args)
  ServerArgs.url(self)
  ServerArgs.get_hf_config(self)
  ServerArgs.check_server_args(self)
  ServerArgs.check_lora_server_args(self)
  ServerArgs.validate_disagg_tp_size(self,prefill_tp,decode_tp)
  ServerArgs.model_specific_adjustments(self)
  ServerArgs.adjust_mem_fraction_for_vlm(self,model_config)
  prepare_server_args(argv)
  PortArgs.init_new(server_args,dp_rank)
  LoRAPathAction.__call__(self,parser,namespace,values,option_string)
  DeprecatedAction.__init__(self,option_strings,dest,nargs,**kwargs)
  DeprecatedAction.__call__(self,parser,namespace,values,option_string)
  print_deprecated_warning(message)
  auto_choose_speculative_params(self)
speculative/build_eagle_tree.py:
  build_tree_kernel_efficient_preprocess(verified_id,score_list,token_list,parents_list,num_verify_tokens)
  build_tree_kernel_efficient(verified_id,score_list,token_list,parents_list,seq_lens,seq_lens_sum,topk,spec_steps,num_verify_tokens,tree_mask_mode,tree_mask_buf,position_buf)
  test_build_tree_kernel_efficient()
speculative/eagle_draft_cuda_graph_runner.py:
  EAGLEDraftCudaGraphRunner.__init__(self,eagle_worker)
  EAGLEDraftCudaGraphRunner.can_run(self,forward_batch)
  EAGLEDraftCudaGraphRunner.capture(self)
  EAGLEDraftCudaGraphRunner.capture_one_batch_size(self,num_seqs,forward)
  EAGLEDraftCudaGraphRunner.run_once()
  EAGLEDraftCudaGraphRunner._postprocess_output_to_raw_bs(self,out,raw_bs)
  EAGLEDraftCudaGraphRunner.replay(self,forward_batch)
speculative/eagle_draft_extend_cuda_graph_runner.py:
  EAGLEDraftExtendCudaGraphRunner.__init__(self,eagle_worker)
  EAGLEDraftExtendCudaGraphRunner.can_run(self,forward_batch)
  EAGLEDraftExtendCudaGraphRunner.capture(self)
  EAGLEDraftExtendCudaGraphRunner.capture_one_batch_size(self,bs,forward)
  EAGLEDraftExtendCudaGraphRunner.run_once()
  EAGLEDraftExtendCudaGraphRunner.replay(self,forward_batch)
speculative/eagle_utils.py:
  EagleDraftInput.prepare_for_extend(self,batch)
  EagleDraftInput.create_idle_input(cls,device,hidden_size,dtype,topk,capture_hidden_mode)
  EagleDraftInput.prepare_extend_after_decode(self,batch,speculative_num_steps)
  EagleDraftInput.generate_attn_arg_prefill(self,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,req_to_token)
  EagleDraftInput.filter_batch(self,new_indices,has_been_filtered)
  EagleDraftInput.merge_batch(self,spec_info)
  EagleVerifyInput.create_idle_input(cls,topk,spec_steps,num_verify_tokens)
  EagleVerifyInput.prepare_for_verify(self,batch,page_size)
  EagleVerifyInput.generate_attn_arg_prefill(self,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,req_to_token)
  EagleVerifyInput.verify(self,batch,logits_output,token_to_kv_pool_allocator,page_size,vocab_mask)
  create_extend_after_decode_spec_info(verified_id,seq_lens,accept_lens,positions,new_verified_id,bs_upper)
  assign_req_to_token_pool(req_pool_indices,req_to_token,start_offset,end_offset,out_cache_loc,pool_len,bs_upper)
  assign_draft_cache_locs(req_pool_indices,req_to_token,seq_lens,extend_lens,num_new_pages_per_topk,out_cache_loc,pool_len,topk,speculative_num_steps,page_size,bs_upper,iter_upper)
  generate_draft_decode_kv_indices(req_pool_indices,req_to_token,paged_kernel_lens,kv_indices,kv_indptr,positions,pool_len,kv_indices_stride,kv_indptr_stride,bs_upper,iter_upper,num_tokens_upper,page_size)
  align_evict_mask_to_page_size(seq_lens,evict_mask,page_size,num_draft_tokens,BLOCK_SIZE)
  get_target_cache_loc(tgt_cache_loc,to_free_slots,accept_length,to_free_num_slots,out_cache_loc,num_verify_tokens,num_verify_tokens_upper,bs_upper)
  get_src_tgt_cache_loc(seq_lens,out_cache_loc,accept_index,accept_length,draft_token_num,page_size)
  filter_finished_cache_loc_kernel(out_cache_loc,tgt_cache_loc,accept_length,accept_length_filter,bs_upper,num_verify_tokens_upper)
  create_accept_length_filter(accept_length,unfinished_index_device,seq_lens)
  select_top_k_tokens(i,topk_p,topk_index,hidden_states,scores,topk)
  _generate_simulated_accept_index(accept_index,predict,accept_length,simulate_acc_len,bs,spec_steps)
  traverse_tree(retrieve_next_token,retrieve_next_sibling,draft_tokens,grammar,allocate_token_bitmask)
  dfs(curr,retrieve_next_token,retrieve_next_sibling,parent_pos)
  generate_token_bitmask(reqs,verify_input,retrieve_next_token_cpu,retrieve_next_sibling_cpu,draft_tokens_cpu,vocab_size)
speculative/eagle_worker.py:
  draft_tp_context(tp_group)
  EAGLEWorker.__init__(self,server_args,gpu_id,tp_rank,dp_rank,moe_ep_rank,nccl_port,target_worker)
  EAGLEWorker.init_attention_backend(self)
  EAGLEWorker.init_cuda_graphs(self)
  EAGLEWorker.draft_model_runner(self)
  EAGLEWorker.forward_batch_speculative_generation(self,batch)
  EAGLEWorker.check_forward_draft_extend_after_decode(self,batch)
  EAGLEWorker.forward_target_extend(self,batch)
  EAGLEWorker._draft_preprocess_decode(self,batch)
  EAGLEWorker._draft_preprocess_idle(self,batch)
  EAGLEWorker.draft(self,batch)
  EAGLEWorker.draft_forward(self,forward_batch)
  EAGLEWorker.verify(self,batch,spec_info)
  EAGLEWorker.add_logprob_values(self,batch,res,logits_output)
  EAGLEWorker.forward_draft_extend(self,batch,hidden_states,next_token_ids,seq_lens_cpu)
  EAGLEWorker.forward_draft_extend_after_decode(self,batch)
  EAGLEWorker.capture_for_decode(self,logits_output,draft_input)
  EAGLEWorker._detect_nan_if_needed(self,logits_output)
  load_token_map(token_map_path)
  get_last_loc_large_page_size_top_k_1(req_to_token,req_pool_indices,seq_lens,speculative_num_steps)
  get_last_loc_large_page_size_large_top_k(req_to_token,req_pool_indices,seq_lens,speculative_num_steps,topk,page_size)
speculative/spec_info.py:
  SpeculativeAlgorithm.is_none(self)
  SpeculativeAlgorithm.is_eagle(self)
  SpeculativeAlgorithm.is_eagle3(self)
  SpeculativeAlgorithm.from_string(name)
tokenizer/tiktoken_tokenizer.py:
  TiktokenProcessor.__init__(self,name)
  TiktokenProcessor.image_processor(self,image)
  TiktokenTokenizer.__init__(self,tokenizer_path)
  TiktokenTokenizer.encode_patched(self,text,allowed_special,disallowed_special)
  TiktokenTokenizer.encode(self,x,add_special_tokens)
  TiktokenTokenizer.decode(self,x,*args,**kwargs)
  TiktokenTokenizer.batch_decode(self,batch,skip_special_tokens,spaces_between_special_tokens)
  TiktokenTokenizer.apply_chat_template(self,messages,tokenize,add_generation_prompt,tools,reasoning_effort)
  TiktokenTokenizer.__call__(self,text,**kwargs)
  TiktokenTokenizer.init_xgrammar(self)
torch_memory_saver_adapter.py:
  TorchMemorySaverAdapter.create(enable)
  TorchMemorySaverAdapter.check_validity(self,caller_name)
  TorchMemorySaverAdapter.configure_subprocess(self)
  TorchMemorySaverAdapter.region(self,tag)
  TorchMemorySaverAdapter.pause(self,tag)
  TorchMemorySaverAdapter.resume(self,tag)
  TorchMemorySaverAdapter.enabled(self)
  _TorchMemorySaverAdapterReal.configure_subprocess(self)
  _TorchMemorySaverAdapterReal.region(self,tag)
  _TorchMemorySaverAdapterReal.pause(self,tag)
  _TorchMemorySaverAdapterReal.resume(self,tag)
  _TorchMemorySaverAdapterReal.enabled(self)
  _TorchMemorySaverAdapterNoop.configure_subprocess(self)
  _TorchMemorySaverAdapterNoop.region(self,tag)
  _TorchMemorySaverAdapterNoop.pause(self,tag)
  _TorchMemorySaverAdapterNoop.resume(self,tag)
  _TorchMemorySaverAdapterNoop.enabled(self)
two_batch_overlap.py:
  get_token_num_per_seq(forward_mode,spec_info)
  compute_split_seq_index(forward_mode,num_tokens,extend_lens,token_num_per_seq)
  _is_two_chunk_split_enabled(extend_lens)
  _split_extend_seqs(arr)
  _split_array_by_cum_less_than_half(arr)
  _split_array_by_balanced_sum(arr)
  _update_device_and_sum_field_from_cpu_field(batch,cpu_field,device_field,sum_field)
  _compute_mask_offset(seq_index,spec_info)
  split_spec_info(spec_info,start_seq_index,end_seq_index,start_token_index,end_token_index)
  compute_split_token_index(split_seq_index,forward_mode,extend_seq_lens,token_num_per_seq)
  compute_split_indices_for_cuda_graph_replay(forward_mode,cuda_graph_num_tokens,spec_info)
  TboCudaGraphRunnerPlugin.__init__(self)
  TboCudaGraphRunnerPlugin.capture_one_batch_size(self,batch,num_tokens)
  TboCudaGraphRunnerPlugin.replay_prepare(self,forward_mode,bs,num_token_non_padded,spec_info)
  TboDPAttentionPreparer.prepare_all_gather(self,local_batch)
  TboDPAttentionPreparer.compute_output(self,partial_global_info)
  TboDPAttentionPreparer._compute_local_forward_mode(local_batch)
  TboDPAttentionPreparer._compute_global_forward_mode(forward_modes)
  TboDPAttentionPreparer._is_all_same(x)
  TboForwardBatchPreparer.prepare(cls,batch,is_draft_worker)
  TboForwardBatchPreparer.prepare_raw(cls,batch,tbo_children_num_token_non_padded)
  TboForwardBatchPreparer.derive_fields_related_to_seq_len_for_two_chunk(cls,batch,child_a,child_b,tbo_split_seq_index)
  TboForwardBatchPreparer.filter_batch(cls,batch,start_token_index,end_token_index,start_seq_index,end_seq_index,output_attn_backend,out_num_token_non_padded)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded(cls,batch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded_raw(cls,tbo_split_token_index,num_token_non_padded)
  TboForwardBatchPreparer._compute_split_token_index(cls,batch)
  _compute_extend_num_tokens(input_ids,forward_mode)
  model_forward_maybe_tbo(layers,enable_tbo,positions,forward_batch,hidden_states,input_data_scatter_mode,residual,zero_allocator)
  _model_forward_tbo(inputs,operations_strategy,input_data_scatter_mode,layer_input_scatter_mode)
  _model_forward_non_tbo(inputs,operations_strategy)
  _model_forward_tbo_split_inputs(hidden_states,residual,positions,forward_batch,zero_allocator,input_data_scatter_mode,layer_input_scatter_mode)
  _post_transform(hidden_states,residual,forward_batch,**kwargs)
  _model_forward_tbo_split_inputs_raw(hidden_states,residual,positions,forward_batch,zero_allocator)
  _model_forward_filter_inputs(hidden_states,residual,positions,output_forward_batch,tbo_subbatch_index)
  _model_forward_tbo_merge_outputs(output_a,output_b)
  _handle_key(name)
  MaybeTboDeepEPDispatcher.__init__(self,**kwargs)
  MaybeTboDeepEPDispatcher._execute(self,name,tbo_subbatch_index,**kwargs)
  MaybeTboDeepEPDispatcher.dispatch(self,**kwargs)
  MaybeTboDeepEPDispatcher.dispatch_a(self,**kwargs)
  MaybeTboDeepEPDispatcher.dispatch_b(self,**kwargs)
  MaybeTboDeepEPDispatcher.combine(self,**kwargs)
  MaybeTboDeepEPDispatcher.combine_a(self,**kwargs)
  MaybeTboDeepEPDispatcher.combine_b(self,**kwargs)
utils.py:
  is_hip()
  is_cuda()
  is_cuda_alike()
  is_hpu()
  is_xpu()
  is_npu()
  is_host_cpu_x86()
  is_cpu()
  get_cuda_version()
  _check(cc_major)
  is_blackwell()
  is_sm100_supported(device)
  is_sm90_supported(device)
  get_bool_env_var(name,default)
  get_int_env_var(name,default)
  support_triton(backend)
  cpu_has_amx_support()
  use_intel_amx_backend(layer)
  is_flashinfer_available()
  random_uuid()
  DynamicGradMode.set_inference_mode(mode)
  DynamicGradMode.__init__(self,mode)
  DynamicGradMode.__new__(cls,mode_or_orig_func)
  DynamicGradMode.__enter__(self)
  DynamicGradMode.__exit__(self,exc_type,exc_value,traceback)
  DynamicGradMode.clone(self)
  enable_show_time_cost()
  TimeInfo.__init__(self,name,interval,color,indent)
  TimeInfo.check(self)
  TimeInfo.pretty_print(self)
  mark_start(name,interval,color,indent)
  mark_end(name)
  calculate_time(show,min_cost_ms)
  wrapper(func)
  inner_func(*args,**kwargs)
  get_available_gpu_memory(device,gpu_id,distributed,empty_cache,cpu_group)
  is_pin_memory_available()
  LayerFn.__call__(self,layer_id,prefix)
  make_layers(num_hidden_layers,layer_fn,pp_rank,pp_size,prefix,return_tuple,offloader_kwargs)
  set_random_seed(seed)
  find_process_using_port(port)
  wait_port_available(port,port_name,timeout_s,raise_exception)
  is_port_available(port)
  get_free_port()
  decode_video_base64(video_base64)
  load_audio(audio_file,sr,mono)
  load_image(image_file)
  load_video(video_file,use_gpu)
  suppress_other_loggers()
  assert_pkg_version(pkg,min_version,message)
  kill_process_tree(parent_pid,include_parent,skip_pid)
  monkey_patch_p2p_access_check()
  monkey_patch_vllm_gguf_config()
  get_quant_method_with_embedding_replaced(self,layer,prefix)
  set_ulimit(target_soft_limit)
  add_api_key_middleware(app,api_key)
  async authentication(request,call_next)
  prepare_model_and_tokenizer(model_path,tokenizer_path)
  configure_logger(server_args,prefix)
  replace_submodule(model,module_name,new_module)
  set_weight_attrs(weight,weight_attrs)
  broadcast_pyobj(data,rank,dist_group,src,force_cpu_device)
  point_to_point_pyobj(data,rank,group,src,dst)
  pytorch_profile(name,func,*args,data_size)
  get_zmq_socket(context,socket_type,endpoint,bind)
  set_send_opt()
  set_recv_opt()
  dump_to_file(dirpath,name,value)
  is_triton_3()
  maybe_torch_compile(*args,**kwargs)
  decorator(func)
  delete_directory(dirpath)
  set_prometheus_multiproc_dir()
  add_prometheus_middleware(app)
  bind_port(port)
  get_amdgpu_memory_capacity()
  get_device_sm()
  get_nvgpu_memory_capacity()
  get_hpu_memory_capacity()
  get_npu_memory_capacity()
  get_device_memory_capacity(device)
  init_custom_process_group(backend,init_method,timeout,world_size,rank,store,group_name,pg_options)
  crash_on_warnings()
  print_warning_once(msg)
  print_info_once(msg)
  get_device_name(device_id)
  is_habana_available()
  get_device(device_id)
  get_device_count()
  get_device_core_count(device_id)
  get_device_capability(device_id)
  get_npu_compiler_config()
  get_compiler_backend()
  supports_custom_op()
  direct_register_custom_op(op_name,op_func,mutates_args,fake_impl,target_lib)
  set_gpu_proc_affinity(tp_size,nnodes,gpu_id)
  disable_request_logging()
  dataclass_to_string_truncated(data,max_length,skip_names)
  permute_weight(x)
  MultiprocessingSerializer.serialize(obj,output_str)
  MultiprocessingSerializer.deserialize(data)
  debug_timing(func)
  wrapper(*args,**kwargs)
  nullable_str(val)
  pyspy_dump_schedulers()
  kill_itself_when_parent_died()
  set_uvicorn_logging_configs()
  get_ip()
  get_open_port()
  is_valid_ipv6_address(address)
  maybe_wrap_ipv6_address(address)
  format_tcp_address(ip,port)
  configure_ipv6(dist_init_addr)
  launch_dummy_health_check_server(host,port,enable_metrics)
  async health()
  async health_generate()
  create_checksum(directory)
  set_cuda_arch()
  next_power_of_2(n)
  round_up(x,y)
  EmptyContextManager.__enter__(self)
  EmptyContextManager.__exit__(self,exc_type,exc_value,traceback)
  empty_context(*args,**kwargs)
  add_prefix(name,prefix)
  is_remote_url(url)
  parse_connector_type(url)
  retry(fn,max_retry,initial_delay,max_delay,should_retry)
  flatten_nested_list(nested_list)
  is_non_idle_and_non_empty(forward_mode,hidden_states)
  fast_topk(values,topk,dim)
  bind_or_assign(target,source)
  get_local_ip_auto()
  get_local_ip_by_nic(interface)
  get_local_ip_by_remote()
  is_page_size_one(server_args)
  is_no_spec_infer_or_topk_one(server_args)
  is_fa3_default_architecture(hf_config)
  BumpAllocator.__init__(self,buffer_size,dtype,device)
  BumpAllocator.allocate(self,size)
  log_info_on_rank0(logger,msg)
  load_json_config(data)
  dispose_tensor(x)
  Withable.__init__(self)
  Withable.value(self)
  Withable.with_value(self,new_value)
  require_mlp_tp_gather(server_args)
  require_attn_tp_gather(server_args)
  require_gathered_buffer(server_args)
  require_mlp_sync(server_args)
  find_local_repo_dir(repo_id,revision)
  read_system_prompt_from_file(model_name)
  bind_or_assign(target,source)
  prepack_weight_if_needed(weight)
  dim_is_supported(weight)
  _process_weight_after_loading(module,weight_names,transpose_dims)
  PackWeightMethod.__init__(self,weight_names,transpose_dims)
  PackWeightMethod.process_weights_after_loading(self,module)
  LazyValue.__init__(self,creator)
  LazyValue.value(self)
  dynamic_import(func_path)
  gc_object_counts()
  configure_gc_warning(warn_threshold_secs)
  gc_callback(phase,info)
  freeze_gc(context)
  configure_gc_logger()
  gc_callback(phase,info)
  align(x,y)
  ceil_div(x,y)
  parse_lscpu_topology()
  get_physical_cpus_by_numa()
  get_cpu_ids_by_node()
  is_shm_available(dtype,world_size,local_size)
  lru_cache_frozenset(maxsize)
  _to_hashable(o)
  decorator(func)
  wrapper(*args,**kwargs)
  apply_module_patch(target_module,target_function,wrappers)
  parse_module_path(module_path,function_name,create_dummy)
  create_dummy_module(full_path,parent)
  create_placeholder_function(func_name)
  placeholder(*args,**kwargs)
  mxfp_supported()
  ConcurrentCounter.__init__(self,initial)
  ConcurrentCounter.value(self)
  ConcurrentCounter.__repr__(self)
  async ConcurrentCounter.increment(self,n,notify_all)
  async ConcurrentCounter.decrement(self,n,notify_all)
  async ConcurrentCounter.wait_for(self,condition)
  async ConcurrentCounter.wait_for_zero(self)
  is_triton_kernels_available()
  check_cuda_result(raw_output)
warmup.py:
  warmup(name)
  decorator(fn)
  async execute_warmups(disaggregation_mode,warmup_names,tokenizer_manager)
  async voice_chat(disaggregation_mode,tokenizer_manager)
weight_sync/tensor_bucket.py:
  FlattenedTensorBucket.__init__(self,named_tensors,flattened_tensor,metadata)
  FlattenedTensorBucket.get_flattened_tensor(self)
  FlattenedTensorBucket.get_metadata(self)
  FlattenedTensorBucket.reconstruct_tensors(self)
weight_sync/utils.py:
  async update_weights(engine,params_batch,device_mesh_key,device_mesh,load_format)
  _preprocess_tensor_for_update_weights(tensor)
