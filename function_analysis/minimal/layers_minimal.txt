layers/activation.py:
  GeluAndMul.__init__(self,approximate)
  GeluAndMul.forward_cuda(self,x)
  GeluAndMul.forward_native(self,x)
  GeluAndMul.forward_npu(self,x)
  NewGELU.forward_cuda(self,x)
  NewGELU.forward_native(self,x)
  QuickGELU.forward_cuda(self,x)
  QuickGELU.forward_hip(self,x)
  QuickGELU.forward_native(self,x)
  QuickGELU.forward_npu(self,x)
  ReLU2.forward(self,x)
  ScaledActivation.__init__(self,act_module,intermediate_size,input_is_parallel,params_dtype)
  ScaledActivation.forward(self,x)
  ScaledActivation.weight_loader(self,param,loaded_weight)
  SiluAndMul.forward_cpu(self,x)
  SiluAndMul.forward_cuda(self,x)
  SiluAndMul.forward_native(self,x)
  SiluAndMul.forward_npu(self,x)
  get_act_fn(act_fn_name,quant_config,intermediate_size,input_is_parallel,params_dtype)
  get_cross_encoder_activation_function(config)
layers/amx_utils.py:
  PackWeightMethod.__init__(self,weight_names,transpose_dims)
  PackWeightMethod.process_weights_after_loading(self,module)
  _amx_process_weight_after_loading(module,weight_names,transpose_dims)
  amx_process_weight_after_loading(weight)
  dim_is_supported(weight)
layers/attention/aiter_backend.py:
  AiterAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  AiterAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  AiterAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AiterAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  AiterAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  AiterAttnBackend.init_forward_metadata(self,forward_batch)
  AiterAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AiterAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  AiterIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  AiterIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,encoder_lens,spec_info)
  AiterIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,encoder_lens,spec_info)
  AiterMlaIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  AiterMlaIndicesUpdaterPrefill.update(self,req_pool_indices,kv_lens,kv_lens_sum,extend_lens,max_q_len,max_kv_len,spec_info)
  AiterMlaIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,kv_lens,kv_lens_sum,extend_lens,max_q_len,max_kv_len,spec_info)
  AiterMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
  AiterMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  AiterMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AiterMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  AiterMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  AiterMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
layers/attention/ascend_backend.py:
  AscendAttnBackend.__init__(self,model_runner)
  AscendAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  AscendAttnBackend.forward_decode_graph(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  AscendAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AscendAttnBackend.gen_attention_mask(self,max_seq_len,dtype)
  AscendAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  AscendAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AscendAttnBackend.init_forward_metadata(self,forward_batch)
  AscendAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AscendAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
layers/attention/base_attn_backend.py:
  AttentionBackend.forward(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  AttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  AttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  AttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AttentionBackend.init_forward_metadata(self,forward_batch)
  AttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  AttentionBackend.support_triton(self)
layers/attention/cutlass_mla_backend.py:
  CutlassMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  CutlassMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  CutlassMLABackend.get_cuda_graph_seq_len_fill_value(self)
  CutlassMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,block_kv_indices)
  CutlassMLABackend.init_forward_metadata(self,forward_batch)
  CutlassMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  CutlassMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  CutlassMLADecodeMetadata.__init__(self,workspace,block_kv_indices)
layers/attention/double_sparsity_backend.py:
  DoubleSparseAttnBackend.__init__(self,model_runner)
  DoubleSparseAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  DoubleSparseAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  DoubleSparseAttnBackend.init_forward_metadata(self,forward_batch)
layers/attention/dual_chunk_flashattention_backend.py:
  DualChunkFlashAttentionBackend.__init__(self,model_runner)
  DualChunkFlashAttentionBackend._do_flash_attn(self,query_states,key_states,value_states,softmax_scale,causal,max_seqlen_k,stage,vertical_indices,slash_indices,vertical_indices_count,slash_indices_count,mergehead_softmax_scale,sparse_attn_enabled)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_decoding(self,query,query_succ,query_inter,key_cache,value_cache,block_table,cache_seqlens,softmax_scale,causal,chunk_size,local_size,original_max_position_embeddings,decode_meta)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_decoding_with_exp_sums(self,query,key_cache,value_cache,block_table,cache_seqlens,softmax_scale,causal)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_prefill(self,q,q_succ,q_inter,q_succ_critical,q_inter_critical,k,v,cu_seqlens_q,cu_seqlens_k,orig_seq_lens,scaling_factor,softmax_scale,causal,window_size,block_table,chunk_size,local_size)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_prefill_func(self,q,q_succ,q_inter,q_succ_critical,q_inter_critical,k,v,block_table,softmax_scale,chunk_size,local_size,scaling_factor,k_length,sparse_attn_enabled,heads_vertical_size,heads_slash_size,group_size)
  DualChunkFlashAttentionBackend._merge_attn_outputs(self,flash_results,return_lse)
  DualChunkFlashAttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  DualChunkFlashAttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  DualChunkFlashAttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  DualChunkFlashAttentionBackend.get_sparse_attention_config(self,layer_idx)
  DualChunkFlashAttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  DualChunkFlashAttentionBackend.init_forward_metadata(self,forward_batch)
  DualChunkFlashAttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  DualChunkFlashAttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu,out_cache_loc)
  _get_block(block_table,block_size,begin,end)
  _sum_all_diagonal_matrix(mat)
  _vertical_slash_sparse_attention(query,key,value,v_idx,s_idx,softmax_scale,causal,stage,block_size_M,block_size_N,vertical_indices_count,slash_indices_count)
layers/attention/flashattention_backend.py:
  FlashAttentionBackend.__init__(self,model_runner,skip_prefill,speculative_step_id,topk,speculative_num_steps)
  FlashAttentionBackend._init_local_attn_metadata(self,forwardbatch,metadata,device)
  FlashAttentionBackend._init_sliding_window_attn_spec_metadata(self,metadata,metadata_expand,metadata_swa)
  FlashAttentionBackend._update_local_attn_metadata_for_capture(self,metadata,bs)
  FlashAttentionBackend._update_local_attn_metadata_for_replay(self,metadata,bs)
  FlashAttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,sinks)
  FlashAttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,sinks)
  FlashAttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashAttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashAttentionBackend.init_forward_metadata(self,forward_batch)
  FlashAttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashAttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu,out_cache_loc)
  FlashAttentionMultiStepBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashAttentionMultiStepBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashAttentionMultiStepBackend.init_forward_metadata(self,forward_batch)
  FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  _prepare_swa_spec_page_table_kernel(dst_ptr,src_a_ptr,src_b_ptr,seq_len_a_ptr,seq_len_b_ptr,dst_stride_m,dst_stride_n,a_stride_m,a_stride_n,b_stride_m,b_stride_n,LEN_A,LEN_B,REPEAT_STEP,BLOCK_N)
  cdiv(a,b)
  make_local_attention_virtual_batches(attn_chunk_size,query_start_loc_np,seq_lens_np,block_table,page_size)
  merge_state_v2_wrapper(o,s_a,o_exp,s_b)
  normal_decode_set_metadata(cache_seqlens_int32,cu_seqlens_k,page_table,req_to_token,req_pool_indices,strided_indices,max_seq_pages,seq_lens,seq_len_delta,page_size)
  prepare_swa_spec_page_table_triton(page_table_dst,page_table_a,page_table_b,seq_len_a,seq_len_b,speculative_num_draft_tokens)
layers/attention/flashinfer_backend.py:
  FlashInferAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  FlashInferAttnBackend._get_wrapper_idx(self,layer)
  FlashInferAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashInferAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashInferAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashInferAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  FlashInferAttnBackend.init_forward_metadata(self,forward_batch)
  FlashInferAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashInferAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashInferIndicesUpdaterDecode.__init__(self,model_runner,attn_backend)
  FlashInferIndicesUpdaterDecode.call_begin_forward(self,wrapper,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,kv_indptr,kv_start_idx,spec_info,seq_lens_cpu,use_sliding_window_kv_pool)
  FlashInferIndicesUpdaterDecode.update(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_cross_attention(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_sliding_window(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  FlashInferIndicesUpdaterPrefill.call_begin_forward(self,wrapper_ragged,wrapper_paged,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,seq_lens,prefix_lens,kv_start_idx,kv_indptr,qo_indptr,use_ragged,spec_info,use_sliding_window_kv_pool)
  FlashInferIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_cross_attention(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_sliding_window(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  FlashInferMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashInferMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashInferMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashInferMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  fast_decode_plan(self,indptr,indices,last_page_len,num_qo_heads,num_kv_heads,head_dim,page_size,pos_encoding_mode,window_left,logits_soft_cap,q_data_type,kv_data_type,data_type,sm_scale,rope_scale,rope_theta,non_blocking)
  should_use_tensor_core(kv_cache_dtype,num_attention_heads,num_kv_heads)
layers/attention/flashinfer_mla_backend.py:
  FlashInferMLAAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,q_indptr_decode_buf)
  FlashInferMLAAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  FlashInferMLAAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  FlashInferMLAAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashInferMLAAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  FlashInferMLAAttnBackend.init_forward_metadata(self,forward_batch)
  FlashInferMLAAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashInferMLAAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashInferMLAAttnBackend.init_mha_chunk_metadata(self,forward_batch)
  FlashInferMLAIndicesUpdaterDecode.__init__(self,model_runner,attn_backend)
  FlashInferMLAIndicesUpdaterDecode.call_begin_forward(self,wrapper,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,q_indptr,kv_indptr,init_metadata_replay,spec_info,**fast_decode_kwargs)
  FlashInferMLAIndicesUpdaterDecode.update(self,req_pool_indices,seq_lens,seq_lens_sum,decode_wrapper,init_metadata_replay,spec_info,**fast_decode_kwargs)
  FlashInferMLAIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  FlashInferMLAIndicesUpdaterPrefill.call_begin_forward(self,wrapper_ragged,wrapper_paged,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,seq_lens,prefix_lens,kv_indptr,qo_indptr,use_ragged,spec_info)
  FlashInferMLAIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,prefill_wrapper_paged,use_ragged,spec_info)
  FlashInferMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMLAMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  FlashInferMLAMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  FlashInferMhaChunkKVRunner.__init__(self,model_runner,attn_backend)
  FlashInferMhaChunkKVRunner.forward(self,q,k,v,layer,forward_batch)
  FlashInferMhaChunkKVRunner.update_prefix_chunks(self,num_prefix_chunks)
  FlashInferMhaChunkKVRunner.update_wrapper(self,forward_batch)
  fast_mla_decode_plan(self,qo_indptr_cpu,kv_indptr_cpu,kv_indices,kv_len_arr_cpu,num_heads,head_dim_ckv,head_dim_kpe,page_size,causal,sm_scale,q_data_type,kv_data_type)
layers/attention/flashmla_backend.py:
  FlashMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  FlashMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashMLABackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashMLABackend.get_cuda_graph_seq_len_fill_value(self)
  FlashMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,block_kv_indices)
  FlashMLABackend.init_forward_metadata(self,forward_batch)
  FlashMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashMLADecodeMetadata.__init__(self,flashmla_metadata,num_splits,block_kv_indices)
  FlashMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashMLAMultiStepDraftBackend.common_template(self,forward_batch,call_fn)
  FlashMLAMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashMLAMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
layers/attention/hybrid_attn_backend.py:
  HybridAttnBackend.__init__(self,model_runner,prefill_backend,decode_backend)
  HybridAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  HybridAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  HybridAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  HybridAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  HybridAttnBackend.init_forward_metadata(self,forward_batch)
  HybridAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  HybridAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
layers/attention/intel_amx_backend.py:
  IntelAMXAttnBackend.__init__(self,model_runner)
  IntelAMXAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  IntelAMXAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  IntelAMXAttnBackend.init_forward_metadata(self,forward_batch)
  IntelAMXAttnBackend.support_triton(self)
layers/attention/merge_state.py:
  _supported_dtypes(o)
  _supported_headdim(o)
  merge_state(prefix_output,prefix_lse,suffix_output,suffix_lse,output,output_lse)
layers/attention/tbo_backend.py:
  TboAttnBackend.__init__(self,primary,children)
  TboAttnBackend._init_forward_metadata_cuda_graph_children(self,fn_name,bs,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info,capture_num_tokens,replay_seq_lens_sum,replay_seq_lens_cpu)
  TboAttnBackend.forward_decode(self,*args,**kwargs)
  TboAttnBackend.forward_extend(self,*args,**kwargs)
  TboAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TboAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TboAttnBackend.init_forward_metadata(self,forward_batch)
  TboAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TboAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TboAttnBackend.init_new(cls,creator)
  _init_forward_metadata_cuda_graph_split(fn_name,seq_slice,output_bs,bs,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info,capture_num_tokens,replay_seq_lens_sum,replay_seq_lens_cpu)
layers/attention/torch_native_backend.py:
  TorchNativeAttnBackend.__init__(self,model_runner)
  TorchNativeAttnBackend._run_sdpa_forward_decode(self,query,output,k_cache,v_cache,req_to_token,req_pool_indices,seq_lens,scaling,enable_gqa,causal)
  TorchNativeAttnBackend._run_sdpa_forward_extend(self,query,output,k_cache,v_cache,req_to_token,req_pool_indices,seq_lens,extend_prefix_lens,extend_seq_lens,scaling,enable_gqa,causal)
  TorchNativeAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  TorchNativeAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  TorchNativeAttnBackend.init_forward_metadata(self,forward_batch)
  TorchNativeAttnBackend.support_triton(self)
layers/attention/triton_backend.py:
  TritonAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  TritonAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,sinks)
  TritonAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,sinks)
  TritonAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TritonAttnBackend.get_num_kv_splits(self,num_kv_splits,seq_lens)
  TritonAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TritonAttnBackend.init_forward_metadata(self,forward_batch)
  TritonAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TritonAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TritonMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  TritonMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  TritonMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TritonMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  TritonMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  TritonMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  get_num_kv_splits_triton(num_kv_splits_ptr,seq_lens_ptr,num_seq,num_group,num_head,num_kv_head,max_kv_splits,device_core_count,MAX_NUM_SEQ)
  logit_capping_mod(logit_capping_method,logit_cap)
  update_sliding_window_buffer(window_kv_indptr,req_to_token,sliding_window_size,seq_lens,req_pool_indices,bs,device,token_to_kv_pool_allocator)
  update_sliding_window_buffer_cuda_graph(window_kv_indptr,window_kv_indices,req_to_token,sliding_window_size,seq_lens,req_pool_indices,bs,token_to_kv_pool_allocator)
layers/attention/triton_ops/decode_attention.py:
  _decode_att_m_fwd(q,k_buffer,v_buffer,att_out,att_lse,kv_indptr,kv_indices,num_kv_splits,max_kv_splits,sm_scale,logit_cap,xai_temperature_len)
  _decode_grouped_att_m_fwd(q,k_buffer,v_buffer,att_out,att_lse,kv_indptr,kv_indices,num_kv_splits,max_kv_splits,sm_scale,logit_cap,xai_temperature_len)
  _decode_softmax_reducev_fwd(logits,lse,q,o,v_buffer,kv_indptr,num_kv_splits,max_kv_splits,sinks)
  _fwd_grouped_kernel_stage1(Q,K_Buffer,V_Buffer,sm_scale,kv_indptr,kv_indices,Att_Out,Att_Lse,num_kv_splits,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,kv_group_num,q_head_num,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_N,BLOCK_H,MIN_BLOCK_KV,logit_cap,xai_temperature_len,Lk,Lv)
  _fwd_kernel_stage1(Q,K_Buffer,V_Buffer,sm_scale,kv_indptr,kv_indices,Att_Out,Att_Lse,num_kv_splits,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,kv_group_num,BLOCK_DMODEL,BLOCK_DV,BLOCK_N,MIN_BLOCK_KV,logit_cap,Lk,Lv,xai_temperature_len)
  _fwd_kernel_stage2(Mid_O,Mid_O_1,O,kv_indptr,num_kv_splits,sink_ptr,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_obs,stride_oh,MAX_KV_SPLITS,MIN_BLOCK_KV,BLOCK_DV,Lv,HAS_SINK)
  decode_attention_fwd(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
  decode_attention_fwd_grouped(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
  decode_attention_fwd_normal(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
  tanh(x)
layers/attention/triton_ops/double_sparsity_attention.py:
  _fwd_kernel(Q_Extend,K_Extend,V_Extend,O_Extend,K_Buffer,V_Buffer,Req_to_tokens,B_req_idx,B_Seq_Len,B_Start_Loc_Extend,B_Seq_Len_Extend,sm_scale,kv_group_num,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_req_to_tokens_b,logit_cap,Lq,Lv,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_M,BLOCK_N)
  _fwd_kernel_flash_decode_stage1(Q,K,V,sm_scale,Req_to_tokens,B_req_idx,B_Seqlen,Mid_O,Mid_O_LogExpSum,stride_req_to_tokens_b,stride_req_to_tokens_s,stride_qbs,stride_qh,stride_qd,stride_kbs,stride_kh,stride_kd,stride_vbs,stride_vh,stride_vd,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_od,stride_mid_o_eb,stride_mid_o_eh,stride_mid_o_es,gqa_group_size,BLOCK_SEQ,BLOCK_DMODEL,BLOCK_N)
  _fwd_kernel_flash_decode_stage2(B_Seqlen,Mid_O,Mid_O_LogExpSum,O,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_od,stride_mid_o_eb,stride_mid_o_eh,stride_mid_o_es,stride_obs,stride_oh,stride_od,BLOCK_SEQ,BLOCK_DMODEL)
  _sparse_fwd_kernel_flash_decode_stage1(Q_Label,K_Label_Buffer,sm_scale,Req_to_tokens,B_Seqlen,Att_Out,stride_req_to_tokens_b,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,att_stride_h,att_stride_b,kv_group_num,BLOCK_DMODEL,BLOCK_N,logit_cap)
  _sparse_fwd_kernel_flash_decode_stage2(Q,K,V,sm_scale,Req_to_tokens,Topk_token_indices,Mid_O,Mid_O_LogExpSum,Heavy_token_num,stride_req_to_tokens_b,stride_topk_token_indices_h,stride_topk_token_indices_b,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_o_eb,stride_mid_o_eh,gqa_group_size,BLOCK_SEQ,BLOCK_DMODEL,BLOCK_N)
  _sparse_fwd_kernel_flash_decode_stage3(Mid_O,Mid_O_LogExpSum,O,seq_len,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_o_eb,stride_mid_o_eh,stride_obs,stride_oh,BLOCK_SEQ,BLOCK_DMODEL)
  extend_attention_fwd(q_extend,k_extend,v_extend,o_extend,k_buffer,v_buffer,req_to_tokens,b_req_idx,b_seq_len,b_seq_len_extend,b_start_loc_extend,max_len_extend,sm_scale,logit_cap)
  flash_decode_attention_fwd(q,k_buffer,v_buffer,o,req_to_token,b_req_idx,b_start_loc,b_seq_len,attn_logits,max_len_in_batch,sm_scale,logit_cap)
  flash_decode_sparse_attention_fwd(q,k_buffer,v_buffer,o,q_label,k_label_buffer,req_to_token,b_seq_len,max_len_in_batch,sm_scale,logit_cap,heavy_token_num,att_out_approx,mid_out,mid_o_logexpsum,BLOCK_SEQ)
  flash_decode_stage1(q,k,v,Req_to_tokens,B_req_idx,B_Seqlen,max_len_in_batch,mid_out,mid_out_logsumexp,block_seq)
  flash_decode_stage2(mid_out,mid_out_logexpsum,B_Seqlen,O,block_seq)
  sparse_flash_decode_stage1(q_label,k_label_buffer,att_out,Req_to_tokens,B_Seqlen,max_len_in_batch,sm_scale,logit_cap)
  sparse_flash_decode_stage2(q,k,v,Req_to_tokens,Topk_token_indices,heavy_token_num,mid_out,mid_out_logsumexp,block_seq,sm_scale)
  sparse_flash_decode_stage3(Seqlen,mid_out,mid_out_logexpsum,O,block_seq)
  tanh(x)
layers/attention/triton_ops/extend_attention.py:
  _fwd_kernel(Q_Extend,K_Extend,V_Extend,O_Extend,K_Buffer,V_Buffer,qo_indptr,kv_indptr,kv_indices,mask_ptr,mask_indptr,sink_ptr,window_kv_offset_ptr,sm_scale,kv_group_num,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,SLIDING_WINDOW_SIZE,logit_cap,xai_temperature_len,Lq,Lv,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_M,BLOCK_N,USE_CUSTOM_MASK,IS_CAUSAL,SKIP_PREFIX_CUSTOM_MASK,STORE_TRANSPOSE,HAS_SINK)
  extend_attention_fwd(q_extend,k_extend,v_extend,o_extend,k_buffer,v_buffer,qo_indptr,kv_indptr,kv_indices,custom_mask,is_causal,mask_indptr,max_len_extend,sm_scale,logit_cap,skip_prefix_custom_mask,sliding_window_size,sinks,window_kv_offsets,xai_temperature_len)
  redundant_attention(q_extend,o_extend,k_buffer,v_buffer,b_req_idx,b_start_loc,b_seq_len,b_seq_len_prefix,max_len_in_batch)
  tanh(x)
layers/attention/triton_ops/merge_state.py:
  merge_state_kernel(output,output_lse,prefix_output,prefix_lse,suffix_output,suffix_lse,HEAD_SIZE,PADDED_HEAD_SIZE,OUTPUT_LSE)
  merge_state_triton(prefix_output,prefix_lse,suffix_output,suffix_lse,output,output_lse)
layers/attention/triton_ops/prefill_attention.py:
  _fwd_kernel(Q,K,V,sm_scale,B_Start_Loc,B_Seqlen,Out,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,kv_group_num,BLOCK_M,BLOCK_DMODEL,BLOCK_N,IS_CAUSAL,Lk)
  context_attention_fwd(q,k,v,o,b_start_loc,b_seq_len,max_input_len,is_causal)
layers/attention/triton_ops/rocm_mla_decode_rope.py:
  _decode_grouped_att_m_fwd_rope(q,k_buffer,v_buffer,att_out,k_pe_tokens_out,kv_lora_rank,cos_sin_cache,positions,rotary_dim,kv_indptr,kv_indices,num_kv_splits,sm_scale,logit_cap,use_rope,is_neox_style)
  _fwd_grouped_kernel_stage1_rope(Q,K_Buffer,V_buffer,cos_sin_cache,positions,sm_scale,kv_indptr,kv_indices,Att_Out,k_pe_t_out,stride_qb,stride_qh,stride_buf_kbs,stride_buf_vbs,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_kpe_tokens_out_b,stride_cos_sin_cache_s,stride_positions_b,rotary_dim,kv_lora_rank,qk_rope_head_dim,kv_group_num,q_head_num,BLOCK_C,BLOCK_R,BLOCK_N,BLOCK_H,NUM_KV_SPLITS,logit_cap,USE_ROPE,IS_NEOX_STYLE)
  decode_attention_fwd_grouped_rope(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,k_pe_tokens,kv_lora_rank,rotary_dim,cos_sin_cache,positions,attn_logits,num_kv_splits,sm_scale,logit_cap,use_rope,is_neox_style)
  is_hip()
  tanh(x)
layers/attention/trtllm_mha_backend.py:
  TRTLLMHAAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf,speculative_step_id)
  TRTLLMHAAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  TRTLLMHAAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  TRTLLMHAAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TRTLLMHAAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TRTLLMHAAttnBackend.init_forward_metadata(self,forward_batch)
  TRTLLMHAAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TRTLLMHAAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TRTLLMHAAttnMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  TRTLLMHAAttnMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
layers/attention/trtllm_mla_backend.py:
  TRTLLMMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,q_indptr_decode_buf)
  TRTLLMMLABackend._calc_padded_blocks(self,max_seq_len)
  TRTLLMMLABackend._create_block_kv_indices(self,batch_size,max_blocks,req_pool_indices,seq_lens,device)
  TRTLLMMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,cos_sin_cache,is_neox)
  TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value(self)
  TRTLLMMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TRTLLMMLABackend.init_forward_metadata(self,forward_batch)
  TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TRTLLMMLABackend.quantize_and_rope_for_fp8(self,q_nope,q_rope,k_nope,k_rope,forward_batch,cos_sin_cache,is_neox)
  TRTLLMMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
layers/attention/utils.py:
  create_flashinfer_kv_indices_triton(req_to_token_ptr,req_pool_indices_ptr,page_kernel_lens_ptr,kv_indptr,kv_start_idx,kv_indices_ptr,req_to_token_ptr_stride)
  create_flashmla_kv_indices_triton(req_to_token_ptr,req_pool_indices_ptr,page_kernel_lens_ptr,kv_start_idx,kv_indices_ptr,req_to_token_ptr_stride,kv_indices_ptr_stride,NUM_PAGE_PER_BLOCK,PAGED_SIZE)
layers/attention/vision.py:
  SingletonCache.empty(self)
  SingletonCache.get_data(self)
  SingletonCache.set_data(self,value)
  VisionAttention.__init__(self,embed_dim,num_heads,projection_size,use_qkv_parallel,qkv_backend,quant_config,dropout,softmax_in_single_precision,flatten_batch,prefix,proj_bias,num_dummy_heads,qkv_bias,qk_normalization,layer_norm_eps,customized_position_embedding_applier,**kwargs)
  VisionAttention._apply_qk_norm(self,q,k)
  VisionAttention._determine_attention_backend(self,passed_backend)
  VisionAttention.forward(self,x,cu_seqlens,position_embeddings,attention_mask,**kwargs)
  VisionFlash3Attention.__init__(self,**kwargs)
  VisionFlash3Attention.forward(self,q,k,v,cu_seqlens,bsz,seq_len,**kwargs)
  VisionSdpaAttention.__init__(self,head_dim,num_heads,num_kv_heads,dropout,flatten_batch,softmax_in_single_precision,**kwargs)
  VisionSdpaAttention._generate_mask_cache(s,flatten_batch,cu_seqlens)
  VisionSdpaAttention.forward(self,q,k,v,bsz,cu_seqlens,attention_mask,**kwargs)
  VisionSdpaAttention.generate_patch_attention_mask(self,s,cu_seqlens,flatten_batch)
  VisionTritonAttention.__init__(self,**kwargs)
  VisionTritonAttention.forward(self,q,k,v,cu_seqlens,bsz,seq_len,**kwargs)
  _get_cu_seqlens_for_shape(batch_size,seqlen,device)
layers/attention/vision_utils.py:
  pad_vit_attn_dummy_heads(config,name,loaded_weight)
  update_vit_attn_dummy_heads_config(config)
layers/attention/wave_backend.py:
  WaveAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  WaveAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  WaveAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  WaveAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  WaveAttnBackend.get_num_kv_splits(self,num_kv_splits,seq_lens)
  WaveAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  WaveAttnBackend.init_forward_metadata(self,forward_batch)
  WaveAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  WaveAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  get_num_kv_splits_triton(num_kv_splits_ptr,seq_lens_ptr,num_seq,num_group,num_head,num_kv_head,max_kv_splits,device_core_count,MAX_NUM_SEQ)
layers/attention/wave_ops/decode_attention.py:
  decode_attention_fwd(q,k_buffer,v_buffer,o,b_req_idx,req_to_token,attn_logits,attn_logits_max,num_kv_splits,max_kv_splits,sm_scale,logit_cap)
  decode_attention_intermediate_arrays_shapes(num_seqs,head_size_kv,num_query_heads,max_kv_splits)
  decode_attention_wave(q,k_buffer,v_buffer,o,b_req_idx,req_to_token,attn_logits,attn_logits_max,num_kv_splits,max_kv_splits,sm_scale,logit_cap)
  get_wave_kernel(shape,max_kv_splits,input_dtype,output_dtype,logit_cap)
layers/attention/wave_ops/extend_attention.py:
  extend_attention_wave(q_extend,k_extend,v_extend,k_buffer,v_buffer,qo_indptr,kv_indptr,kv_indices,custom_mask,mask_indptr,max_seq_len,output,is_causal,layer_scaling,logit_cap)
  get_wave_kernel(shape,q_shape,k_shape,v_shape,k_cache_shape,v_cache_shape,o_shape,input_dtype,output_dtype,size_dtype,is_causal,logit_cap,layer_scaling)
layers/attention/wave_ops/prefill_attention.py:
  prefill_attention_wave(q,k,v,o,b_start_loc,b_seq_len,max_seq_len,is_causal)
layers/communicator.py:
  CommunicateContext.init_new(cls)
  CommunicateContext.is_same_group_size(self,a,b)
  CommunicateSimpleFn._scattered_to_tp_attn_full(hidden_states,forward_batch,context)
  CommunicateSimpleFn._trivial(hidden_states,forward_batch,context)
  CommunicateSimpleFn.get_fn(input_mode,output_mode,context)
  CommunicateSummableTensorPairFn._gather(hidden_states,residual,forward_batch,context,**kwargs)
  CommunicateSummableTensorPairFn._scatter(hidden_states,residual,forward_batch,context)
  CommunicateSummableTensorPairFn._scatter_hidden_states(hidden_states,residual,forward_batch,context,allow_reduce_scatter)
  CommunicateSummableTensorPairFn._trivial(hidden_states,residual,forward_batch,context,**kwargs)
  CommunicateSummableTensorPairFn.execute(cls,hidden_states_input_mode,residual_input_mode,output_mode,context,**kwargs)
  CommunicateSummableTensorPairFn.get_fn(hidden_states_input_mode,residual_input_mode,output_mode,context)
  CommunicateWithAllReduceAndLayerNormFn._gather_hidden_states_and_residual(hidden_states,residual,forward_batch,layernorm,context,residual_input_mode)
  CommunicateWithAllReduceAndLayerNormFn._scatter_hidden_states_and_residual(hidden_states,residual,forward_batch,layernorm,context,residual_input_mode)
  CommunicateWithAllReduceAndLayerNormFn._simple(hidden_states,residual,forward_batch,layernorm,context)
  CommunicateWithAllReduceAndLayerNormFn.get_fn(hidden_states_input_mode,residual_input_mode,hidden_states_output_mode,residual_output_mode,context)
  LayerCommunicator.__init__(self,layer_scatter_modes,input_layernorm,post_attention_layernorm,allow_reduce_scatter,is_last_layer)
  LayerCommunicator.postprocess_layer(self,hidden_states,residual,forward_batch)
  LayerCommunicator.prepare_attn(self,hidden_states,residual,forward_batch)
  LayerCommunicator.prepare_mlp(self,hidden_states,residual,forward_batch)
  LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer(self,forward_batch)
  LayerCommunicator.should_use_reduce_scatter(self,forward_batch)
  LayerScatterModes._compute_layer_input_mode(cls,context)
  LayerScatterModes._compute_layer_output_mode(cls,context)
  LayerScatterModes._compute_middle_residual_mode(cls,context)
  LayerScatterModes._compute_mlp_mode(cls,context)
  LayerScatterModes.init_new(cls,**kwargs)
  ScatterMode.model_input_output()
  _LayerModeComputationContext.previous_layer(self)
  enable_moe_dense_fully_dp()
layers/dp_attention.py:
  DpPaddingMode.get_default_mode_in_cuda_graph(cls)
  DpPaddingMode.get_dp_padding_mode(cls,global_num_tokens)
  DpPaddingMode.is_max_len(self)
  DpPaddingMode.is_sum_len(self)
  _DpGatheredBufferWrapper.get_dp_global_num_tokens(cls)
  _DpGatheredBufferWrapper.get_global_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_global_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.set_dp_buffer_len(cls,global_dp_buffer_len,local_dp_buffer_len,global_num_tokens)
  _DpGatheredBufferWrapper.set_metadata(cls,hidden_size,dtype,device)
  _dp_gather(global_tokens,local_tokens,forward_batch,is_partial)
  _dp_gather_via_all_gather(global_tokens,local_tokens,forward_batch,is_partial)
  _dp_gather_via_all_reduce(global_tokens,local_tokens,forward_batch,is_partial)
  attn_tp_all_gather(output_list,input)
  attn_tp_all_gather_into_tensor(output,input)
  attn_tp_reduce_scatter_tensor(output,input)
  compute_dp_attention_local_info(enable_dp_attention,tp_rank,tp_size,dp_size,moe_dense_tp_size)
  compute_dp_attention_world_info(enable_dp_attention,tp_rank,tp_size,dp_size)
  disable_dp_size()
  dp_gather_partial(global_tokens,local_tokens,forward_batch)
  dp_gather_replicate(global_tokens,local_tokens,forward_batch)
  dp_reduce_scatter_tensor(output,input)
  dp_scatter(local_tokens,global_tokens,forward_batch)
  get_attention_dp_rank()
  get_attention_dp_size()
  get_attention_tp_group()
  get_attention_tp_rank()
  get_attention_tp_size()
  get_dp_global_num_tokens()
  get_dp_local_info(forward_batch)
  get_global_dp_buffer()
  get_global_dp_buffer_len()
  get_local_attention_dp_rank()
  get_local_attention_dp_size()
  get_local_dp_buffer()
  get_local_dp_buffer_len()
  initialize_dp_attention(server_args,model_config)
  is_dp_attention_enabled()
  memcpy_triton(dst,src,dim,offset,sz,offset_src)
  memcpy_triton_kernel(dst_ptr,src_ptr,offset_ptr,sz_ptr,offset_src,chunk_size,BLOCK_SIZE)
  prod(x)
  set_dp_buffer_len(global_dp_buffer_len,local_dp_buffer_len,global_num_tokens)
layers/elementwise.py:
  FusedDualResidualRMSNorm.__call__(self,*args,**kwargs)
  FusedDualResidualRMSNorm.__init__(self,rmsnorm1,rmsnorm2)
  FusedDualResidualRMSNorm.forward(self,x,residual)
  FusedDualResidualRMSNorm.forward_cuda(self,x,residual,autotune)
  FusedDualResidualRMSNorm.forward_flashinfer(self,x,residual)
  FusedDualResidualRMSNorm.forward_native(self,x,residual)
  Softcap.__call__(self,*args,**kwargs)
  Softcap.__init__(self,softcap_const)
  Softcap.forward(self,x)
  Softcap.forward_cuda(self,x,autotune)
  Softcap.forward_native(self,x)
  experts_combine_kernel(out_hidden_states,moe_hidden_states,mlp_hidden_states,combine_k,hidden_dim,BLOCK_SIZE)
  experts_combine_triton(moe_hidden_states,mlp_hidden_states,output_buffer)
  fused_dual_residual_rmsnorm(x,residual,weight1,weight2,eps,autotune)
  fused_dual_residual_rmsnorm_kernel(output_ptr,mid_ptr,activ_ptr,residual_ptr,weight1_ptr,weight2_ptr,eps,hidden_dim,BLOCK_SIZE)
  fused_rmsnorm(x,weight,eps,autotune,inplace)
  fused_rmsnorm_kernel(output_ptr,activ_ptr,weight_ptr,eps,hidden_dim,BLOCK_SIZE)
  fused_softcap(x,softcap_const,autotune)
  fused_softcap_kernel(output_ptr,input_ptr,n_ele,softcap_const,BLOCK_SIZE)
  gelu_and_mul_kernel(out_hidden_states_ptr,out_scales_ptr,hidden_states_ptr,quant_max,static_scale,hidden_dim,BLOCK_SIZE)
  gelu_and_mul_triton(hidden_states,scales,quantize,out)
  silu_and_mul_kernel(out_hidden_states_ptr,out_scales_ptr,hidden_states_ptr,quant_max,static_scale,hidden_dim,BLOCK_SIZE)
  silu_and_mul_triton(hidden_states,scales,quantize,out)
layers/flashinfer_comm_fusion.py:
  FlashInferWorkspaceManager.__init__(self)
  FlashInferWorkspaceManager.cleanup(self)
  FlashInferWorkspaceManager.initialize(self,world_size,rank,max_token_num,hidden_dim,group,use_fp32_lamport)
  cleanup_flashinfer_workspace()
  ensure_workspace_initialized(max_token_num,hidden_dim,use_fp32_lamport)
  fake_flashinfer_allreduce_residual_rmsnorm(input_tensor,residual,weight,eps,max_token_num,use_oneshot,trigger_completion_at_end,fp32_acc)
  flashinfer_allreduce_residual_rmsnorm(input_tensor,residual,weight,eps,max_token_num,use_oneshot,trigger_completion_at_end,fp32_acc)
layers/layernorm.py:
  Gemma3RMSNorm.__init__(self,dim,eps)
  Gemma3RMSNorm._norm(self,x)
  Gemma3RMSNorm.extra_repr(self)
  Gemma3RMSNorm.forward_cuda(self,x)
  Gemma3RMSNorm.forward_native(self,x)
  Gemma3RMSNorm.forward_npu(self,x)
  GemmaRMSNorm.__init__(self,hidden_size,eps)
  GemmaRMSNorm.forward_cuda(self,x,residual)
  GemmaRMSNorm.forward_native(self,x,residual)
  GemmaRMSNorm.forward_npu(self,x,residual)
  RMSNorm.__init__(self,hidden_size,eps,var_hidden_size)
  RMSNorm.forward_aiter(self,x,residual)
  RMSNorm.forward_cpu(self,x,residual)
  RMSNorm.forward_cuda(self,x,residual)
  RMSNorm.forward_hip(self,x,residual)
  RMSNorm.forward_native(self,x,residual)
  RMSNorm.forward_npu(self,x,residual)
  RMSNorm.forward_with_allreduce_fusion(self,x,residual)
layers/linear.py:
  ColumnParallelLinear.__init__(self,input_size,output_size,bias,gather_output,skip_bias_add,params_dtype,quant_config,output_sizes,prefix,tp_rank,tp_size,use_presharded_weights)
  ColumnParallelLinear.extra_repr(self)
  ColumnParallelLinear.forward(self,input_)
  ColumnParallelLinear.weight_loader(self,param,loaded_weight)
  ColumnParallelLinear.weight_loader_v2(self,param,loaded_weight)
  LinearBase.__init__(self,input_size,output_size,skip_bias_add,params_dtype,quant_config,prefix)
  LinearBase.forward(self,x)
  MergedColumnParallelLinear.__init__(self,input_size,output_sizes,bias,gather_output,skip_bias_add,params_dtype,quant_config,prefix,tp_rank,tp_size,use_presharded_weights)
  MergedColumnParallelLinear._load_fused_module_from_checkpoint(self,param,loaded_weight)
  MergedColumnParallelLinear.weight_loader(self,param,loaded_weight,loaded_shard_id)
  MergedColumnParallelLinear.weight_loader_v2(self,param,loaded_weight,loaded_shard_id)
  QKVParallelLinear.__init__(self,hidden_size,head_size,total_num_heads,total_num_kv_heads,bias,skip_bias_add,params_dtype,quant_config,prefix,tp_rank,tp_size,load_presharded_attn)
  QKVParallelLinear._get_shard_offset_mapping(self,loaded_shard_id)
  QKVParallelLinear._get_shard_size_mapping(self,loaded_shard_id)
  QKVParallelLinear._load_fused_module_from_checkpoint(self,param,loaded_weight)
  QKVParallelLinear.weight_loader(self,param,loaded_weight,loaded_shard_id)
  QKVParallelLinear.weight_loader_v2(self,param,loaded_weight,loaded_shard_id)
  ReplicatedLinear.__init__(self,input_size,output_size,bias,skip_bias_add,params_dtype,quant_config,prefix)
  ReplicatedLinear.extra_repr(self)
  ReplicatedLinear.forward(self,x)
  ReplicatedLinear.weight_loader(self,param,loaded_weight)
  RowParallelLinear.__init__(self,input_size,output_size,bias,input_is_parallel,skip_bias_add,params_dtype,reduce_results,quant_config,prefix,tp_rank,tp_size,use_presharded_weights)
  RowParallelLinear.extra_repr(self)
  RowParallelLinear.forward(self,input_,skip_all_reduce)
  RowParallelLinear.weight_loader(self,param,loaded_weight)
  RowParallelLinear.weight_loader_v2(self,param,loaded_weight)
  adjust_bitsandbytes_4bit_shard(param,shard_offsets,loaded_shard_id)
  adjust_marlin_shard(param,shard_size,shard_offset)
  adjust_scalar_to_fused_array(param,loaded_weight,shard_id)
  adjust_shard_offsets(shard_offsets,loaded_weight,dim)
layers/logits_processor.py:
  LogitsMetadata.compute_dp_attention_metadata(self)
  LogitsMetadata.from_forward_batch(cls,forward_batch)
  LogitsProcessor.__init__(self,config,skip_all_gather,logit_scale)
  LogitsProcessor._get_logits(self,hidden_states,lm_head,logits_metadata,embedding_bias)
  LogitsProcessor.compute_temp_top_p_normalized_logprobs(last_logits,logits_metadata)
  LogitsProcessor.forward(self,input_ids,hidden_states,lm_head,logits_metadata,aux_hidden_states)
  LogitsProcessor.get_token_ids_logprobs(all_logprobs,logits_metadata)
  LogitsProcessor.get_top_logprobs(all_logprobs,logits_metadata)
  fused_softcap(full_logits,final_logit_softcapping)
  fused_softcap_kernel(full_logits_ptr,softcapping_value,n_elements,BLOCK_SIZE)
layers/moe/cutlass_moe.py:
  cutlass_fused_experts_fp8(a,w1_q,w2_q,w1_scale,w2_scale,topk_weights,topk_ids,a1_strides,c1_strides,a2_strides,c2_strides,workspace,a_ptrs,b_ptrs,out_ptrs,a_scales_ptrs,b_scales_ptrs,expert_offsets,problem_sizes1,problem_sizes2,use_fp8_blockscale)
  cutlass_moe_fp4(a,a1_gscale,w1_fp4,w1_blockscale,w1_alphas,a2_gscale,w2_fp4,w2_blockscale,w2_alphas,topk_weights,topk_ids,params,apply_router_weight_on_input)
layers/moe/cutlass_moe_params.py:
  CutlassMoEParams.__init__(self,cutlass_moe_type,device,num_experts,intermediate_size_per_partition,hidden_size)
  CutlassMoEParams.to_gemm1_args(self)
  CutlassMoEParams.to_gemm2_args(self)
layers/moe/cutlass_w4a8_moe.py:
  cutlass_w4a8_moe(start_expert_id,end_expert_id,total_num_experts,a,w1_q,w2_q,w1_scale,w2_scale,topk_weights,topk_ids_,local_topk_ids,a_strides1,b_strides1,c_strides1,a_strides2,b_strides2,c_strides2,s_strides13,s_strides2,expert_offsets,problem_sizes1,problem_sizes2,a1_scale,a2_scale,apply_router_weight_on_input)
layers/moe/ep_moe/kernels.py:
  _fwd_kernel_ep_gather(total_token_num,input_tensor,input_tensor_stride0,input_tensor_stride1,recv_topk_ids,recv_topk_ids_stride0,recv_topk_ids_stride1,recv_topk_weight,recv_topk_weight_stride0,recv_topk_weight_stride1,input_index,input_index_stride0,input_index_stride1,output_tensor,output_tensor_stride0,output_tensor_stride1,topk_num,BLOCK_D)
  _fwd_kernel_ep_scatter_1(num_recv_tokens_per_expert,expert_start_loc,m_indices,num_experts,BLOCK_E,BLOCK_EXPERT_NUM)
  _fwd_kernel_ep_scatter_2(total_token_num,expert_start_loc,recv_x,recv_x_stride0,recv_x_stride1,recv_x_scale,recv_x_scale_stride0,recv_x_scale_stride1,recv_topk,recv_topk_stride0,recv_topk_stride1,output_tensor,output_tensor_stride0,output_tensor_stride1,output_tensor_scale,output_tensor_scale_stride0,output_tensor_scale_stride1,output_index,output_index_stride0,output_index_stride1,topk_num,HIDDEN_SIZE,HIDDEN_SIZE_PAD,SCALE_HIDDEN_SIZE,SCALE_HIDDEN_SIZE_PAD)
  _silu_and_mul_post_quant_kernel(input_ptr,stride_input_0,stride_input_1,stride_input_2,output_ptr,stride_output_0,stride_output_1,stride_output_2,output_scale_ptr,stride_output_scale_0,stride_output_scale_1,stride_output_scale_2,masked_m_ptr,size_n,fp8_max,fp8_min,BLOCK_N,NUM_STAGE,SCALE_UE8M0)
  _tma_align_input_scale_kernel(input_scale_ptr,output_ptr,m,k_div_block_size,input_scale_stride_m,input_scale_stride_k,output_stride_m,output_stride_k,BLOCK_SIZE_K)
  compute_identity_kernel(top_k,hidden_states_ptr,expert_scales_ptr,num_tokens,output_ptr,hidden_dim,scales_stride,BLOCK_SIZE)
  compute_m_num_tiles_indptr(m_num_tiles_indptr,seg_indptr,batch_size,BLOCK_SIZE_M)
  compute_m_range(pid,batch_size,seg_indptr,weight_indices,m_num_tiles_indptr,BLOCK_SIZE_M)
  compute_masked_m_triton_kernel(seg_indptr,masked_m)
  compute_seg_indptr_triton_kernel(reorder_topk_ids,seg_indptr,num_toks)
  compute_src2dst_triton_kernel(reorder_ids,src2dst,num_toks,BLOCK_SIZE)
  deepep_compute_src2dst_triton_kernel(reorder_ids,src2dst,num_toks,num_minus_one,BLOCK_SIZE)
  deepep_permute_triton_kernel(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,topk,hidden_size,BLOCK_SIZE)
  deepep_post_reorder_triton_kernel(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,topk,hidden_size,BLOCK_SIZE)
  deepep_run_moe_deep_preprocess(topk_ids,num_experts)
  deepgemm_compute_src2dst_triton_kernel(topk_ids,reorder_ids,seg_indptr,src2dst,m_max,num_toks,BLOCK_SIZE)
  ep_gather(input_tensor,recv_topk_ids,recv_topk_weight,input_index,output_tensor)
  ep_scatter(recv_x,recv_x_scale,recv_topk,num_recv_tokens_per_expert,expert_start_loc,output_tensor,output_tensor_scale,m_indices,output_index,scale_ue8m0)
  fill_gateup_input_triton_kernel(input_ptr,scale_ptr,gateup_input_ptr,gateup_input_scale_ptr,src2dst_ptr,topk_ids_ptr,start_expert_id,end_expert_id,topk,m_max,hidden_size,scale_size,BLOCK_SIZE)
  gelu_and_mul_triton_kernel(gateup_output,down_input,hidden_size,reorder_topk_ids,scales,start_expert_id,end_expert_id,BLOCK_SIZE)
  get_tma_aligned_size(x,element_size)
  grouped_gemm_triton(a,b,c,batch_size,weight_column_major,seg_indptr,weight_indices,use_fp8_w8a8,scale_a,scale_b,block_shape,c_dtype,use_per_token_if_dynamic)
  grouped_gemm_triton_kernel(a,b,c,batch_size,N,K,seg_indptr,weight_indices,m_num_tiles_indptr,scale_a,scale_b,use_fp8_w8a8,group_n,group_k,a_stride_0,b_stride_0,b_stride_1,as_stride_0,as_stride_1,bs_stride_0,bs_stride_2,bs_stride_1,use_per_token_if_dynamic,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K)
  moe_ep_deepgemm_preprocess(topk_ids,num_experts,hidden_states,top_k,start_expert_id,end_expert_id,block_shape,output_dtype)
  post_reorder_triton_kernel(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,start_expert_id,end_expert_id,topk,hidden_size,dst_start,BLOCK_SIZE)
  post_reorder_triton_kernel_for_cutlass_moe(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,num_experts,topk,hidden_size,dst_start,BLOCK_SIZE)
  pre_reorder_triton_kernel(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,start_expert_id,end_expert_id,topk,hidden_size,BLOCK_SIZE,use_per_token_if_dynamic)
  pre_reorder_triton_kernel_for_cutlass_moe(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,num_experts,topk,hidden_size,BLOCK_SIZE)
  run_cutlass_moe_ep_preproess(local_topk_ids,local_num_experts)
  run_moe_ep_preproess(topk_ids,num_experts)
  silu_and_mul_masked_post_quant_fwd(input,output,output_scale,quant_group_size,masked_m,scale_ue8m0)
  silu_and_mul_triton_kernel(gateup_output,down_input,hidden_size,reorder_topk_ids,scales,start_expert_id,end_expert_id,BLOCK_SIZE)
  tanh(x)
  tma_align_input_scale(input_scale)
  zero_experts_compute_triton(expert_indices,expert_scales,num_experts,zero_expert_type,hidden_states)
layers/moe/ep_moe/layer.py:
  DeepEPMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,num_fused_shared_experts,params_dtype,quant_config,prefix,activation,routed_scaling_factor)
  DeepEPMoE.combine(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.dispatch(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.forward(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.forward_aiter(self,dispatch_output)
  DeepEPMoE.forward_deepgemm_contiguous(self,dispatch_output)
  DeepEPMoE.forward_deepgemm_masked(self,dispatch_output)
  DeepEPMoE.forward_npu(self,dispatch_output)
  DeepEPMoE.moe_impl(self,dispatch_output)
  EPMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,num_fused_shared_experts,params_dtype,quant_config,prefix,activation,routed_scaling_factor,gemm1_alpha,gemm1_clamp_limit,with_bias)
  EPMoE.forward(self,hidden_states,topk_output)
  EPMoE.forward_deepgemm(self,hidden_states,topk_output)
  _cast_to_e8m0_with_rounding_up(x)
  get_moe_impl_class(quant_config)
layers/moe/fused_moe_native.py:
  fused_moe_forward_native(layer,x,topk_output,moe_runner_config)
  moe_forward_native(layer,x,topk_output,moe_runner_config)
layers/moe/fused_moe_triton/__init__.py:
  get_config()
  override_config(config)
layers/moe/fused_moe_triton/fused_moe.py:
  _moe_sum_reduce_kernel(input_ptr,input_stride_0,input_stride_1,input_stride_2,output_ptr,output_stride_0,output_stride_1,token_num,topk_num,hidden_dim,routed_scaling_factor,BLOCK_M,BLOCK_DIM,NUM_STAGE)
  fused_experts(hidden_states,w1,w2,topk_output,moe_runner_config,b1,b2,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape)
  fused_experts_impl(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,inplace,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  fused_moe(hidden_states,w1,w2,topk_output,moe_runner_config,b1,b2,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape)
  fused_moe_kernel(a_ptr,b_ptr,bias_ptr,c_ptr,a_scale_ptr,b_scale_ptr,topk_weights_ptr,sorted_token_ids_ptr,expert_ids_ptr,num_tokens_post_padded_ptr,N,K,EM,num_valid_tokens,stride_am,stride_ak,stride_be,stride_bk,stride_bn,stride_bias_e,stride_bias_n,stride_cm,stride_cn,stride_asm,stride_ask,stride_bse,stride_bsk,stride_bsn,group_n,group_k,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M,MUL_ROUTED_WEIGHT,top_k,compute_type,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,per_channel_quant,even_Ks)
  fused_moe_kernel_gptq_awq(a_ptr,b_ptr,c_ptr,b_scale_ptr,b_zp_ptr,topk_weights_ptr,sorted_token_ids_ptr,expert_ids_ptr,num_tokens_post_padded_ptr,N,K,EM,num_valid_tokens,stride_am,stride_ak,stride_be,stride_bk,stride_bn,stride_cm,stride_cn,stride_bse,stride_bsk,stride_bsn,stride_bze,stride_bzk,stride_bzn,group_size,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M,MUL_ROUTED_WEIGHT,top_k,compute_type,has_zp,use_int4_w4a16,use_int8_w8a16,even_Ks)
  get_config_dtype_str(dtype,use_int8_w8a16,use_int4_w4a16,use_fp8_w8a8,use_int8_w8a8)
  get_config_file_name(E,N,dtype,block_shape)
  get_default_config(M,E,N,K,topk,dtype,is_marlin,block_shape)
  get_moe_configs(E,N,dtype,block_n,block_k)
  inplace_fused_experts(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  inplace_fused_experts_fake(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  invoke_fused_moe_kernel(A,B,bias,C,A_scale,B_scale,B_zp,topk_weights,topk_ids,sorted_token_ids,expert_ids,num_tokens_post_padded,mul_routed_weight,top_k,config,compute_type,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,block_shape,no_combine)
  moe_align_block_size(topk_ids,block_size,num_experts)
  moe_sum_reduce_torch_compile(x,out,routed_scaling_factor)
  moe_sum_reduce_triton(input,output,routed_scaling_factor)
  outplace_fused_experts(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  outplace_fused_experts_fake(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  swiglu_with_alpha_and_limit(x,gemm1_alpha,gemm1_limit)
  try_get_optimal_moe_config(w1_shape,w2_shape,top_k,dtype,M,is_marlin,block_shape)
  write_zeros_to_output(c_ptr,stride_cm,stride_cn,pid_n,N,offs_token,token_mask,BLOCK_SIZE_M,BLOCK_SIZE_N,compute_type)
layers/moe/fused_moe_triton/layer.py:
  FlashInferFP4MoE.__init__(self,*args,**kwargs)
  FlashInferFP4MoE._quantize_hidden_states_fp4(self,hidden_states)
  FlashInferFP4MoE.forward(self,hidden_states,topk_output)
  FlashInferFusedMoE.__init__(self,*args,**kwargs)
  FlashInferFusedMoE.forward(self,hidden_states,topk_output)
  FusedMoE.__init__(self,num_experts,hidden_size,intermediate_size,layer_id,top_k,num_fused_shared_experts,params_dtype,reduce_results,quant_config,prefix,activation,apply_router_weight_on_input,use_presharded_weights,inplace,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_clamp_limit,use_weight_loader_fused,with_bias)
  FusedMoE._load_g_idx(self,shard_id,expert_data,shard_dim,loaded_weight,tp_rank)
  FusedMoE._load_model_weight_or_group_weight_scale(self,shard_dim,expert_data,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._load_per_channel_weight_scale(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank)
  FusedMoE._load_per_tensor_weight_scale(self,shard_id,param,loaded_weight,expert_id)
  FusedMoE._load_single_value(self,param,loaded_weight,expert_id)
  FusedMoE._load_w13(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._load_w2(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._map_global_expert_id_to_local_expert_id(self,expert_id)
  FusedMoE._weight_loader_impl(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE._weight_loader_physical(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE.forward(self,hidden_states,topk_output)
  FusedMoE.make_expert_input_scale_params_mapping(cls,num_experts)
  FusedMoE.make_expert_params_mapping(cls,ckpt_gate_proj_name,ckpt_down_proj_name,ckpt_up_proj_name,num_experts)
  FusedMoE.make_expert_params_mapping_fused(cls,ckpt_gate_up_proj_name,ckpt_down_proj_name,ckpt_gate_up_proj_bias_name,ckpt_down_proj_bias_name)
  FusedMoE.make_expert_params_mapping_fused_mxfp4(cls,ckpt_gate_up_proj_name,ckpt_down_proj_name,ckpt_gate_up_proj_bias_name,ckpt_down_proj_bias_name,ckpt_gate_up_proj_scale_name,ckpt_down_proj_scale_name)
  FusedMoE.should_fuse_routed_scaling_factor_in_topk(self)
  FusedMoE.weight_loader(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE.weight_loader_fused(self,param,loaded_weight,weight_name,shard_id)
  _get_tile_tokens_dim(num_tokens,top_k,num_experts)
  _is_fp4_quantization_enabled()
  get_fused_moe_impl_class()
layers/moe/fused_moe_triton/triton_kernels_moe.py:
  quantize(w,dtype,dev,**opt)
  triton_kernel_fused_experts(hidden_states,w1,w2,routing_data,gather_indx,scatter_indx,inplace,activation,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
  triton_kernel_fused_experts_with_bias(hidden_states,w1,w1_pcg,b1,w2,w2_pcg,b2,routing_data,gather_indx,scatter_indx,inplace,activation,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape,gemm1_alpha,gemm1_clamp_limit)
  triton_kernel_moe_forward(hidden_states,w1,w2,topk_output,moe_runner_config,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
  triton_kernel_moe_with_bias_forward(hidden_states,w1,w1_pcg,b1,w2,w2_pcg,b2,topk_output,moe_runner_config,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
layers/moe/rocm_moe_utils.py:
  rocm_aiter_asm_moe_tkw1_fake(hidden_states,w1,w2,topk_weights,topk_ids,fc1_scale,fc2_scale,fc1_smooth_scale,fc2_smooth_scale,a16,per_tensor_quant_scale,expert_mask,activation_method)
  rocm_aiter_asm_moe_tkw1_impl(hidden_states,w1,w2,topk_weights,topk_ids,fc1_scale,fc2_scale,fc1_smooth_scale,fc2_smooth_scale,a16,per_tensor_quant_scale,expert_mask,activation_method)
  rocm_fused_experts_tkw1(hidden_states,w1,w2,topk_weights,topk_ids,activation,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
layers/moe/router.py:
  FusedMoeRouter.__call__(self,*args,**kwargs)
  FusedMoeRouter.__init__(self,router_linear,topk,moe_softcapping)
  FusedMoeRouter.forward(self,x,residual)
  FusedMoeRouter.forward_cuda(self,x,autotune)
  FusedMoeRouter.forward_vllm(self,x)
  fused_moe_router_impl(x,router_weight,topk,moe_softcapping,correction_bias)
  fused_moe_router_kernel(input_ptr,moe_router_weight_ptr,topk_weights_ptr,topk_ids_ptr,correction_bias_ptr,is_correction_bias,num_experts,topk,moe_softcapping,moe_renormalize,hidden_dim,BLOCK_SIZE)
  fused_moe_router_large_bs_impl(x,router_weight,topk,moe_softcapping,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K)
  fused_moe_router_large_bs_kernel(a_ptr,b_ptr,topk_weights_ptr,topk_ids_ptr,bs,num_experts,topk,moe_softcapping,moe_renormalize,K,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,stride_am,stride_bn)
  fused_moe_router_shim(moe_softcapping,hidden_states,gating_output,topk,renormalize,correction_bias)
layers/moe/token_dispatcher/base_dispatcher.py:
  BaseDispatcher.combine(self,*args,**kwargs)
  BaseDispatcher.dispatch(self,*args,**kwargs)
  DispatchOutput.format(self)
  DispatchOutputChecker.format_is_ascent_ll(dispatch_output)
  DispatchOutputChecker.format_is_deepep(dispatch_output)
  DispatchOutputChecker.format_is_deepep_ll(dispatch_output)
  DispatchOutputChecker.format_is_deepep_normal(dispatch_output)
  DispatchOutputChecker.format_is_standard(dispatch_output)
  DispatchOutputFormat.is_ascent_ll(self)
  DispatchOutputFormat.is_deepep(self)
  DispatchOutputFormat.is_deepep_ll(self)
  DispatchOutputFormat.is_deepep_normal(self)
  DispatchOutputFormat.is_standard(self)
layers/moe/token_dispatcher/deepep.py:
  AscendDeepEPLLOutput.format(self)
  DeepEPBuffer.clean_buffer(cls)
  DeepEPBuffer.get_deepep_buffer(cls,group,hidden_size,param_bytes,deepep_mode,num_max_dispatch_tokens_per_rank,num_experts)
  DeepEPBuffer.set_dispatch_mode_as_low_latency(cls)
  DeepEPBuffer.set_dispatch_mode_as_normal(cls)
  DeepEPConfig.__init__(self)
  DeepEPConfig.get_instance(cls)
  DeepEPDispatcher.__init__(self,group,router_topk,permute_fusion,num_experts,num_local_experts,hidden_size,params_dtype,deepep_mode,async_finish,return_recv_hook)
  DeepEPDispatcher._get_impl(self,forward_batch)
  DeepEPDispatcher._update_stage(self,old_stage,new_stage)
  DeepEPDispatcher.combine(self,*args,**kwargs)
  DeepEPDispatcher.combine_a(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPDispatcher.combine_b(self)
  DeepEPDispatcher.dispatch(self,*args,**kwargs)
  DeepEPDispatcher.dispatch_a(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPDispatcher.dispatch_b(self)
  DeepEPLLOutput.format(self)
  DeepEPNormalOutput.format(self)
  _DeepEPDispatcherImplBase.__init__(self,group,router_topk,permute_fusion,num_experts,num_local_experts,hidden_size,params_dtype,deepep_mode)
  _DeepEPDispatcherImplBase._get_buffer(self)
  _DeepEPDispatcherImplBase.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplBase.combine_b(self,*args,**kwargs)
  _DeepEPDispatcherImplBase.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplBase.dispatch_b(self,*args,**kwargs)
  _DeepEPDispatcherImplLowLatency.__init__(self,return_recv_hook,**kwargs)
  _DeepEPDispatcherImplLowLatency._combine_core(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency._dispatch_core(self,hidden_states,topk_idx,use_fp8)
  _DeepEPDispatcherImplLowLatency._get_buffer(self)
  _DeepEPDispatcherImplLowLatency.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency.combine_b(self,hidden_states,event,hook)
  _DeepEPDispatcherImplLowLatency.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency.dispatch_b(self,hidden_states,topk_idx,topk_weights,masked_m,expected_m,event,hook)
  _DeepEPDispatcherImplNormal.__init__(self,async_finish,**kwargs)
  _DeepEPDispatcherImplNormal._combine_core(self,x,previous_event)
  _DeepEPDispatcherImplNormal._dispatch_core(self,x,topk_idx,topk_weights,previous_event)
  _DeepEPDispatcherImplNormal._get_buffer(self)
  _DeepEPDispatcherImplNormal.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplNormal.combine_b(self,output,previous_event)
  _DeepEPDispatcherImplNormal.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplNormal.dispatch_b(self,hidden_states,topk_idx,topk_weights,previous_event)
layers/moe/token_dispatcher/standard.py:
  StandardDispatchOutput.format(self)
layers/moe/topk.py:
  BypassedTopKOutput.format(self)
  StandardTopKOutput.format(self)
  TopK.__init__(self,top_k,use_grouped_topk,topk_group,num_expert_group,renormalize,num_fused_shared_experts,custom_routing_function,scoring_func,correction_bias,routed_scaling_factor,apply_routed_scaling_factor_on_output,force_topk)
  TopK.empty_topk_output(self,device)
  TopK.forward_cpu(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_cuda(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_native(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_npu(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopKOutput.format(self)
  TopKOutputChecker.format_is_bypassed(topk_output)
  TopKOutputChecker.format_is_standard(topk_output)
  TopKOutputChecker.format_is_triton_kernel(topk_output)
  TopKOutputFormat.is_bypassed(self)
  TopKOutputFormat.is_standard(self)
  TopKOutputFormat.is_triton_kernel(self)
  TritonKernelTopKOutput.format(self)
  _biased_grouped_topk_postprocess(topk_ids,expert_location_dispatch_info,num_token_non_padded)
  _mask_topk_ids_padded_region(topk_ids,num_token_non_padded)
  apply_topk_weights_cpu(need_apply,topk_weights,inputs)
  biased_grouped_topk_cpu(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,compiled,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  biased_grouped_topk_gpu(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  biased_grouped_topk_impl(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  fused_topk(hidden_states,gating_output,topk,renormalize,num_token_non_padded,expert_location_dispatch_info)
  fused_topk_cpu(hidden_states,gating_output,topk,renormalize,num_token_non_padded,expert_location_dispatch_info,correction_bias)
  fused_topk_torch_native(hidden_states,gating_output,topk,renormalize,correction_bias)
  grouped_topk_cpu(hidden_states,gating_output,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  grouped_topk_gpu(hidden_states,gating_output,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  is_power_of_two(n)
  select_experts(hidden_states,router_logits,topk_config,num_token_non_padded,expert_location_dispatch_info)
layers/moe/utils.py:
  DeepEPMode.enable_low_latency(self)
  DeepEPMode.enable_normal(self)
  DeepEPMode.is_auto(self)
  DeepEPMode.is_low_latency(self)
  DeepEPMode.is_normal(self)
  DeepEPMode.resolve(self,is_extend_in_batch)
  MoeA2ABackend._missing_(cls,value)
  MoeA2ABackend.is_deepep(self)
  MoeA2ABackend.is_none(self)
  MoeRunnerBackend.is_auto(self)
  MoeRunnerBackend.is_flashinfer_cutlass(self)
  MoeRunnerBackend.is_flashinfer_mxfp4(self)
  MoeRunnerBackend.is_flashinfer_trtllm(self)
  MoeRunnerBackend.is_triton(self)
  MoeRunnerBackend.is_triton_kernel(self)
  get_deepep_config()
  get_deepep_mode()
  get_moe_a2a_backend()
  get_moe_runner_backend()
  get_tbo_token_distribution_threshold()
  initialize_moe_config(server_args)
  is_tbo_enabled()
  should_use_flashinfer_cutlass_moe_fp4_allgather()
  should_use_flashinfer_trtllm_moe()
layers/multimodal.py:
  _as_uint32_words(t)
  _final_splitmix64(x)
  _fmix32(x,C1,C2)
  _rotl32(x,r)
  add_tree_reduce_u64_kernel(in_ptr,out_ptr,n_elems,CHUNK)
  gpu_tensor_hash(tensor,seed,tile_words,block_words,reduce_chunk,num_warps,num_stages,use_cg)
  hash_tiles32_kernel_blocked(in_ptr,out_ptr,n_u32,seed1,seed2,FM_C1,FM_C2,POS_A,POS_B,TILE,BLOCK,USE_CG)
layers/parameter.py:
  BasevLLMParameter.__init__(self,data,weight_loader)
  BasevLLMParameter.__new__(cls,data,**kwargs)
  BasevLLMParameter._assert_and_load(self,loaded_weight)
  BasevLLMParameter.load_column_parallel_weight(self,loaded_weight)
  BasevLLMParameter.load_merged_column_weight(self,loaded_weight,**kwargs)
  BasevLLMParameter.load_qkv_weight(self,loaded_weight,**kwargs)
  BasevLLMParameter.load_row_parallel_weight(self,loaded_weight)
  BasevLLMParameter.weight_loader(self)
  PackedColumnParameter.__init__(self,packed_factor,packed_dim,marlin_tile_size,**kwargs)
  PackedColumnParameter.adjust_shard_indexes_for_packing(self,shard_size,shard_offset)
  PackedColumnParameter.marlin_tile_size(self)
  PackedColumnParameter.packed_dim(self)
  PackedColumnParameter.packed_factor(self)
  PackedvLLMParameter.__init__(self,packed_factor,packed_dim,marlin_tile_size,**kwargs)
  PackedvLLMParameter.adjust_shard_indexes_for_packing(self,shard_size,shard_offset)
  PackedvLLMParameter.marlin_tile_size(self)
  PackedvLLMParameter.packed_dim(self)
  PackedvLLMParameter.packed_factor(self)
  PerTensorScaleParameter.__init__(self,**kwargs)
  PerTensorScaleParameter._load_into_shard_id(self,loaded_weight,shard_id,**kwargs)
  PerTensorScaleParameter._shard_id_as_int(self,shard_id)
  PerTensorScaleParameter.load_column_parallel_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_merged_column_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_qkv_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_row_parallel_weight(self,*args,**kwargs)
  RowvLLMParameter.__init__(self,input_dim,**kwargs)
  RowvLLMParameter.input_dim(self)
  RowvLLMParameter.load_row_parallel_weight(self,loaded_weight,tp_rank,use_presharded_weights)
  _ColumnvLLMParameter.__init__(self,output_dim,**kwargs)
  _ColumnvLLMParameter.load_column_parallel_weight(self,loaded_weight,tp_rank,use_presharded_weights)
  _ColumnvLLMParameter.load_merged_column_weight(self,loaded_weight,**kwargs)
  _ColumnvLLMParameter.load_qkv_weight(self,loaded_weight,tp_rank,use_presharded_weights,**kwargs)
  _ColumnvLLMParameter.output_dim(self)
  _adjust_shard_indexes_for_marlin(shard_size,shard_offset,marlin_tile_size)
  _adjust_shard_indexes_for_packing(shard_size,shard_offset,packed_factor,marlin_tile_size)
  permute_param_layout_(param,input_dim,output_dim,**kwargs)
layers/pooler.py:
  CrossEncodingPooler.__init__(self,config,classifier,pooler)
  CrossEncodingPooler.forward(self,hidden_states,forward_batch)
  Pooler.__init__(self,pooling_type,normalize)
  Pooler.forward(self,hidden_states,forward_batch)
layers/quantization/__init__.py:
  DummyConfig.override_quantization_method(self,*args,**kwargs)
  get_quantization_config(quantization)
  monkey_patch_isinstance_for_vllm_base_layer(reverse)
  monkey_patch_moe_apply(class_obj)
  monkey_patch_quant_configs()
  new_apply(self,layer,x,topk_output,activation,apply_router_weight_on_input,inplace,no_combine,routed_scaling_factor)
  patched_isinstance(obj,classinfo)
layers/quantization/awq.py:
  AWQConfig.__init__(self,weight_bits,group_size,zero_point,modules_to_not_convert)
  AWQConfig.__repr__(self)
  AWQConfig.from_config(cls,config)
  AWQConfig.get_config_filenames()
  AWQConfig.get_min_capability(cls)
  AWQConfig.get_name(self)
  AWQConfig.get_quant_method(self,layer,prefix)
  AWQConfig.get_scaled_act_names(self)
  AWQConfig.get_supported_act_dtypes(self)
  AWQLinearMethod.__init__(self,quant_config)
  AWQLinearMethod.apply(self,layer,x,bias)
  AWQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  AWQLinearMethod.process_weights_after_loading(self,layer)
  AWQMarlinConfig.__init__(self,weight_bits,group_size,zero_point,lm_head_quantized,modules_to_not_convert,full_config)
  AWQMarlinConfig.__repr__(self)
  AWQMarlinConfig.from_config(cls,config)
  AWQMarlinConfig.get_config_filenames(cls)
  AWQMarlinConfig.get_min_capability(cls)
  AWQMarlinConfig.get_name(cls)
  AWQMarlinConfig.get_quant_method(self,layer,prefix)
  AWQMarlinConfig.get_scaled_act_names(self)
  AWQMarlinConfig.get_supported_act_dtypes(cls)
  AWQMarlinConfig.is_awq_marlin_compatible(cls,quant_config)
  AWQMarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  AWQMarlinLinearMethod.__init__(self,quant_config)
  AWQMarlinLinearMethod.apply(self,layer,x,bias)
  AWQMarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  AWQMarlinLinearMethod.process_weights_after_loading(self,layer)
  AWQMoEMethod.__init__(self,quant_config)
  AWQMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  AWQMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  AWQMoEMethod.process_weights_after_loading(self,layer)
  is_layer_skipped_awq(prefix,modules_to_not_convert)
layers/quantization/awq_triton.py:
  awq_dequantize_kernel(qweight_ptr,scales_ptr,zeros_ptr,group_size,result_ptr,num_cols,num_rows,BLOCK_SIZE_X,BLOCK_SIZE_Y)
  awq_dequantize_triton(qweight,scales,zeros,block_size_x,block_size_y)
  awq_gemm_kernel(a_ptr,b_ptr,c_ptr,zeros_ptr,scales_ptr,M,N,K,group_size,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,SPLIT_K)
  awq_gemm_triton(input,qweight,scales,qzeros,split_k_iters,block_size_m,block_size_n,block_size_k)
layers/quantization/base_config.py:
  FusedMoEMethodBase.apply(self,layer,x,topk_output,moe_runner_config)
  FusedMoEMethodBase.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  LinearMethodBase.apply(self,layer,x,bias)
  LinearMethodBase.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  QuantizationConfig.__init__(self)
  QuantizationConfig.from_config(cls,config)
  QuantizationConfig.get_config_filenames()
  QuantizationConfig.get_from_keys(config,keys)
  QuantizationConfig.get_from_keys_or(config,keys,default)
  QuantizationConfig.get_min_capability(cls)
  QuantizationConfig.get_name(self)
  QuantizationConfig.get_quant_method(self,layer,prefix)
  QuantizationConfig.get_scaled_act_names(self)
  QuantizationConfig.get_supported_act_dtypes(self)
  QuantizationConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  QuantizeMethodBase.apply(self,layer,*args,**kwargs)
  QuantizeMethodBase.create_weights(self,layer,*weight_args,**extra_weight_attrs)
  QuantizeMethodBase.process_weights_after_loading(self,layer)
  method_has_implemented_embedding(method_class)
layers/quantization/blockwise_int8.py:
  BlockInt8Config.__init__(self,is_checkpoint_int8_serialized,activation_scheme,ignored_layers,weight_block_size)
  BlockInt8Config.from_config(cls,config)
  BlockInt8Config.get_config_filenames(cls)
  BlockInt8Config.get_min_capability(cls)
  BlockInt8Config.get_name(cls)
  BlockInt8Config.get_quant_method(self,layer,prefix)
  BlockInt8Config.get_scaled_act_names(self)
  BlockInt8Config.get_supported_act_dtypes(cls)
  BlockInt8LinearMethod.__init__(self,quant_config)
  BlockInt8LinearMethod.apply(self,layer,x,bias)
  BlockInt8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  BlockInt8LinearMethod.process_weights_after_loading(self,layer)
  BlockInt8MoEMethod.__init__(self,quant_config)
  BlockInt8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  BlockInt8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  BlockInt8MoEMethod.process_weights_after_loading(self,layer)
layers/quantization/compressed_tensors/compressed_tensors.py:
  CompressedTensorsConfig.__init__(self,target_scheme_map,ignore,quant_format,sparsity_scheme_map,sparsity_ignore_list,kv_cache_scheme,config,packed_modules_mapping)
  CompressedTensorsConfig._check_scheme_supported(self,min_capability,error)
  CompressedTensorsConfig._get_scheme_from_parts(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_dynamic_token_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_fp8_w8a16(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_fp8_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_static_tensor_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_wNa16_group_channel(self,weight_quant,input_quant)
  CompressedTensorsConfig._parse_sparsity_config(cls,config)
  CompressedTensorsConfig._quantization_scheme_map_from_config(cls,config)
  CompressedTensorsConfig.from_config(cls,config)
  CompressedTensorsConfig.get_cache_scale(self,name)
  CompressedTensorsConfig.get_config_filenames(cls)
  CompressedTensorsConfig.get_linear_method(self)
  CompressedTensorsConfig.get_min_capability(cls)
  CompressedTensorsConfig.get_name(self)
  CompressedTensorsConfig.get_quant_method(self,layer,prefix)
  CompressedTensorsConfig.get_scaled_act_names(self)
  CompressedTensorsConfig.get_scheme(self,layer,layer_name)
  CompressedTensorsConfig.get_supported_act_dtypes(cls)
  CompressedTensorsConfig.supports_cutlass_24(weight_quant,input_quant,sparsity_scheme)
  CompressedTensorsLinearMethod.__init__(self,quantization_config)
  CompressedTensorsLinearMethod.apply(self,layer,x,bias)
  CompressedTensorsLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  CompressedTensorsLinearMethod.process_weights_after_loading(self,layer)
  DeviceCapability.as_version_str(self)
  DeviceCapability.to_int(self)
layers/quantization/compressed_tensors/compressed_tensors_moe.py:
  CompressedTensorsMoEMethod.__new__(cls,*args,**kwargs)
  CompressedTensorsMoEMethod.get_moe_method(quant_config)
  CompressedTensorsW8A8Fp8MoEMethod.__init__(self,quant_config)
  CompressedTensorsW8A8Fp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  CompressedTensorsW8A8Fp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  CompressedTensorsW8A8Fp8MoEMethod.process_weights_after_loading(self,layer)
  CompressedTensorsWNA16MoEMethod.__init__(self,quant_config)
  CompressedTensorsWNA16MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  CompressedTensorsWNA16MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  CompressedTensorsWNA16MoEMethod.get_scale_perms(num_bits)
  CompressedTensorsWNA16MoEMethod.marlin_moe_permute_scales(s,size_k,size_n,group_size,num_bits)
  CompressedTensorsWNA16MoEMethod.marlin_permute_scales(s,size_k,size_n,group_size,num_bits)
  CompressedTensorsWNA16MoEMethod.process_weights_after_loading(self,layer)
  CompressedTensorsWNA16MoEMethod.replace_tensor(name,new_t)
layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py:
  CompressedTensorsScheme.apply_weights(self,layer,x,bias)
  CompressedTensorsScheme.create_weights(self,*args,**kwargs)
  CompressedTensorsScheme.get_min_capability(cls)
  CompressedTensorsScheme.process_weights_after_loading(self,layer)
layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py:
  CompressedTensorsW8A16Fp8.__init__(self,strategy,is_static_input_scheme)
  CompressedTensorsW8A16Fp8.apply_weights(self,layer,x,bias)
  CompressedTensorsW8A16Fp8.create_weights(self,layer,input_size,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  CompressedTensorsW8A16Fp8.get_min_capability(cls)
  CompressedTensorsW8A16Fp8.process_weights_after_loading(self,layer)
  apply_fp8_marlin_linear(*args,**kwargs)
  prepare_fp8_layer_for_marlin(*args,**kwargs)
layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py:
  CompressedTensorsW8A8Fp8.__init__(self,strategy,is_static_input_scheme)
  CompressedTensorsW8A8Fp8.apply_weights(self,layer,x,bias)
  CompressedTensorsW8A8Fp8.create_weights(self,layer,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  CompressedTensorsW8A8Fp8.get_min_capability(cls)
  CompressedTensorsW8A8Fp8.process_weights_after_loading(self,layer)
layers/quantization/compressed_tensors/utils.py:
  _find_first_match(value,targets,check_contains)
  _is_equal_or_regex_match(value,target,check_contains)
  _match_fused_layer(layer_name,target_layers,fused_mapping)
  check_equal_or_regex_match(layer_name,targets)
  find_matched_target(layer_name,module,targets,fused_mapping)
  is_activation_quantization_format(format)
  should_ignore_layer(layer_name,ignore,fused_mapping)
layers/quantization/deep_gemm_wrapper/compile_utils.py:
  _BaseWarmupExecutor.create(kernel_type,**kwargs)
  _BaseWarmupExecutor.execute(self,m)
  _GroupedContWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _GroupedContWarmupExecutor.execute(self,m)
  _GroupedMaskedWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _GroupedMaskedWarmupExecutor.execute(self,m)
  _NormalWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _NormalWarmupExecutor.execute(self,m)
  _compile_deep_gemm_one_type_all(kernel_type,n,k,num_groups,m_list)
  _empty_block_fp8(size)
  _empty_token_fp8(size)
  _maybe_compile_deep_gemm_one_type_all(kernel_type,n,k,num_groups)
  deep_gemm_execution_hook(m,n,k,num_groups,kernel_type)
  update_deep_gemm_config(gpu_id,server_args)
layers/quantization/deep_gemm_wrapper/configurer.py:
  _compute_enable_deep_gemm()
  _is_blackwell_arch()
layers/quantization/deep_gemm_wrapper/entrypoint.py:
  configure_deep_gemm_num_sms(num_sms)
  gemm_nt_f8f8bf16(lhs,rhs,out)
  grouped_gemm_nt_f8f8bf16_contig(lhs,rhs,out,m_indices)
  grouped_gemm_nt_f8f8bf16_masked(lhs,rhs,out,masked_m,expected_m)
  update_deep_gemm_config(gpu_id,server_args)
layers/quantization/fp8.py:
  Fp8Config.__init__(self,is_checkpoint_fp8_serialized,activation_scheme,ignored_layers,weight_block_size)
  Fp8Config.from_config(cls,config)
  Fp8Config.get_config_filenames(cls)
  Fp8Config.get_min_capability(cls)
  Fp8Config.get_name(cls)
  Fp8Config.get_quant_method(self,layer,prefix)
  Fp8Config.get_scaled_act_names(self)
  Fp8Config.get_supported_act_dtypes(cls)
  Fp8KVCacheMethod.__init__(self,quant_config)
  Fp8LinearMethod.__init__(self,quant_config)
  Fp8LinearMethod.apply(self,layer,x,bias)
  Fp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  Fp8LinearMethod.process_weights_after_loading(self,layer)
  Fp8MoEMethod.__init__(self,quant_config)
  Fp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  Fp8MoEMethod.apply_with_router_logits(self,layer,x,topk_output,moe_runner_config)
  Fp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  Fp8MoEMethod.maybe_apply_hip_fused_experts(self,layer,x,topk_output,activation,no_combine)
  Fp8MoEMethod.process_weights_after_loading(self,layer)
  Fp8MoEMethod.process_weights_hip_int4(self,layer)
  Fp8MoEMethod.process_weights_hip_scale_padding(self,layer)
  dummy_func(*args,**kwargs)
  get_tile_tokens_dim(num_tokens,top_k,num_experts)
layers/quantization/fp8_kernel.py:
  _per_group_transpose(data_ptr,trans_data_ptr,expert_offsets,k,M_ALIGNMENT,BLOCK_SIZE_M,BLOCK_SIZE_K)
  _per_tensor_quant_mla_fp8_stage1(x_ptr,x_s_ptr,head_size,x_stride_h,x_stride_s,eps,fp8_max,BLOCK_SIZE)
  _per_tensor_quant_mla_fp8_stage2(x_ptr,x_s_ptr,x_q_ptr,num_seq,head_size,x_stride_h,x_stride_s,fp8_min,fp8_max,BLOCK_SIZE)
  _per_token_group_quant_8bit(y_ptr,y_q_ptr,y_s_ptr,y_stride,N,eps,bit8_min,bit8_max,BLOCK)
  _per_token_group_quant_8bit_colmajor(y_ptr,y_q_ptr,y_s_ptr,group_size,y_num_columns,y_s_col_stride,eps,bit8_min,bit8_max,BLOCK,SCALE_UE8M0)
  _per_token_group_quant_8bit_fuse_silu_and_mul(x,group_size,dst_dtype,column_major_scales,scale_tma_aligned,scale_ue8m0,masked_m)
  _per_token_group_quant_8bit_raw(x,group_size,eps,dtype,column_major_scales,scale_tma_aligned,scale_ue8m0)
  _per_token_group_quant_fp8_hopper_moe_mn_major(a,expert_offsets,problem_sizes,a_fp8,sfa,K,BLOCK_K,M_ALIGNMENT,BLOCK_M)
  _per_token_group_quant_mla_deep_gemm_masked_fp8(y_ptr,y_q_ptr,y_s_ptr,masked_m_ptr,group_size,y_stride_b,y_stride_t,y_q_stride_b,y_q_stride_t,y_s_stride_b,y_s_stride_g,eps,fp8_min,fp8_max,NUM_GROUP,BLOCK)
  _static_quant_fp8(y_ptr,y_q_ptr,y_s_ptr,y_s_repeat_ptr,y_stride,N,fp8_min,fp8_max,BLOCK,REPEAT_SCALE)
  _w8a8_block_fp8_matmul(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  _w8a8_block_fp8_matmul_unrolledx4(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  create_per_token_group_quant_fp8_output_scale(x_shape,device,group_size,column_major_scales,scale_tma_aligned,scale_ue8m0)
  deep_gemm_fp8_fp8_bf16_nt(A,As,B,Bs,C)
  deep_gemm_fp8_fp8_bf16_nt_fake(A,As,B,Bs,C)
  get_w8a8_block_fp8_configs(N,K,block_n,block_k)
  grid(META)
  is_fp8_fnuz()
  is_weak_contiguous(x)
  per_group_transpose(a,expert_offsets,M_ALIGNMENT)
  per_tensor_quant_mla_fp8(x,x_s_out,eps)
  per_token_group_quant_8bit(x,group_size,dst_dtype,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  per_token_group_quant_fp8_hopper_moe_mn_major(A,expert_offsets,problem_sizes,group_size,expert_tokens_alignment)
  per_token_group_quant_mla_deep_gemm_masked_fp8(x,group_size,eps,dtype)
  prepare_block_fp8_matmul_inputs(A,B,As,Bs,block_size,output_dtype)
  scaled_fp8_quant(input,scale,num_token_padding,use_per_token_if_dynamic)
  scaled_fp8_quant(input,scale,num_token_padding,use_per_token_if_dynamic)
  scaled_mm_kernel(a_ptr,b_ptr,scale_a_ptr,scale_b_ptr,c_ptr,bias_ptr,M,N,K,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,ACCUMULATOR_DTYPE,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,BLOCK_SIZE_SCALE_A,BLOCK_SIZE_SCALE_B)
  select_w8a8_block_fp8_matmul_kernel(M,N,META)
  select_w8a8_block_fp8_matmul_kernel(M,N,META)
  sglang_per_token_group_quant_8bit(x,group_size,dst_dtype,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  sglang_per_token_group_quant_fp8(x,group_size,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  sglang_per_token_quant_fp8(x,dtype)
  static_quant_fp8(x,x_s,repeat_scale)
  triton_scaled_mm(input,weight,scale_a,scale_b,out_dtype,bias,block_size_m,block_size_n,block_size_k,use_heuristic)
  use_w8a8_block_fp8_matmul_unrolledx4(M,N,META)
  w8a8_block_fp8_matmul(A,B,As,Bs,block_size,output_dtype)
  w8a8_block_fp8_matmul_deepgemm(A,B,As,Bs,block_size,output_dtype)
  w8a8_block_fp8_matmul_triton(A,B,As,Bs,block_size,output_dtype)
layers/quantization/fp8_utils.py:
  _apply_fallback_scaled_mm(qinput,weight,x_scale,weight_scale,input_2d_shape,output_shape,bias,input_dtype)
  _check_ue8m0(name,x)
  _process_scaled_mm_output(output,input_2d_shape,output_shape)
  _requant_weight_ue8m0(weight,weight_scale_inv,weight_block_size)
  _transform_scale(sf,mn)
  aiter_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  apply_fp8_linear(input,weight,weight_scale,input_scale,input_scale_ub,bias,cutlass_fp8_supported,use_per_token_if_dynamic,pad_output,compressed_tensor_quant)
  block_quant_dequant(x_q_block,x_s,block_size,dtype)
  block_quant_to_tensor_quant(x_q_block,x_s,block_size)
  can_auto_enable_marlin_fp8()
  ceil_to_ue8m0(x)
  channel_quant_to_tensor_quant(x_q_channel,x_s)
  cutlass_block_fp8_supported()
  cutlass_fp8_supported()
  cutlass_w8a8_block_fp8_linear_with_fallback(input,weight,block_size,weight_scale,input_scale,bias)
  deepgemm_w8a8_block_fp8_linear_with_fallback(input,weight,block_size,weight_scale,input_scale,bias)
  dequant_mxfp4(w_block,w_scale,out_dtype)
  dispatch_w8a8_block_fp8_linear()
  flashinfer_gemm_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  input_to_float8(x,dtype)
  normalize_e4m3fn_to_e4m3fnuz(weight,weight_scale,input_scale)
  per_block_cast_to_fp8(x)
  requant_weight_ue8m0_inplace(weight,weight_scale_inv,weight_block_size)
  triton_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  use_rowwise_torch_scaled_mm()
layers/quantization/fpgemm_fp8.py:
  FBGEMMFp8Config.__init__(self,ignore_list,input_scale_ub)
  FBGEMMFp8Config.from_config(cls,config)
  FBGEMMFp8Config.get_config_filenames(cls)
  FBGEMMFp8Config.get_min_capability(cls)
  FBGEMMFp8Config.get_name(cls)
  FBGEMMFp8Config.get_quant_method(self,layer,prefix)
  FBGEMMFp8Config.get_scaled_act_names(self)
  FBGEMMFp8Config.get_supported_act_dtypes(cls)
  FBGEMMFp8LinearMethod.__init__(self,quant_config)
  FBGEMMFp8LinearMethod.apply(self,layer,x,bias)
  FBGEMMFp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  FBGEMMFp8LinearMethod.process_weights_after_loading(self,layer)
layers/quantization/gptq.py:
  GPTQConfig.__init__(self,weight_bits,group_size,desc_act,lm_head_quantized,dynamic)
  GPTQConfig.__repr__(self)
  GPTQConfig.from_config(cls,config)
  GPTQConfig.get_config_filenames(cls)
  GPTQConfig.get_min_capability(cls)
  GPTQConfig.get_name(cls)
  GPTQConfig.get_quant_method(self,layer,prefix)
  GPTQConfig.get_scaled_act_names(self)
  GPTQConfig.get_supported_act_dtypes(cls)
  GPTQLinearMethod.__init__(self,quant_config)
  GPTQLinearMethod.apply(self,layer,x,bias)
  GPTQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  GPTQLinearMethod.process_weights_after_loading(self,layer)
  GPTQMarlinConfig.__init__(self,weight_bits,group_size,desc_act,is_sym,lm_head_quantized,dynamic,full_config)
  GPTQMarlinConfig.__repr__(self)
  GPTQMarlinConfig.from_config(cls,config)
  GPTQMarlinConfig.get_config_filenames(cls)
  GPTQMarlinConfig.get_min_capability(cls)
  GPTQMarlinConfig.get_name(cls)
  GPTQMarlinConfig.get_quant_method(self,layer,prefix)
  GPTQMarlinConfig.get_scaled_act_names(self)
  GPTQMarlinConfig.get_supported_act_dtypes(cls)
  GPTQMarlinConfig.is_gptq_marlin_compatible(cls,quant_config)
  GPTQMarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  GPTQMarlinLinearMethod.__init__(self,quant_config)
  GPTQMarlinLinearMethod._get_weight_params(layer)
  GPTQMarlinLinearMethod._transform_param(layer,name,fn)
  GPTQMarlinLinearMethod.apply(self,layer,x,bias)
  GPTQMarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  GPTQMarlinLinearMethod.process_weights_after_loading(self,layer)
  GPTQMarlinLinearMethod.transform_w_q(x)
  GPTQMarlinLinearMethod.transform_w_s(x)
  GPTQMarlinMoEMethod.__init__(self,quant_config)
  GPTQMarlinMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  GPTQMarlinMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  GPTQMarlinMoEMethod.process_weights_after_loading(self,layer)
  check_marlin_format(hf_quant_cfg)
  gptq_marlin_moe_repack(b_q_weight,perm,size_k,size_n,num_bits)
layers/quantization/int8_kernel.py:
  _per_token_group_quant_int8(y_ptr,y_q_ptr,y_s_ptr,y_stride,N,eps,int8_min,int8_max,BLOCK)
  _per_token_quant_int8(x_ptr,xq_ptr,scale_ptr,x_sum_ptr,stride_x,stride_xq,N,CAL_SUM,BLOCK)
  _w8a8_block_int8_matmul(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  get_w8a8_block_int8_configs(N,K,block_n,block_k)
  grid(META)
  per_token_group_quant_int8(x,group_size,eps,dtype)
  per_token_quant_int8(x,scale_dtype,cal_sum)
  sglang_per_token_group_quant_int8(x,group_size,eps,dtype)
  w8a8_block_int8_matmul(A,B,As,Bs,block_size,output_dtype)
layers/quantization/int8_utils.py:
  apply_w8a8_block_int8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  block_dequant(x_q_block,x_s,block_size)
  input_to_int8(x,dtype)
layers/quantization/kv_cache.py:
  BaseKVCacheMethod.__init__(self,quant_config)
  BaseKVCacheMethod.apply(self,layer)
  BaseKVCacheMethod.create_weights(self,layer)
  BaseKVCacheMethod.process_weights_after_loading(self,layer)
layers/quantization/marlin_utils.py:
  MarlinConfig.__init__(self,group_size,lm_head_quantized)
  MarlinConfig.__repr__(self)
  MarlinConfig.from_config(cls,config)
  MarlinConfig.get_config_filenames(cls)
  MarlinConfig.get_min_capability(cls)
  MarlinConfig.get_name(cls)
  MarlinConfig.get_quant_method(self,layer,prefix)
  MarlinConfig.get_supported_act_dtypes(cls)
  MarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  MarlinLinearMethod.__init__(self,quant_config)
  MarlinLinearMethod.apply(self,layer,x,bias)
  MarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  MarlinLinearMethod.process_weights_after_loading(self,layer)
  _check_marlin_supported(quant_type,group_size,has_zp,device_capability)
  apply_awq_marlin_linear(input,weight,weight_scale,weight_zp,g_idx,g_idx_sort_indices,workspace,quant_type,output_size_per_partition,input_size_per_partition,bias,use_fp32_reduce)
  apply_gptq_marlin_linear(input,weight,weight_scale,weight_zp,g_idx,g_idx_sort_indices,workspace,wtype,output_size_per_partition,input_size_per_partition,is_k_full,bias,use_fp32_reduce)
  awq_to_marlin_zero_points(q_zp_packed,size_k,size_n,num_bits)
  check_marlin_supported(quant_type,group_size,has_zp,device_capability)
  check_marlin_supports_layer(layer,group_size)
  check_marlin_supports_shape(output_size_per_partition,input_size_per_partition,input_size,group_size)
  check_moe_marlin_supports_layer(layer,group_size)
  get_scale_perms()
  marlin_is_k_full(act_order,is_row_parallel)
  marlin_make_empty_g_idx(device)
  marlin_make_empty_zp(device)
  marlin_make_workspace(device,max_blocks_per_sm)
  marlin_moe_permute_scales(s,size_k,size_n,group_size)
  marlin_permute_bias(s)
  marlin_permute_scales(s,size_k,size_n,group_size)
  marlin_repeat_scales_on_all_ranks(act_order,group_size,is_row_parallel)
  marlin_sort_g_idx(g_idx)
  marlin_zero_points(zp,size_k,size_n,num_bits)
  maybe_warn_marlin_atomic_add(device,dtype)
  maybe_warn_marlin_atomic_add_env()
  moe_awq_to_marlin_zero_points(q_zp_packed,size_k,size_n,num_bits)
  query_marlin_supported_quant_types(has_zp,include_fp_type,device_capability)
  should_use_atomic_add_reduce(m,n,k,device,dtype)
  verify_marlin_supported(quant_type,group_size,has_zp)
  verify_marlin_supports_shape(output_size_per_partition,input_size_per_partition,input_size,group_size)
layers/quantization/marlin_utils_fp8.py:
  apply_fp8_marlin_linear(input,weight,weight_scale,workspace,size_n,size_k,bias,use_fp32_reduce)
  fp8_fused_exponent_bias_into_scales(scales)
  marlin_quant_fp8_torch(weight,group_size)
  pack_fp8_to_int32(fp8_tensor,size_k_first)
  prepare_fp8_layer_for_marlin(layer,size_k_first)
  prepare_moe_fp8_layer_for_marlin(layer,size_k_first)
layers/quantization/modelopt_quant.py:
  ModelOptFp4Config.__init__(self,is_checkpoint_nvfp4_serialized,kv_cache_quant_algo,group_size,exclude_modules)
  ModelOptFp4Config.from_config(cls,config)
  ModelOptFp4Config.get_config_filenames(cls)
  ModelOptFp4Config.get_min_capability(cls)
  ModelOptFp4Config.get_name(cls)
  ModelOptFp4Config.get_quant_method(self,layer,prefix)
  ModelOptFp4Config.get_scaled_act_names(self)
  ModelOptFp4Config.get_supported_act_dtypes(cls)
  ModelOptFp4Config.is_layer_excluded(self,prefix,exclude_modules)
  ModelOptFp4LinearMethod.__init__(self,quant_config)
  ModelOptFp4LinearMethod.apply(self,layer,x,bias)
  ModelOptFp4LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  ModelOptFp4LinearMethod.process_weights_after_loading(self,layer)
  ModelOptFp8Config.__init__(self,is_checkpoint_fp8_serialized,kv_cache_quant_method,exclude_modules)
  ModelOptFp8Config.from_config(cls,config)
  ModelOptFp8Config.get_config_filenames(cls)
  ModelOptFp8Config.get_min_capability(cls)
  ModelOptFp8Config.get_name(cls)
  ModelOptFp8Config.get_quant_method(self,layer,prefix)
  ModelOptFp8Config.get_scaled_act_names(self)
  ModelOptFp8Config.get_supported_act_dtypes(cls)
  ModelOptFp8KVCacheMethod.__init__(self,quant_config)
  ModelOptFp8LinearMethod.__init__(self,quant_config)
  ModelOptFp8LinearMethod.apply(self,layer,x,bias)
  ModelOptFp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,params_dtype,**extra_weight_attrs)
  ModelOptFp8LinearMethod.process_weights_after_loading(self,layer)
  ModelOptFp8MoEMethod.__init__(self,quant_config)
  ModelOptFp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  ModelOptFp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  ModelOptFp8MoEMethod.process_weights_after_loading(self,layer)
  ModelOptNvFp4FusedMoEMethod.__init__(self,quant_config)
  ModelOptNvFp4FusedMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  ModelOptNvFp4FusedMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe(self)
  ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first(self)
  ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel(self,gemm1_weights,gemm2_weights,gemm1_scales_linear_fp4_bytes,gemm2_scales_linear_fp4_bytes,hidden_size,intermediate_size,num_experts)
  ModelOptNvFp4FusedMoEMethod.process_weights_after_loading(self,layer)
  ModelOptNvFp4FusedMoEMethod.swizzle_blockscale(self,scale)
layers/quantization/moe_wna16.py:
  MoeWNA16Config.__init__(self,linear_quant_method,weight_bits,group_size,has_zp,lm_head_quantized,modules_to_not_convert,full_config)
  MoeWNA16Config.from_config(cls,config)
  MoeWNA16Config.get_config_filenames(cls)
  MoeWNA16Config.get_min_capability(cls)
  MoeWNA16Config.get_name(cls)
  MoeWNA16Config.get_quant_method(self,layer,prefix)
  MoeWNA16Config.get_scaled_act_names(self)
  MoeWNA16Config.get_supported_act_dtypes(cls)
  MoeWNA16Config.is_moe_wna16_compatible(cls,quant_config)
  MoeWNA16Config.override_quantization_method(cls,hf_quant_cfg,user_quant)
  MoeWNA16Method.__init__(self,quant_config)
  MoeWNA16Method.apply(self,layer,x,topk_output,moe_runner_config)
  MoeWNA16Method.convert_awq_tensor(tensor,tensor_type)
  MoeWNA16Method.convert_gptq_int4_qzeros(tensor)
  MoeWNA16Method.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  MoeWNA16Method.get_weight_loader(layer,weight_loader)
  MoeWNA16Method.moe_wna16_weight_loader(param,loaded_weight,weight_name,shard_id,expert_id)
  get_weight_perm(num_bits)
  is_layer_skipped_quant(prefix,modules_to_not_convert)
layers/quantization/mxfp4.py:
  Mxfp4Config.__init__(self,ignored_layers,is_checkpoint_mxfp4_serialized)
  Mxfp4Config.from_config(cls,config)
  Mxfp4Config.get_config_filenames(cls)
  Mxfp4Config.get_min_capability(cls)
  Mxfp4Config.get_name(cls)
  Mxfp4Config.get_quant_method(self,layer,prefix)
  Mxfp4Config.get_scaled_act_names(self)
  Mxfp4Config.get_supported_act_dtypes(cls)
  Mxfp4Config.is_static_cfg(self)
  Mxfp4DynamicQuantMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  Mxfp4DynamicQuantMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  Mxfp4DynamicQuantMoEMethod.mxfp4_quantize(self,w)
  Mxfp4DynamicQuantMoEMethod.process_weights_after_loading(self,layer)
  Mxfp4MoEMethod.__init__(self,prefix)
  Mxfp4MoEMethod._get_tile_tokens_dim(self,x,top_k)
  Mxfp4MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  Mxfp4MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,with_bias,**extra_weight_attrs)
  Mxfp4MoEMethod.process_weights_after_loading(self,layer)
  Mxfp4MoEMethod.swap_every_two_rows(x,axis)
  _dequant_mxfp4(x,scale,float_dtype)
  _dequant_mxfp4_fake(x,scale,float_dtype)
  _quant_dequant_mxfp4(x,scale_calculation_mode)
  _quant_dequant_mxfp4_fake(x,scale_calculation_mode)
  _swizzle_mxfp4(quant_tensor,scale,num_warps)
layers/quantization/mxfp4_tensor.py:
  MXFP4QuantizeUtil.cast_fp4(x)
  MXFP4QuantizeUtil.dequantize(cls,quantized_data,dtype,scale,block_sizes)
  MXFP4QuantizeUtil.fuse_uint4_to_uint8(x)
  MXFP4QuantizeUtil.quantize(cls,input,block_size)
  MXFP4QuantizeUtil.unfuse_uint8_to_uint4(x)
layers/quantization/petit.py:
  PetitNvFp4Config.__init__(self,is_checkpoint_nvfp4_serialized,kv_cache_quant_algo,group_size,exclude_modules)
  PetitNvFp4Config.from_config(cls,config)
  PetitNvFp4Config.get_config_filenames(cls)
  PetitNvFp4Config.get_min_capability(cls)
  PetitNvFp4Config.get_name(cls)
  PetitNvFp4Config.get_quant_method(self,layer,prefix)
  PetitNvFp4Config.get_scaled_act_names(self)
  PetitNvFp4Config.get_supported_act_dtypes(cls)
  PetitNvFp4Config.is_layer_excluded(self,prefix,exclude_modules)
  PetitNvFp4Config.is_petit_nvfp4_compatible(cls,quant_config)
  PetitNvFp4Config.override_quantization_method(cls,hf_quant_cfg,user_quant)
  PetitNvFp4LinearMethod.__init__(self,quant_config)
  PetitNvFp4LinearMethod.apply(self,layer,x,bias)
  PetitNvFp4LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  PetitNvFp4LinearMethod.process_weights_after_loading(self,layer)
layers/quantization/petit_utils.py:
  _check_petit_nvfp4_supported(quant_method,group_size)
  _check_petit_nvfp4_supported(quant_method,group_size)
  apply_petit_nvfp4_linear(input,weight,weight_scale,weight_scale_2,size_n,size_k,bias)
  apply_petit_nvfp4_linear(input,weight,weight_scale,weight_scale_2,size_n,size_k,bias)
  prepare_nvfp4_layer_for_petit(layer)
  prepare_nvfp4_layer_for_petit(layer)
  verify_petit_nvfp4_supported(quant_method,group_size)
layers/quantization/qoq.py:
  QoQConfig.__init__(self,weight_bits,group_size)
  QoQConfig.__repr__(self)
  QoQConfig.from_config(cls,config)
  QoQConfig.get_config_filenames(cls)
  QoQConfig.get_min_capability(cls)
  QoQConfig.get_name(cls)
  QoQConfig.get_quant_method(self,layer,prefix)
  QoQConfig.get_scaled_act_names(self)
  QoQConfig.get_supported_act_dtypes(cls)
  QoQLinearMethod.__init__(self,quant_config)
  QoQLinearMethod.apply(self,layer,x,bias)
  QoQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  QoQLinearMethod.process_weights_after_loading(self,layer)
layers/quantization/quark/quark.py:
  QuarkConfig.__init__(self,quant_config,kv_cache_group,kv_cache_config,pack_method)
  QuarkConfig._check_scheme_supported(self,min_capability,error)
  QuarkConfig._find_matched_config(self,layer_name,module)
  QuarkConfig._get_scheme_from_config(self,config)
  QuarkConfig._is_mx_fp4(self,weight_quant,input_quant)
  QuarkConfig.from_config(cls,config)
  QuarkConfig.get_config_filenames(cls)
  QuarkConfig.get_linear_method(self)
  QuarkConfig.get_min_capability(cls)
  QuarkConfig.get_name(self)
  QuarkConfig.get_quant_method(self,layer,prefix)
  QuarkConfig.get_scaled_act_names(self)
  QuarkConfig.get_scheme(self,layer,layer_name)
  QuarkConfig.get_supported_act_dtypes(cls)
  QuarkKVCacheMethod.__init__(self,quant_config)
  QuarkKVCacheMethod.validate_kv_cache_config(kv_cache_config)
  QuarkLinearMethod.__init__(self,quantization_config)
  QuarkLinearMethod.apply(self,layer,x,bias)
  QuarkLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  QuarkLinearMethod.process_weights_after_loading(self,layer)
layers/quantization/quark/quark_moe.py:
  QuarkMoEMethod.__new__(cls,*args,**kwargs)
  QuarkMoEMethod.get_moe_method(quant_config,module,layer_name)
  QuarkW4A4MXFp4MoEMethod.__init__(self,weight_config,input_config)
  QuarkW4A4MXFp4MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  QuarkW4A4MXFp4MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  QuarkW4A4MXFp4MoEMethod.process_weights_after_loading(self,layer)
layers/quantization/quark/schemes/quark_scheme.py:
  QuarkScheme.apply_weights(self,layer,x,bias)
  QuarkScheme.create_weights(self,*args,**kwargs)
  QuarkScheme.get_min_capability(cls)
  QuarkScheme.process_weights_after_loading(self,layer)
layers/quantization/quark/schemes/quark_w4a4_mxfp4.py:
  QuarkW4A4MXFP4.__init__(self,weight_quant_spec,input_quant_spec)
  QuarkW4A4MXFP4.apply_weights(self,layer,x,bias)
  QuarkW4A4MXFP4.create_weights(self,layer,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  QuarkW4A4MXFP4.get_min_capability(cls)
  QuarkW4A4MXFP4.process_weights_after_loading(self,layer)
layers/quantization/quark/utils.py:
  _is_equal_or_regex_match(value,target,check_contains)
  check_equal_or_regex_match(layer_name,targets)
  deep_compare(dict1,dict2)
  should_ignore_layer(layer_name,ignore,fused_mapping)
layers/quantization/unquant.py:
  UnquantizedEmbeddingMethod.apply(self,layer,x,bias)
  UnquantizedEmbeddingMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  UnquantizedEmbeddingMethod.embedding(self,layer,input_)
  UnquantizedFusedMoEMethod.__init__(self,use_triton_kernels)
  UnquantizedFusedMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,with_bias,**extra_weight_attrs)
  UnquantizedFusedMoEMethod.forward_cpu(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_cuda(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_npu(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_tpu(self,*args,**kwargs)
  UnquantizedFusedMoEMethod.process_weights_after_loading(self,layer)
  UnquantizedLinearMethod.apply(self,layer,x,bias)
  UnquantizedLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  UnquantizedLinearMethod.process_weights_after_loading(self,layer)
layers/quantization/utils.py:
  MockScalarTypes.__getattr__(self,name)
  all_close_1d(x)
  assert_fp8_all_close(a,b)
  convert_to_channelwise(weight_scale,logical_widths)
  get_dynamic_override(config,layer_name,key,default_value)
  get_linear_quant_method(config,layer,prefix,linear_method_cls)
  get_pack_factor(num_bits)
  get_scalar_types()
  gptq_quantize_weights(w,quant_type,group_size,act_order,test_perm)
  is_layer_skipped(prefix,ignored_layers,fused_mapping)
  override_config(config,prefix)
  pack_cols(q_w,num_bits,size_k,size_n)
  pack_rows(q_w,num_bits,size_k,size_n)
  per_tensor_dequantize(tensor,inv_scale)
  permute_rows(q_w,w_ref,group_size,test_perm)
  quantize_weights(w,quant_type,group_size,zero_points,ref_zero_points_after_scales)
  replace_parameter(mod,name,new)
  requantize_with_max_scale(weight,weight_scale,logical_widths)
  reshape_w(w)
  sort_weights(q_w,g_idx)
  unpack_cols(packed_q_w,num_bits,size_k,size_n)
  update_tensor_inplace(old,new)
layers/quantization/w4afp8.py:
  W4AFp8Config.__init__(self,is_checkpoint_fp8_serialized,is_checkpoint_w4afp8_serialized,linear_activation_scheme,moe_activation_scheme,ignored_layers,weight_block_size,group_size)
  W4AFp8Config.from_config(cls,config)
  W4AFp8Config.get_config_filenames(cls)
  W4AFp8Config.get_min_capability(cls)
  W4AFp8Config.get_name(cls)
  W4AFp8Config.get_quant_method(self,layer,prefix)
  W4AFp8Config.get_scaled_act_names(self)
  W4AFp8Config.get_supported_act_dtypes(cls)
  W4AFp8MoEMethod.__init__(self,quant_config)
  W4AFp8MoEMethod._interleave_scales(self,scales)
  W4AFp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  W4AFp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W4AFp8MoEMethod.process_weights_after_loading(self,layer)
layers/quantization/w8a8_fp8.py:
  W8A8FP8MoEMethod.__init__(self,quant_config)
  W8A8FP8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  W8A8FP8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W8A8FP8MoEMethod.process_weights_after_loading(self,layer)
  W8A8Fp8Config.__init__(self,is_checkpoint_fp8_serialized)
  W8A8Fp8Config.from_config(cls,config)
  W8A8Fp8Config.get_config_filenames(cls)
  W8A8Fp8Config.get_min_capability(cls)
  W8A8Fp8Config.get_name(self)
  W8A8Fp8Config.get_quant_method(self,layer,prefix)
  W8A8Fp8Config.get_scaled_act_names(self)
  W8A8Fp8Config.get_supported_act_dtypes(cls)
  W8A8Fp8LinearMethod.__init__(self,quantization_config)
  W8A8Fp8LinearMethod.apply(self,layer,x,bias)
  W8A8Fp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  W8A8Fp8LinearMethod.process_weights_after_loading(self,layer)
layers/quantization/w8a8_int8.py:
  NPU_W8A8DynamicLinearMethod.__init__(self,quantization_config)
  NPU_W8A8DynamicLinearMethod.apply(self,layer,x,bias)
  NPU_W8A8DynamicLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8DynamicLinearMethod.process_weights_after_loading(self,layer)
  NPU_W8A8DynamicLinearMethodImpl.__init__(self)
  NPU_W8A8DynamicLinearMethodImpl.apply(layer,x,bias,tp_rank)
  NPU_W8A8DynamicLinearMethodImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethod.__init__(self,quantization_config)
  NPU_W8A8LinearMethod.apply(self,layer,x,bias)
  NPU_W8A8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8LinearMethod.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethodImpl.__init__(self)
  NPU_W8A8LinearMethodImpl.apply(layer,x,bias)
  NPU_W8A8LinearMethodImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8LinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8LinearMethodImpl.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethodMTImpl.__init__(self)
  NPU_W8A8LinearMethodMTImpl.apply(layer,x,bias)
  NPU_W8A8LinearMethodMTImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8LinearMethodMTImpl.process_weights_after_loading(self,layer)
  NPU_W8A8MoEMethod.__init__(self,quantization_config)
  NPU_W8A8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  NPU_W8A8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8MoEMethod.process_weights_after_loading(self,layer)
  W8A8Int8Config.__init__(self,quant_config)
  W8A8Int8Config.from_config(cls,config)
  W8A8Int8Config.get_config_filenames(cls)
  W8A8Int8Config.get_min_capability(cls)
  W8A8Int8Config.get_name(self)
  W8A8Int8Config.get_quant_method(self,layer,prefix)
  W8A8Int8Config.get_scaled_act_names(self)
  W8A8Int8Config.get_supported_act_dtypes(cls)
  W8A8Int8Config.is_layer_skipped(self,prefix,fused_mapping)
  W8A8Int8LinearMethod.__init__(self,quantization_config)
  W8A8Int8LinearMethod.apply(self,layer,x,bias)
  W8A8Int8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  W8A8Int8LinearMethod.process_weights_after_loading(self,layer)
  W8A8Int8MoEMethod.__init__(self,quant_config)
  W8A8Int8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  W8A8Int8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W8A8Int8MoEMethod.process_weights_after_loading(self,layer)
  _rmsnorm_forward_oot(self,x,residual)
  init(self,hidden_size,**extra_args)
  npu_fused_experts(hidden_states,w13,w13_scale,w2,w2_scale,topk_weights,topk_ids,top_k)
  npu_wrapper_rmsnorm_forward(func)
  npu_wrapper_rmsnorm_init(func)
layers/radix_attention.py:
  RadixAttention.__init__(self,num_heads,head_dim,scaling,num_kv_heads,layer_id,logit_cap,v_head_dim,sliding_window_size,is_cross_attention,pos_encoding_mode,logit_capping_method,quant_config,attn_type,use_irope,prefix)
  RadixAttention.forward(self,q,k,v,forward_batch,save_kv_cache,**kwargs)
layers/rotary_embedding.py:
  DeepseekScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype,extrapolation_factor,attn_factor,beta_fast,beta_slow,mscale,mscale_all_dim,device)
  DeepseekScalingRotaryEmbedding._compute_cos_sin_cache(self)
  DeepseekScalingRotaryEmbedding._compute_inv_freq(self,scaling_factor)
  DeepseekScalingRotaryEmbedding.forward_cpu(self,positions,query,key,offsets)
  DeepseekScalingRotaryEmbedding.forward_native(self,positions,query,key,offsets)
  DeepseekScalingRotaryEmbedding.forward_npu(self,positions,query,key,offsets)
  DualChunkRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,chunk_size,local_size)
  DualChunkRotaryEmbedding._apply_rotary_embedding(self,cos_sin,hidden_rot,hidden_pass)
  DualChunkRotaryEmbedding._compute_cos_sin_cache(self)
  DualChunkRotaryEmbedding._compute_inv_freq(self,base)
  DualChunkRotaryEmbedding.extra_repr(self)
  DualChunkRotaryEmbedding.forward(self,positions,query,key,offsets)
  DynamicNTKAlphaRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_alpha,dtype)
  DynamicNTKAlphaRotaryEmbedding._compute_cos_sin_cache(self)
  DynamicNTKScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype)
  DynamicNTKScalingRotaryEmbedding._compute_cos_sin_cache(self)
  LinearScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factors,dtype)
  LinearScalingRotaryEmbedding._compute_cos_sin_cache(self)
  LinearScalingRotaryEmbedding.scaling_factor_to_offset(self)
  Llama3RotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,scaling_factor,low_freq_factor,high_freq_factor,orig_max_position)
  Llama3RotaryEmbedding._compute_inv_freq(self,base)
  Llama4VisionRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype)
  Llama4VisionRotaryEmbedding._compute_cos_sin_cache(self)
  Llama4VisionRotaryEmbedding._compute_inv_freq(self,base)
  Llama4VisionRotaryEmbedding.forward(self,query,key)
  MRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,mrope_section)
  MRotaryEmbedding.forward(self,positions,query,key)
  MRotaryEmbedding.get_next_input_positions(mrope_position_delta,context_len,seq_len)
  MRotaryEmbedding.get_rope_index(spatial_merge_size,image_token_id,video_token_id,vision_start_token_id,model_type,tokens_per_second,input_ids,image_grid_thw,video_grid_thw,second_per_grid_ts,**kwargs)
  MRotaryEmbedding.get_rope_index_glm4v(input_ids,hf_config,image_grid_thw,video_grid_thw,attention_mask,**kwargs)
  Phi3LongRoPEScaledRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,original_max_position_embeddings,base,is_neox_style,dtype,short_factor,long_factor,short_mscale,long_mscale)
  Phi3LongRoPEScaledRotaryEmbedding._compute_cos_sin_cache(self,max_position_embeddings,rescale_factors,mscale)
  Phi3LongRoPEScaledRotaryEmbedding._compute_inv_freq(self,rescale_factors)
  Phi3LongRoPEScaledRotaryEmbedding.forward(self,positions,query,key,offsets)
  RotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype)
  RotaryEmbedding._compute_cos_sin_cache(self)
  RotaryEmbedding._compute_inv_freq(self,base)
  RotaryEmbedding.extra_repr(self)
  RotaryEmbedding.forward_cpu(self,positions,query,key,offsets)
  RotaryEmbedding.forward_cuda(self,positions,query,key,offsets,fused_set_kv_buffer_arg)
  RotaryEmbedding.forward_native(self,positions,query,key,offsets)
  RotaryEmbedding.forward_npu(self,positions,query,key,offsets)
  YaRNScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype,extrapolation_factor,attn_factor,beta_fast,beta_slow)
  YaRNScalingRotaryEmbedding._compute_cos_sin_cache(self)
  YaRNScalingRotaryEmbedding._compute_inv_freq(self,scaling_factor)
  _apply_rotary_emb(x,cos,sin,is_neox_style)
  _rotate_gptj(x)
  _rotate_neox(x)
  _yarn_find_correction_dim(num_rotations,dim,base,max_position_embeddings)
  _yarn_find_correction_range(low_rot,high_rot,dim,base,max_position_embeddings)
  _yarn_get_mscale(scale)
  _yarn_linear_ramp_mask(low,high,dim,dtype,device)
  apply_rotary_pos_emb_native(q,k,cos,sin,unsqueeze_dim)
  apply_rotary_pos_emb_npu(q,k,cos,sin,unsqueeze_dim)
  get_rope(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,dual_chunk_attention_config)
  get_rope_cpu(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,device)
  get_rope_wrapper(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,device)
  rotate_half(x)
  yarn_get_mscale(scale,mscale)
layers/sampler.py:
  Sampler.__init__(self)
  Sampler.forward(self,logits_output,sampling_info,return_logprob,top_logprobs_nums,token_ids_logprobs)
  apply_custom_logit_processor(logits,sampling_batch_info,num_tokens_in_batch)
  get_token_ids_logprobs(logprobs,token_ids_logprobs)
  get_top_logprobs(logprobs,top_logprobs_nums)
  sampling_from_probs_torch(probs)
  top_k_top_p_min_p_sampling_from_probs_torch(probs,top_ks,top_ps,min_ps,need_min_p_sampling)
  top_p_normalize_probs_torch(probs,top_ps)
layers/torchao_utils.py:
  apply_torchao_config_to_model(model,torchao_config,filter_fn)
  get_gemlite_cache_path()
  proj_filter(module,fqn)
  save_gemlite_cache(print_error)
layers/utils.py:
  PPMissingLayer.__init__(self,*args,**kwargs)
  PPMissingLayer.forward(self,*args,**kwargs)
  get_layer_id(weight_name)
layers/vocab_parallel_embedding.py:
  ParallelLMHead.__init__(self,num_embeddings,embedding_dim,bias,params_dtype,org_num_embeddings,padding_size,quant_config,prefix,use_attn_tp_group,use_presharded_weights)
  ParallelLMHead.forward(self,input_)
  ParallelLMHead.tie_weights(self,embed_tokens)
  VocabParallelEmbedding.__init__(self,num_embeddings,embedding_dim,params_dtype,org_num_embeddings,padding_size,quant_config,prefix,enable_tp,use_attn_tp_group,use_presharded_weights)
  VocabParallelEmbedding._get_indices(cls,vocab_size_padded,org_vocab_size_padded,vocab_size,org_vocab_size,tp_rank,tp_size)
  VocabParallelEmbedding.extra_repr(self)
  VocabParallelEmbedding.forward(self,input_)
  VocabParallelEmbedding.get_sharded_to_full_mapping(self)
  VocabParallelEmbedding.weight_loader(self,param,loaded_weight)
  VocabParallelEmbeddingShardIndices.__post_init__(self)
  VocabParallelEmbeddingShardIndices.num_added_elements(self)
  VocabParallelEmbeddingShardIndices.num_added_elements_padded(self)
  VocabParallelEmbeddingShardIndices.num_added_vocab_padding(self)
  VocabParallelEmbeddingShardIndices.num_elements_padded(self)
  VocabParallelEmbeddingShardIndices.num_org_elements(self)
  VocabParallelEmbeddingShardIndices.num_org_elements_padded(self)
  VocabParallelEmbeddingShardIndices.num_org_vocab_padding(self)
  get_masked_input_and_mask(input_,org_vocab_start_index,org_vocab_end_index,num_org_vocab_padding,added_vocab_start_index,added_vocab_end_index)
  pad_vocab_size(vocab_size,pad_to)
  vocab_range_from_global_vocab_size(global_vocab_size,rank,world_size,offset)
  vocab_range_from_per_partition_vocab_size(per_partition_vocab_size,rank,offset)
