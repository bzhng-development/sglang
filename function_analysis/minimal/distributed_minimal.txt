distributed/communication_op.py:
  broadcast_tensor_dict(tensor_dict,src)
  tensor_model_parallel_all_gather(input_,dim)
  tensor_model_parallel_all_reduce(input_)
  tensor_model_parallel_gather(input_,dst,dim)
distributed/device_communicators/cuda_wrapper.py:
  CudaRTLibrary.CUDART_CHECK(self,result)
  CudaRTLibrary.__init__(self,so_file)
  CudaRTLibrary.cudaDeviceReset(self)
  CudaRTLibrary.cudaDeviceSynchronize(self)
  CudaRTLibrary.cudaFree(self,devPtr)
  CudaRTLibrary.cudaGetErrorString(self,error)
  CudaRTLibrary.cudaIpcGetMemHandle(self,devPtr)
  CudaRTLibrary.cudaIpcOpenMemHandle(self,handle)
  CudaRTLibrary.cudaMalloc(self,size)
  CudaRTLibrary.cudaMemcpy(self,dst,src,count)
  CudaRTLibrary.cudaMemset(self,devPtr,value,count)
  CudaRTLibrary.cudaSetDevice(self,device)
  find_loaded_library(lib_name)
distributed/device_communicators/custom_all_reduce.py:
  CustomAllreduce.__del__(self)
  CustomAllreduce.__init__(self,group,device,max_size)
  CustomAllreduce._gather_ipc_meta(self,shard_data)
  CustomAllreduce._get_ipc_meta(self,inp)
  CustomAllreduce.all_reduce(self,inp,out,registered)
  CustomAllreduce.all_reduce_reg(self,inp,out)
  CustomAllreduce.all_reduce_unreg(self,inp,out)
  CustomAllreduce.capture(self)
  CustomAllreduce.close(self)
  CustomAllreduce.create_shared_buffer(size_in_bytes,group)
  CustomAllreduce.custom_all_reduce(self,input)
  CustomAllreduce.free_shared_buffer(pointers,group)
  CustomAllreduce.register_buffer(self,inp)
  CustomAllreduce.register_graph_buffers(self)
  CustomAllreduce.should_custom_ar(self,inp)
  _can_p2p(rank,world_size)
distributed/device_communicators/custom_all_reduce_utils.py:
  can_actually_p2p(batch_src,batch_tgt)
  consumer(batch_tgt,producer_queue,consumer_queue,result_queue,cuda_visible_devices)
  gpu_p2p_access_check(src,tgt)
  is_full_nvlink(physical_device_ids,world_size)
  is_weak_contiguous(inp)
  producer(batch_src,producer_queue,consumer_queue,result_queue,cuda_visible_devices)
  update_environment_variables(envs)
  with_nvml_context(fn)
  wrapper(*args,**kwargs)
distributed/device_communicators/hpu_communicator.py:
  HpuCommunicator.__init__(self,group)
  HpuCommunicator.all_gather(self,x,dim)
  HpuCommunicator.all_reduce(self,x)
distributed/device_communicators/npu_communicator.py:
  NpuCommunicator.__init__(self,group)
  NpuCommunicator.all_gather(self,x,dim)
  NpuCommunicator.all_reduce(self,x)
distributed/device_communicators/pymscclpp.py:
  PyMscclppCommunicator.__init__(self,group,device,max_bytes)
  PyMscclppCommunicator.all_reduce(self,tensor,op)
  PyMscclppCommunicator.change_state(self,enable)
  PyMscclppCommunicator.pre_tune_config(self,dtype)
  PyMscclppCommunicator.should_mscclpp_allreduce(self,inp,op)
  mscclpp_bench_time(func,test_niter,warmup_niter)
  mscclpp_convert_to_bytes(size_str)
  mscclpp_is_weak_contiguous(inp)
distributed/device_communicators/pynccl.py:
  PyNcclCommunicator.__init__(self,group,device,library_path)
  PyNcclCommunicator.all_gather(self,output_tensor,input_tensor,stream,sizes)
  PyNcclCommunicator.all_reduce(self,tensor,op,stream)
  PyNcclCommunicator.broadcast(self,tensor,src,stream)
  PyNcclCommunicator.change_state(self,enable,stream)
  PyNcclCommunicator.deregister_comm_window(self,window)
  PyNcclCommunicator.group_end(self)
  PyNcclCommunicator.group_start(self)
  PyNcclCommunicator.recv(self,tensor,src,stream)
  PyNcclCommunicator.reduce_scatter(self,output_tensor,input_tensor,op,stream,sizes)
  PyNcclCommunicator.register_comm_window_raw(self,ptr,size)
  PyNcclCommunicator.send(self,tensor,dst,stream)
distributed/device_communicators/pynccl_allocator.py:
  get_nccl_mem_pool()
  is_symmetric_memory_enabled()
  set_graph_pool_id(graph_pool_id)
  use_symmetric_memory.__enter__(self)
  use_symmetric_memory.__exit__(self,exc_type,exc_val,exc_tb)
  use_symmetric_memory.__init__(self,group_coordinator)
  use_symmetric_memory.tag(self,tensor)
distributed/device_communicators/pynccl_wrapper.py:
  NCCLLibrary.NCCL_CHECK(self,result)
  NCCLLibrary.__init__(self,so_file)
  NCCLLibrary.ncclAllGather(self,sendbuff,recvbuff,count,datatype,comm,stream)
  NCCLLibrary.ncclAllReduce(self,sendbuff,recvbuff,count,datatype,op,comm,stream)
  NCCLLibrary.ncclBroadcast(self,sendbuff,recvbuff,count,datatype,root,comm,stream)
  NCCLLibrary.ncclCommDestroy(self,comm)
  NCCLLibrary.ncclCommInitRank(self,world_size,unique_id,rank)
  NCCLLibrary.ncclCommWindowDeregister(self,comm,window)
  NCCLLibrary.ncclCommWindowRegister(self,comm,buff,size,win_flags)
  NCCLLibrary.ncclGetErrorString(self,result)
  NCCLLibrary.ncclGetRawVersion(self)
  NCCLLibrary.ncclGetUniqueId(self)
  NCCLLibrary.ncclGetVersion(self)
  NCCLLibrary.ncclGroupEnd(self)
  NCCLLibrary.ncclGroupStart(self)
  NCCLLibrary.ncclRecv(self,recvbuff,count,datatype,src,comm,stream)
  NCCLLibrary.ncclReduce(self,sendbuff,recvbuff,count,datatype,op,root,comm,stream)
  NCCLLibrary.ncclReduceScatter(self,sendbuff,recvbuff,count,datatype,op,comm,stream)
  NCCLLibrary.ncclSend(self,sendbuff,count,datatype,dest,comm,stream)
  find_nccl_library()
  ncclDataTypeEnum.from_torch(cls,dtype)
  ncclRedOpTypeEnum.from_torch(cls,op)
distributed/device_communicators/quick_all_reduce.py:
  QuickAllReduce.__del__(self)
  QuickAllReduce.__init__(self,group,device)
  QuickAllReduce.close(self)
  QuickAllReduce.create_shared_buffer(self)
  QuickAllReduce.init_quick_all_reduce(self)
  QuickAllReduce.quick_all_reduce(self,inp,out)
  QuickAllReduce.should_quick_allreduce(self,inp)
  qr_rocm_arch_available()
distributed/device_communicators/shm_broadcast.py:
  MessageQueue.__init__(self,n_reader,n_local_reader,local_reader_ranks,max_chunk_bytes,max_chunks,connect_ip)
  MessageQueue.acquire_read(self)
  MessageQueue.acquire_write(self)
  MessageQueue.broadcast_object(self,obj)
  MessageQueue.create_from_handle(handle,rank)
  MessageQueue.create_from_process_group(pg,max_chunk_bytes,max_chunks,writer_rank)
  MessageQueue.dequeue(self)
  MessageQueue.enqueue(self,obj)
  MessageQueue.export_handle(self)
  MessageQueue.wait_until_ready(self)
  ShmRingBuffer.__del__(self)
  ShmRingBuffer.__init__(self,n_reader,max_chunk_bytes,max_chunks,name)
  ShmRingBuffer.__reduce__(self)
  ShmRingBuffer.get_data(self,current_idx)
  ShmRingBuffer.get_metadata(self,current_idx)
distributed/device_communicators/xpu_communicator.py:
  XpuCommunicator.__init__(self,group)
  XpuCommunicator.all_reduce(self,x)
  XpuCommunicator.gather(self,input_,rank_in_group,dst,dim)
distributed/naive_distributed.py:
  NaiveDistributed.__init__(self,rank,world_size,rendezvous)
  NaiveDistributed._get_path(interesting_rank)
  NaiveDistributed._read_one(interesting_rank)
  NaiveDistributed.all_gather_object(self,obj)
  NaiveDistributed.barrier(self)
  NaiveDistributed.get_rank(self)
  NaiveDistributed.get_world_size(self)
  NaiveDistributed.scatter(self,tensor,scatter_list,src)
  get_naive_distributed()
  set_naive_distributed(instance)
distributed/parallel_state.py:
  GroupCoordinator.__init__(self,group_ranks,local_rank,torch_distributed_backend,use_pynccl,use_pymscclpp,use_custom_allreduce,use_hpu_communicator,use_xpu_communicator,use_npu_communicator,use_message_queue_broadcaster,group_name)
  GroupCoordinator.__repr__(self)
  GroupCoordinator._all_gather_into_tensor(self,output,input)
  GroupCoordinator._all_gather_single(input_,sizes)
  GroupCoordinator._all_reduce_in_place(self,input_)
  GroupCoordinator._all_reduce_out_place(self,input_,outplace_all_reduce_method)
  GroupCoordinator.all_gather(self,input_,dim,output_tensor_list)
  GroupCoordinator.all_gather_into_tensor(self,output,input)
  GroupCoordinator.all_gather_object(self,obj)
  GroupCoordinator.all_gatherv(self,input_,sizes)
  GroupCoordinator.all_reduce(self,input_)
  GroupCoordinator.barrier(self)
  GroupCoordinator.broadcast(self,input_,src)
  GroupCoordinator.broadcast_object(self,obj,src)
  GroupCoordinator.broadcast_object_list(self,obj_list,src,group)
  GroupCoordinator.broadcast_tensor_dict(self,tensor_dict,src,group,metadata_group)
  GroupCoordinator.destroy(self)
  GroupCoordinator.first_rank(self)
  GroupCoordinator.gather(self,input_,dst,dim)
  GroupCoordinator.graph_capture(self,graph_capture_context)
  GroupCoordinator.is_first_rank(self)
  GroupCoordinator.is_last_rank(self)
  GroupCoordinator.last_rank(self)
  GroupCoordinator.next_rank(self)
  GroupCoordinator.prev_rank(self)
  GroupCoordinator.recv(self,size,dtype,src)
  GroupCoordinator.recv_object(self,src)
  GroupCoordinator.recv_tensor_dict(self,src,all_gather_group)
  GroupCoordinator.reduce_scatter(self,output,input_list)
  GroupCoordinator.reduce_scatter_tensor(self,output,input)
  GroupCoordinator.reduce_scatterv(self,input_,output,sizes)
  GroupCoordinator.send(self,tensor,dst)
  GroupCoordinator.send_object(self,obj,dst)
  GroupCoordinator.send_tensor_dict(self,tensor_dict,dst,all_gather_group)
  _get_unique_name(name)
  _register_group(group)
  _split_tensor_dict(tensor_dict)
  cleanup_dist_env_and_memory(shutdown_ray)
  destroy_distributed_environment()
  destroy_model_parallel()
  ensure_model_parallel_initialized(tensor_model_parallel_size,expert_model_parallel_size,pipeline_model_parallel_size,backend)
  get_moe_ep_group()
  get_moe_expert_parallel_rank()
  get_moe_expert_parallel_world_size()
  get_moe_tensor_parallel_rank()
  get_moe_tensor_parallel_world_size()
  get_moe_tp_group()
  get_pp_group()
  get_tensor_model_parallel_rank()
  get_tensor_model_parallel_world_size()
  get_tp_group()
  get_world_group()
  graph_capture()
  in_the_same_node_as(pg,source_rank)
  init_distributed_environment(world_size,rank,distributed_init_method,local_rank,backend,timeout)
  init_model_parallel_group(group_ranks,local_rank,backend,use_custom_allreduce,use_message_queue_broadcaster,group_name,use_mscclpp_allreduce)
  init_world_group(ranks,local_rank,backend)
  initialize_model_parallel(tensor_model_parallel_size,expert_model_parallel_size,pipeline_model_parallel_size,backend,duplicate_tp_group)
  inplace_all_reduce(tensor,group_name)
  inplace_all_reduce_fake(tensor,group_name)
  model_parallel_is_initialized()
  monkey_patch_vllm_parallel_state(reverse)
  outplace_all_reduce(tensor,group_name,outplace_all_reduce_method)
  outplace_all_reduce_fake(tensor,group_name,outplace_all_reduce_method)
  patch_tensor_parallel_group(tp_group)
  reg_all_gather_into_tensor(output,input,group_name)
  reg_all_gather_into_tensor_fake(output,input,group_name)
  set_custom_all_reduce(enable)
  set_mscclpp_all_reduce(enable)
  set_pdmux_status(enable_prefill_multiplexing)
distributed/utils.py:
  StatelessProcessGroup.__post_init__(self)
  StatelessProcessGroup.all_gather_obj(self,obj)
  StatelessProcessGroup.barrier(self)
  StatelessProcessGroup.broadcast_obj(self,obj,src)
  StatelessProcessGroup.create(host,port,rank,world_size,data_expiration_seconds)
  StatelessProcessGroup.expire_data(self)
  StatelessProcessGroup.recv_obj(self,src)
  StatelessProcessGroup.send_obj(self,obj,dst)
  divide(numerator,denominator)
  ensure_divisibility(numerator,denominator)
  get_pp_indices(num_hidden_layers,pp_rank,pp_size)
  split_tensor_along_last_dim(tensor,num_partitions,contiguous_split_chunks)
