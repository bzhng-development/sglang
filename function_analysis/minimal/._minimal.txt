_custom_ops.py:
  all_reduce(fa,inp,out,reg_buffer,reg_buffer_sz_bytes)
  all_reduce_reg(fa,inp,out)
  all_reduce_unreg(fa,inp,reg_buffer,out)
  allocate_meta_buffer(size)
  dispose(fa)
  dispose(fa)
  get_graph_buffer_ipc_meta(fa)
  get_graph_buffer_ipc_meta(fa)
  get_meta_buffer_ipc_handle(inp)
  init_custom_ar(ipc_tensors,rank_data,rank,full_nvlink)
  init_custom_ar(meta,rank_data,handles,offsets,rank,full_nvlink)
  init_custom_qr(rank,world_size,qr_max_size)
  meta_size()
  meta_size()
  mscclpp_allreduce(context,inp,out,nthreads,nblocks)
  mscclpp_generate_unique_id()
  mscclpp_init_context(unique_id,rank,world_size,scratch,put_buffer,nranks_per_node,rank_to_node,rank_to_ib,context_selection)
  qr_all_reduce(fa,inp,out,quant_level,cast_bf2half)
  qr_destroy(fa)
  qr_get_handle(fa)
  qr_max_size()
  qr_open_handles(fa,handles)
  register_buffer(fa,ipc_tensors)
  register_buffer(fa,t,handles,offsets)
  register_graph_buffers(fa,handles,offsets)
  register_graph_buffers(fa,handles,offsets)
aio_rwlock.py:
  RWLock.__init__(self)
  RWLock.reader_lock(self)
  RWLock.writer_lock(self)
  _ReaderLock.__init__(self,rwlock)
  _WriterLock.__init__(self,rwlock)
  async RWLock.acquire_reader(self)
  async RWLock.acquire_writer(self)
  async RWLock.release_reader(self)
  async RWLock.release_writer(self)
  async _ReaderLock.__aenter__(self)
  async _ReaderLock.__aexit__(self,exc_type,exc_val,exc_tb)
  async _WriterLock.__aenter__(self)
  async _WriterLock.__aexit__(self,exc_type,exc_val,exc_tb)
bench_utils.py:
  bench_kineto(fn,kernel_names,num_tests,suppress_kineto_output,trace_path,flush_l2,with_multiple_kernels)
  suppress_stdout_stderr.__enter__(self)
  suppress_stdout_stderr.__exit__(self,*_)
code_completion_parser.py:
  completion_template_exists(template_name)
  generate_completion_prompt(prompt,suffix,template_name)
  generate_completion_prompt_from_request(request)
  is_completion_template_defined()
  register_completion_template(template,override)
configs/chatglm.py:
  ChatGLMConfig.__init__(self,num_layers,padded_vocab_size,hidden_size,ffn_hidden_size,kv_channels,num_attention_heads,seq_length,hidden_dropout,attention_dropout,layernorm_epsilon,rmsnorm,apply_residual_connection_post_layernorm,post_layer_norm,add_bias_linear,add_qkv_bias,interleaved_qkv,bias_dropout_fusion,multi_query_attention,multi_query_group_num,apply_query_key_layer_scaling,attention_softmax_in_fp32,fp32_residual_connection,quantization_bit,pre_seq_len,prefix_projection,**kwargs)
configs/dbrx.py:
  DbrxAttentionConfig.__init__(self,attn_pdrop,clip_qkv,kv_n_heads,rope_theta,**kwargs)
  DbrxAttentionConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
  DbrxConfig.__init__(self,d_model,n_heads,n_layers,max_seq_len,vocab_size,resid_pdrop,emb_pdrop,attn_config,ffn_config,use_cache,initializer_range,output_router_logits,router_aux_loss_coef,**kwargs)
  DbrxFFNConfig.__init__(self,ffn_act_fn,ffn_hidden_size,moe_num_experts,moe_top_k,moe_jitter_eps,moe_loss_weight,moe_normalize_expert_weights,uniform_expert_assignment,**kwargs)
  DbrxFFNConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
configs/deepseekvl2.py:
  DeepseekV2Config.__init__(self,vocab_size,hidden_size,intermediate_size,moe_intermediate_size,num_hidden_layers,num_attention_heads,num_key_value_heads,n_shared_experts,n_routed_experts,ep_size,routed_scaling_factor,kv_lora_rank,q_lora_rank,qk_rope_head_dim,v_head_dim,qk_nope_head_dim,topk_method,n_group,topk_group,num_experts_per_tok,moe_layer_freq,first_k_dense_replace,norm_topk_prob,scoring_func,aux_loss_alpha,seq_aux,hidden_act,max_position_embeddings,initializer_range,rms_norm_eps,use_cache,pad_token_id,bos_token_id,eos_token_id,pretraining_tp,tie_word_embeddings,rope_theta,rope_scaling,attention_bias,attention_dropout,use_mla,**kwargs)
  DeepseekVL2Config.__init__(self,tile_tag,global_view_pos,candidate_resolutions,**kwargs)
  DeepseekVL2MlpProjectorConfig.__init__(self,projector_type,input_dim,n_embed,depth,mlp_ratio,downsample_ratio,**kwargs)
  DeepseekVL2VisionEncoderConfig.__init__(self,model_name,image_size,patch_size,width,layers,heads,mlp_ratio,global_pool,ignore_head,class_token,num_classes,use_checkpoint,**kwargs)
  DeepseekVLV2Processor.__call__(self,prompt,conversations,images,apply_sft_format,inference_mode,system_prompt,max_req_input_len,**kwargs)
  DeepseekVLV2Processor.__init__(self,tokenizer,candidate_resolutions,patch_size,downsample_ratio,image_mean,image_std,normalize,image_token,pad_token,add_special_token,sft_format,mask_prompt,ignore_id,**kwargs)
  DeepseekVLV2Processor.bos_id(self)
  DeepseekVLV2Processor.decode(self,t,**kwargs)
  DeepseekVLV2Processor.encode(self,text,bos,eos)
  DeepseekVLV2Processor.eos_id(self)
  DeepseekVLV2Processor.find_all_indices(self,messages,target_value)
  DeepseekVLV2Processor.format_messages_v2(self,messages,pil_images,max_req_input_len)
  DeepseekVLV2Processor.pad_id(self)
  DeepseekVLV2Processor.process_one(self,prompt,conversations,images,apply_sft_format,inference_mode,system_prompt,max_req_input_len,**kwargs)
  DeepseekVLV2Processor.tokenize_with_images(self,conversation,images,bos,eos,cropping,max_req_input_len)
  DictOutput.__contains__(self,key)
  DictOutput.__getitem__(self,item)
  DictOutput.__setitem__(self,key,value)
  DictOutput.items(self)
  DictOutput.keys(self)
  ImageTransform.__call__(self,pil_img)
  ImageTransform.__init__(self,mean,std,normalize)
  VLChatProcessorOutput.__len__(self)
  select_best_resolution(image_size,candidate_resolutions)
configs/device_config.py:
  DeviceConfig.__init__(self,device)
configs/exaone.py:
  ExaoneConfig.__init__(self,vocab_size,max_position_embeddings,hidden_size,num_layers,num_attention_heads,num_key_value_heads,intermediate_size,activation_function,rope_theta,rope_scaling,embed_dropout,attention_dropout,layer_norm_epsilon,initializer_range,use_cache,bos_token_id,eos_token_id,tie_word_embeddings,**kwargs)
configs/internvl.py:
  InternLM2Config.__init__(self,vocab_size,hidden_size,intermediate_size,num_hidden_layers,num_attention_heads,num_key_value_heads,hidden_act,max_position_embeddings,initializer_range,rms_norm_eps,use_cache,pad_token_id,bos_token_id,eos_token_id,tie_word_embeddings,bias,rope_theta,rope_scaling,attn_implementation,**kwargs)
  InternLM2Config._rope_scaling_validation(self)
  InternLM2Tokenizer.__init__(self,vocab_file,unk_token,bos_token,eos_token,pad_token,sp_model_kwargs,add_bos_token,add_eos_token,decode_with_prefix_space,clean_up_tokenization_spaces,**kwargs)
  InternLM2Tokenizer._convert_id_to_token(self,index)
  InternLM2Tokenizer._convert_token_to_id(self,token)
  InternLM2Tokenizer._maybe_add_prefix_space(self,tokens,decoded)
  InternLM2Tokenizer._tokenize(self,text)
  InternLM2Tokenizer.bos_token_id(self)
  InternLM2Tokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1)
  InternLM2Tokenizer.convert_tokens_to_string(self,tokens)
  InternLM2Tokenizer.create_token_type_ids_from_sequences(self,token_ids_0,token_ids_1)
  InternLM2Tokenizer.eos_token_id(self)
  InternLM2Tokenizer.get_special_tokens_mask(self,token_ids_0,token_ids_1,already_has_special_tokens)
  InternLM2Tokenizer.get_vocab(self)
  InternLM2Tokenizer.no_prefix_space_tokens(self)
  InternLM2Tokenizer.save_vocabulary(self,save_directory,filename_prefix)
  InternLM2Tokenizer.vocab_size(self)
  InternVLChatConfig.__init__(self,vision_config,llm_config,use_backbone_lora,use_llm_lora,pad2square,select_layer,force_image_size,downsample_ratio,template,dynamic_image_size,use_thumbnail,ps_version,min_dynamic_patch,max_dynamic_patch,**kwargs)
  InternVLChatConfig.to_dict(self)
  InternVisionConfig.__init__(self,num_channels,patch_size,image_size,qkv_bias,hidden_size,num_attention_heads,intermediate_size,qk_normalization,num_hidden_layers,use_flash_attn,hidden_act,layer_norm_eps,dropout,drop_path_rate,attention_dropout,initializer_range,initializer_factor,**kwargs)
  InternVisionConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
configs/janus_pro.py:
  AlignerConfig.__init__(self,**kwargs)
  DictOutput.__contains__(self,key)
  DictOutput.__getitem__(self,item)
  DictOutput.__setitem__(self,key,value)
  DictOutput.items(self)
  DictOutput.keys(self)
  DictToObject.__init__(self,dictionary)
  GenAlignerConfig.__init__(self,**kwargs)
  GenHeadConfig.__init__(self,**kwargs)
  GenVisionConfig.__init__(self,**kwargs)
  MultiModalityConfig.__init__(self,**kwargs)
  VLChatProcessor.__call__(self,prompt,conversations,images,force_batchify,**kwargs)
  VLChatProcessor.__init__(self,image_processor,tokenizer,image_tag,image_start_tag,image_end_tag,pad_tag,num_image_tokens,add_special_token,sft_format,mask_prompt,ignore_id,**kwargs)
  VLChatProcessor.add_image_token(self,image_indices,input_ids)
  VLChatProcessor.batchify(self,prepare_list)
  VLChatProcessor.image_end_id(self)
  VLChatProcessor.image_end_token(self)
  VLChatProcessor.image_id(self)
  VLChatProcessor.image_start_id(self)
  VLChatProcessor.image_start_token(self)
  VLChatProcessor.image_token(self)
  VLChatProcessor.pad_id(self)
  VLChatProcessor.process_one(self,prompt,images,**kwargs)
  VLChatProcessorOutput.__len__(self)
  VLMImageProcessor.__init__(self,image_size,min_size,image_mean,image_std,rescale_factor,do_normalize,**kwargs)
  VLMImageProcessor.default_shape(self)
  VLMImageProcessor.preprocess(self,images,return_tensors,**kwargs)
  VLMImageProcessor.resize(pil_img,size,interpolation,antialias)
  VLMImageProcessor.resize(self,pil_img)
  VLMImageProcessorConfig.__init__(self,image_size,min_size,image_mean,image_std,rescale_factor,do_normalize,**kwargs)
  VisionConfig.__init__(self,**kwargs)
configs/kimi_vl.py:
  KimiVLConfig.__init__(self,vision_config,text_config,ignore_index,media_placeholder_token_id,pad_token_id,**kwargs)
configs/kimi_vl_moonvit.py:
  MoonViTConfig.__init__(self,patch_size,init_pos_emb_height,init_pos_emb_width,num_attention_heads,num_hidden_layers,hidden_size,intermediate_size,merge_kernel_size,**kwargs)
configs/load_config.py:
  LoadConfig.__post_init__(self)
  LoadConfig._verify_load_format(self)
configs/longcat_flash.py:
  LongcatFlashConfig.__init__(self,vocab_size,hidden_size,intermediate_size,ffn_hidden_size,expert_ffn_hidden_size,num_layers,num_hidden_layers,num_attention_heads,ep_size,kv_lora_rank,q_lora_rank,qk_rope_head_dim,qk_nope_head_dim,v_head_dim,n_routed_experts,moe_topk,norm_topk_prob,max_position_embeddings,rms_norm_eps,use_cache,pad_token_id,bos_token_id,eos_token_id,pretraining_tp,tie_word_embeddings,rope_theta,rope_scaling,attention_bias,attention_dropout,mla_scale_q_lora,mla_scale_kv_lora,torch_dtype,params_dtype,rounter_params_dtype,router_bias,topk_method,routed_scaling_factor,zero_expert_num,zero_expert_type,nextn_use_scmoe,num_nextn_predict_layers,**kwargs)
configs/model_config.py:
  ModelConfig.__init__(self,model_path,trust_remote_code,revision,context_length,model_override_args,is_embedding,enable_multimodal,dtype,quantization,override_config_file,is_draft_model,hybrid_kvcache_ratio,model_impl)
  ModelConfig._parse_quant_hf_config(self)
  ModelConfig._verify_dual_chunk_attention_config(self)
  ModelConfig._verify_quantization(self)
  ModelConfig.from_server_args(server_args,model_path,**kwargs)
  ModelConfig.get_hf_eos_token_id(self)
  ModelConfig.get_num_attention_heads(self,tensor_parallel_size)
  ModelConfig.get_num_kv_heads(self,tensor_parallel_size)
  ModelConfig.get_total_num_attention_heads(self)
  ModelConfig.get_total_num_kv_heads(self)
  ModelConfig.maybe_pull_model_tokenizer_from_remote(self)
  _get_and_verify_dtype(config,dtype)
  get_hybrid_layer_ids(model_architectures,num_hidden_layers)
  is_audio_model(model_architectures)
  is_encoder_decoder_model(model_architectures)
  is_generation_model(model_architectures,is_embedding)
  is_hybrid_model(model_architectures,hybrid_kvcache_ratio,context_length,attention_chunk_size)
  is_image_gen_model(model_architectures)
  is_multimodal_chunked_prefill_supported(model_architectures)
  is_multimodal_gen_model(model_architectures)
  is_multimodal_model(model_architectures)
  yarn_get_mscale(scale,mscale)
configs/step3_vl.py:
  Step3TextConfig.__init__(self,hidden_size,intermediate_size,num_attention_heads,num_attention_groups,num_hidden_layers,max_seq_len,vocab_size,rms_norm_eps,moe_intermediate_size,moe_num_experts,moe_top_k,rope_theta,rope_scaling,max_position_embedding,share_expert_dim,share_q_dim,head_dim,norm_expert_weight,moe_layers_enum,**kwargs)
  Step3VLConfig.__init__(self,vision_config,text_config,understand_projector_stride,projector_bias,image_token_id,**kwargs)
  Step3VisionEncoderConfig.__init__(self,hidden_size,intermediate_size,output_hidden_size,num_hidden_layers,num_attention_heads,num_channels,image_size,patch_size,hidden_act,layer_norm_eps,**kwargs)
configs/update_config.py:
  adjust_config_with_unaligned_cpu_tp(model_config,load_config,tp_size)
  get_moe_padding_size(weight_block_size)
  get_num_heads_padding_size(tp_size,weight_block_size)
  may_get_weight_block_size(model_config,load_config)
  update_intermediate_size(model_config,attr_name,intermediate_padding_size)
configs/utils.py:
  register_image_processor(config,image_processor)
  register_processor(config,processor)
connector/__init__.py:
  create_remote_connector(url,**kwargs)
  get_connector_type(client)
connector/base_connector.py:
  BaseConnector.__del__(self)
  BaseConnector.__enter__(self)
  BaseConnector.__exit__(self,exc_type,exc_value,traceback)
  BaseConnector.__init__(self,url)
  BaseConnector._close_by_signal(self,existing_handler)
  BaseConnector.close(self)
  BaseConnector.get_local_dir(self)
  BaseConnector.new_handler(signum,frame)
  BaseConnector.pull_files(self,allow_pattern,ignore_pattern)
  BaseConnector.weight_iterator(self,rank)
  BaseFileConnector.glob(self,allow_pattern)
  BaseKVConnector.get(self,key)
  BaseKVConnector.getstr(self,key)
  BaseKVConnector.list(self,prefix)
  BaseKVConnector.set(self,key,obj)
  BaseKVConnector.setstr(self,key,obj)
connector/redis.py:
  RedisConnector.__init__(self,url)
  RedisConnector.close(self)
  RedisConnector.get(self,key)
  RedisConnector.getstr(self,key)
  RedisConnector.list(self,prefix)
  RedisConnector.pull_files(self,allow_pattern,ignore_pattern)
  RedisConnector.set(self,key,tensor)
  RedisConnector.setstr(self,key,obj)
  RedisConnector.weight_iterator(self,rank)
connector/s3.py:
  S3Connector.__init__(self,url)
  S3Connector.close(self)
  S3Connector.glob(self,allow_pattern)
  S3Connector.pull_files(self,allow_pattern,ignore_pattern)
  S3Connector.weight_iterator(self,rank)
  _filter_allow(paths,patterns)
  _filter_ignore(paths,patterns)
  list_files(s3,path,allow_pattern,ignore_pattern)
connector/serde/__init__.py:
  create_serde(serde_type)
connector/serde/safe_serde.py:
  SafeDeserializer.__init__(self)
  SafeDeserializer.from_bytes(self,b)
  SafeDeserializer.from_bytes_normal(self,b)
  SafeSerializer.__init__(self)
  SafeSerializer.to_bytes(self,t)
connector/serde/serde.py:
  Deserializer.__init__(self,dtype)
  Deserializer.from_bytes(self,bs)
  Serializer.to_bytes(self,t)
connector/utils.py:
  parse_model_name(url)
  pull_files_from_db(connector,model_name,allow_pattern,ignore_pattern)
constrained/base_grammar_backend.py:
  BaseGrammarBackend.__init__(self)
  BaseGrammarBackend._init_value_dispatch(self,key)
  BaseGrammarBackend._not_supported(self,key_type,key_string)
  BaseGrammarBackend.dispatch_ebnf(self,key_string)
  BaseGrammarBackend.dispatch_fallback(self,key_type,key_string)
  BaseGrammarBackend.dispatch_json(self,key_string)
  BaseGrammarBackend.dispatch_regex(self,key_string)
  BaseGrammarBackend.dispatch_structural_tag(self,key_string)
  BaseGrammarBackend.get_cached_or_future_value(self,key)
  BaseGrammarBackend.reset(self)
  BaseGrammarBackend.set_cache(self,key,value)
  BaseGrammarObject.__init__(self)
  BaseGrammarObject.accept_token(self,token)
  BaseGrammarObject.allocate_vocab_mask(self,vocab_size,batch_size,device)
  BaseGrammarObject.apply_vocab_mask(logits,vocab_mask)
  BaseGrammarObject.copy(self)
  BaseGrammarObject.fill_vocab_mask(self,vocab_mask,idx)
  BaseGrammarObject.finished(self)
  BaseGrammarObject.finished(self,finished)
  BaseGrammarObject.is_terminated(self)
  BaseGrammarObject.jump_and_retokenize(self,old_output_ids,new_output_ids,next_state)
  BaseGrammarObject.jump_forward_str_state(self,helper)
  BaseGrammarObject.move_vocab_mask(vocab_mask,device)
  BaseGrammarObject.rollback(self,k)
  BaseGrammarObject.try_jump_forward(self,tokenizer)
  create_grammar_backend(server_args,tokenizer,vocab_size,eos_token_ids)
constrained/llguidance_backend.py:
  GuidanceBackend.__init__(self,tokenizer,whitespace_pattern,n_vocab)
  GuidanceBackend._from_serialized(self,serialized_grammar)
  GuidanceBackend.dispatch_ebnf(self,key_string)
  GuidanceBackend.dispatch_json(self,key_string)
  GuidanceBackend.dispatch_regex(self,key_string)
  GuidanceBackend.dispatch_structural_tag(self,key_string)
  GuidanceGrammar.__init__(self,llguidance_tokenizer,serialized_grammar)
  GuidanceGrammar.accept_token(self,token)
  GuidanceGrammar.allocate_vocab_mask(self,vocab_size,batch_size,device)
  GuidanceGrammar.apply_vocab_mask(logits,vocab_mask)
  GuidanceGrammar.copy(self)
  GuidanceGrammar.fill_vocab_mask(self,vocab_mask,idx)
  GuidanceGrammar.jump_and_retokenize(self,old_output_ids,new_output_ids,next_state)
  GuidanceGrammar.jump_forward_str_state(self,helper)
  GuidanceGrammar.move_vocab_mask(vocab_mask,device)
  GuidanceGrammar.try_jump_forward(self,tokenizer)
constrained/outlines_backend.py:
  OutlinesGrammar.__init__(self,guide,jump_forward_map)
  OutlinesGrammar.accept_token(self,token)
  OutlinesGrammar.allocate_vocab_mask(self,vocab_size,batch_size,device)
  OutlinesGrammar.apply_vocab_mask(logits,vocab_mask)
  OutlinesGrammar.copy(self)
  OutlinesGrammar.fill_vocab_mask(self,vocab_mask,idx)
  OutlinesGrammar.jump_and_retokenize(self,old_output_ids,new_output_ids,next_state)
  OutlinesGrammar.jump_forward_str_state(self,helper)
  OutlinesGrammar.move_vocab_mask(vocab_mask,device)
  OutlinesGrammar.try_jump_forward(self,tokenizer)
  OutlinesGrammarBackend.__init__(self,tokenizer,whitespace_pattern)
  OutlinesGrammarBackend._compile_regex(self,regex)
  OutlinesGrammarBackend.dispatch_ebnf(self,key_string)
  OutlinesGrammarBackend.dispatch_json(self,key_string)
  OutlinesGrammarBackend.dispatch_regex(self,key_string)
  OutlinesGrammarBackend.dispatch_structural_tag(self,key_string)
  OutlinesGrammarBackend.fset(self,value)
  build_regex_from_object(object,whitespace_pattern)
constrained/outlines_jump_forward.py:
  OutlinesJumpForwardMap.__init__(self,regex_string)
  OutlinesJumpForwardMap.is_jump_forward_symbol_state(self,state)
  OutlinesJumpForwardMap.jump_forward_byte(self,state)
  OutlinesJumpForwardMap.jump_forward_symbol(self,state)
  disk_cache(expire,typed,ignore)
  init_state_to_jump_forward(regex_string)
  test_main(regex_string)
constrained/reasoner_grammar_backend.py:
  ReasonerGrammarBackend.__init__(self,grammar_backend,think_end_id)
  ReasonerGrammarBackend._init_value_dispatch(self,key)
  ReasonerGrammarObject.__init__(self,grammar,think_end_id)
  ReasonerGrammarObject.accept_token(self,token)
  ReasonerGrammarObject.allocate_vocab_mask(self,vocab_size,batch_size,device)
  ReasonerGrammarObject.apply_vocab_mask(self)
  ReasonerGrammarObject.copy(self)
  ReasonerGrammarObject.fill_vocab_mask(self,vocab_mask,idx)
  ReasonerGrammarObject.finished(self)
  ReasonerGrammarObject.finished(self,finished)
  ReasonerGrammarObject.jump_and_retokenize(self,old_output_ids,new_output_ids,next_state)
  ReasonerGrammarObject.jump_forward_str_state(self,helper)
  ReasonerGrammarObject.move_vocab_mask(self,vocab_mask,device)
  ReasonerGrammarObject.try_jump_forward(self,tokenizer)
constrained/triton_ops/bitmask_ops.py:
  apply_token_bitmask_inplace_kernel(logits_ptr,bitmask_ptr,indices_ptr,num_rows,vocab_size,logits_strides,bitmask_strides,NUM_SMS,BLOCK_SIZE)
  apply_token_bitmask_inplace_triton(logits,bitmask,indices)
constrained/xgrammar_backend.py:
  XGrammarGrammar.__init__(self,matcher,vocab_size,ctx,override_stop_tokens,key_string)
  XGrammarGrammar.__repr__(self)
  XGrammarGrammar.accept_token(self,token)
  XGrammarGrammar.allocate_vocab_mask(self,vocab_size,batch_size,device)
  XGrammarGrammar.apply_vocab_mask(self,logits,vocab_mask)
  XGrammarGrammar.copy(self)
  XGrammarGrammar.fill_vocab_mask(self,vocab_mask,idx)
  XGrammarGrammar.is_terminated(self)
  XGrammarGrammar.jump_and_retokenize(self,old_output_ids,new_output_ids,next_state)
  XGrammarGrammar.jump_forward_str_state(self,helper)
  XGrammarGrammar.move_vocab_mask(vocab_mask,device)
  XGrammarGrammar.rollback(self,k)
  XGrammarGrammar.try_jump_forward(self,tokenizer)
  XGrammarGrammarBackend.__init__(self,tokenizer,vocab_size,model_eos_token_ids)
  XGrammarGrammarBackend._from_context(self,ctx,key_string)
  XGrammarGrammarBackend.dispatch_ebnf(self,key_string)
  XGrammarGrammarBackend.dispatch_json(self,key_string)
  XGrammarGrammarBackend.dispatch_regex(self,key_string)
  XGrammarGrammarBackend.dispatch_structural_tag(self,key_string)
  XGrammarGrammarBackend.reset(self)
conversation.py:
  Conversation.append_audio(self,audio)
  Conversation.append_image(self,image,detail)
  Conversation.append_message(self,role,message)
  Conversation.append_video(self,video)
  Conversation.copy(self)
  Conversation.dict(self)
  Conversation.get_prompt(self)
  Conversation.set_system_message(self,system_message)
  Conversation.to_gradio_chatbot(self)
  Conversation.to_openai_api_messages(self)
  Conversation.update_last_message(self,message)
  _get_full_multimodal_text_prompt(modality_token,modality_count,text_prompt)
  chat_template_exists(template_name)
  generate_chat_conv(request,template_name)
  generate_embedding_convs(texts,images,template_name)
  get_conv_template_by_model_path(model_path)
  get_model_type(model_path)
  match_deepseek_janus_pro(model_path)
  match_deepseek_vl(model_path)
  match_internvl(model_path)
  match_minicpm(model_path)
  match_phi_4_mm(model_path)
  match_qwen_chat_ml(model_path)
  match_vicuna(model_path)
  register_conv_template(template,override)
  register_conv_template_matching_function(func)
custom_op.py:
  CustomOp.__init__(self)
  CustomOp.dispatch_forward(self)
  CustomOp.enter_torch_compile(self,num_tokens)
  CustomOp.forward(self,*args,**kwargs)
  CustomOp.forward_cpu(self,*args,**kwargs)
  CustomOp.forward_cuda(self,*args,**kwargs)
  CustomOp.forward_hip(self,*args,**kwargs)
  CustomOp.forward_hpu(self,*args,**kwargs)
  CustomOp.forward_native(self,*args,**kwargs)
  CustomOp.forward_npu(self,*args,**kwargs)
  CustomOp.forward_xpu(self,*args,**kwargs)
  CustomOp.leave_torch_compile(self)
debug_utils/dump_comparator.py:
  _calc_rel_diff(x,y)
  check_tensor_pair(path_baseline,path_target)
  main(args)
  read_meta(directory)
debug_utils/dumper.py:
  _Dumper.__init__(self)
  _Dumper.dump(self,name,value,**kwargs)
  _Dumper.on_forward_pass_start(self)
  _get_partial_name()
  get_truncated_value(value)
debug_utils/text_comparator.py:
  _compute_df_meta(df_input)
  _compute_df_raw(args)
  _compute_str_prefix_len(a,b)
  _handle_one_prompt(df_one_prompt)
  _read_df_raw(path,category,trial_index)
  _transform_df_input(df)
  main(args)
disaggregation/ascend/conn.py:
  AscendKVManager.init_engine(self)
  AscendKVManager.process_layer(src_ptr,dst_ptr,item_len)
  AscendKVManager.process_layers(layers_params)
  AscendKVManager.register_buffer_to_engine(self)
  AscendKVManager.send_kvcache(self,mooncake_session_id,prefill_kv_indices,dst_kv_ptrs,dst_kv_indices,executor)
  AscendKVManager.set_transfer_blocks(src_ptr,dst_ptr,item_len)
disaggregation/ascend/transfer_engine.py:
  AscendTransferEngine.__init__(self,hostname,npu_id,disaggregation_mode)
  AscendTransferEngine.batch_register(self,ptrs,lengths)
  AscendTransferEngine.initialize(self)
disaggregation/base/conn.py:
  BaseKVBootstrapServer.__init__(self,port)
  BaseKVManager.__init__(self,args,disaggregation_mode,server_args,is_mla_backend)
  BaseKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room)
  BaseKVReceiver.failure_exception(self)
  BaseKVReceiver.init(self,kv_indices,aux_index)
  BaseKVReceiver.poll(self)
  BaseKVSender.__init__(self,mgr,bootstrap_addr,bootstrap_room,dest_tp_ranks,pp_rank)
  BaseKVSender.failure_exception(self)
  BaseKVSender.init(self,num_kv_indices,aux_index)
  BaseKVSender.poll(self)
  BaseKVSender.send(self,kv_indices)
disaggregation/common/conn.py:
  CommonKVBootstrapServer.__init__(self,port)
  CommonKVBootstrapServer._run_server(self)
  CommonKVBootstrapServer._setup_routes(self)
  CommonKVBootstrapServer.close(self)
  CommonKVBootstrapServer.poll(self)
  CommonKVBootstrapServer.run(self)
  CommonKVManager.__init__(self,args,disaggregation_mode,server_args,is_mla_backend)
  CommonKVManager._connect(self,endpoint,is_ipv6)
  CommonKVManager._register_to_bootstrap(self)
  CommonKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room,data_parallel_rank)
  CommonKVReceiver._connect(cls,endpoint,is_ipv6)
  CommonKVReceiver._connect_to_bootstrap_server(cls,bootstrap_info)
  CommonKVReceiver._get_bootstrap_info_from_server(self,engine_rank,target_dp_group)
  CommonKVReceiver._get_prefill_dp_size_from_server(self)
  CommonKVReceiver._register_kv_args(self)
  CommonKVReceiver.failure_exception(self)
  async CommonKVBootstrapServer._handle_route(self,request)
  async CommonKVBootstrapServer._handle_route_get(self,request)
  async CommonKVBootstrapServer._handle_route_put(self,request)
disaggregation/common/utils.py:
  FastQueue.__init__(self)
  FastQueue.get(self)
  FastQueue.put(self,item)
  group_concurrent_contiguous(src_indices,dst_indices)
disaggregation/decode.py:
  DecodePreallocQueue.__init__(self,req_to_token_pool,token_to_kv_pool_allocator,draft_token_to_kv_pool,req_to_metadata_buffer_idx_allocator,metadata_buffers,scheduler,transfer_queue,tree_cache,gloo_group,tp_rank,tp_size,dp_size,gpu_id,bootstrap_port,max_total_num_tokens,prefill_pp_size,num_reserved_decode_tokens,transfer_backend)
  DecodePreallocQueue._allocatable_tokens(self,retractable_tokens,count_retracted)
  DecodePreallocQueue._check_if_req_exceed_kv_capacity(self,req)
  DecodePreallocQueue._init_kv_manager(self)
  DecodePreallocQueue._pre_alloc(self,req)
  DecodePreallocQueue._update_handshake_waiters(self)
  DecodePreallocQueue.add(self,req,is_retracted)
  DecodePreallocQueue.extend(self,reqs,is_retracted)
  DecodePreallocQueue.num_tokens_pre_allocated(self)
  DecodePreallocQueue.pop_preallocated(self)
  DecodePreallocQueue.resume_retracted_reqs(self)
  DecodeReqToTokenPool.__init__(self,size,max_context_len,device,enable_memory_saver,pre_alloc_size)
  DecodeReqToTokenPool.alloc(self,need_size)
  DecodeReqToTokenPool.available_size(self)
  DecodeReqToTokenPool.clear(self)
  DecodeReqToTokenPool.free(self,free_index)
  DecodeReqToTokenPool.write(self,indices,values)
  DecodeTransferQueue.__init__(self,gloo_group,req_to_metadata_buffer_idx_allocator,tp_rank,metadata_buffers,scheduler,tree_cache)
  DecodeTransferQueue.add(self,decode_req)
  DecodeTransferQueue.extend(self,decode_reqs)
  DecodeTransferQueue.pop_transferred(self)
  SchedulerDisaggregationDecodeMixin._prepare_idle_batch_and_run(self,batch,delay_process)
  SchedulerDisaggregationDecodeMixin.event_loop_normal_disagg_decode(self)
  SchedulerDisaggregationDecodeMixin.event_loop_overlap_disagg_decode(self)
  SchedulerDisaggregationDecodeMixin.get_new_prebuilt_batch(self)
  SchedulerDisaggregationDecodeMixin.get_next_disagg_decode_batch_to_run(self)
  SchedulerDisaggregationDecodeMixin.process_decode_queue(self)
disaggregation/decode_schedule_batch_mixin.py:
  ScheduleBatchDisaggregationDecodeMixin.prepare_for_prebuilt_extend(self)
  ScheduleBatchDisaggregationDecodeMixin.process_prebuilt_extend(self,server_args,model_config)
disaggregation/fake/conn.py:
  FakeKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room,data_parallel_rank)
  FakeKVReceiver.failure_exception(self)
  FakeKVReceiver.init(self,kv_indices,aux_index)
  FakeKVReceiver.poll(self)
  FakeKVSender.__init__(self,mgr,bootstrap_addr,bootstrap_room,dest_tp_ranks,pp_rank)
  FakeKVSender.failure_exception(self)
  FakeKVSender.init(self,kv_indices,aux_index)
  FakeKVSender.poll(self)
  FakeKVSender.send(self,kv_indices)
disaggregation/kv_events.py:
  EventPublisher.__init__(self,attn_dp_rank)
  EventPublisher.publish(self,events)
  EventPublisher.shutdown(self)
  EventPublisherFactory.create(cls,config,attn_dp_rank)
  EventPublisherFactory.register_publisher(cls,name,ctor)
  KVEventsConfig.from_cli(cls,cli_value)
  NullEventPublisher.publish(self,events)
  NullEventPublisher.shutdown(self)
  ZmqEventPublisher.__init__(self,attn_dp_rank,endpoint,replay_endpoint,buffer_steps,hwm,max_queue_size,topic)
  ZmqEventPublisher._publisher_thread(self)
  ZmqEventPublisher._service_replay(self)
  ZmqEventPublisher._socket_setup(self)
  ZmqEventPublisher.offset_endpoint_port(endpoint,data_parallel_rank)
  ZmqEventPublisher.publish(self,events)
  ZmqEventPublisher.shutdown(self)
disaggregation/launch_lb.py:
  LBArgs.add_cli_args(parser)
  LBArgs.from_cli_args(cls,args)
  main()
disaggregation/mini_lb.py:
  MiniLoadBalancer.__init__(self,prefill_configs,decode_servers,timeout)
  MiniLoadBalancer.add_decode_server(self,new_decode_server)
  MiniLoadBalancer.add_prefill_server(self,new_prefill_config)
  MiniLoadBalancer.select_pair(self)
  _generate_bootstrap_room()
  _get_request_batch_size(request)
  async MiniLoadBalancer.generate(self,modified_request,prefill_server,decode_server,endpoint)
  async MiniLoadBalancer.generate_stream(self,modified_request,prefill_server,decode_server,endpoint)
  async MiniLoadBalancer.stream_results()
  async _forward_to_backend(request_data,endpoint_name)
  async flush_cache()
  async get_model_info()
  async get_models()
  async get_server_info()
  async handle_chat_completion_request(request_data)
  async handle_completion_request(request_data)
  async handle_generate_request(request_data)
  async health_check()
  async health_check()
  async register(obj)
  run(prefill_configs,decode_addrs,host,port,timeout)
  setup_logger()
disaggregation/mooncake/conn.py:
  AuxDataCodec.deserialize_data_to_buffer(kv_args,buffer_index,aux_index,data)
  AuxDataCodec.serialize_data_from_buffer(src_addr,data_length)
  KVArgsRegisterInfo.from_zmq(cls,msg)
  KVTransferError.__init__(self,bootstrap_room,failure_reason)
  KVTransferError.__str__(self)
  MooncakeKVBootstrapServer.__init__(self,port)
  MooncakeKVBootstrapServer._run_server(self)
  MooncakeKVBootstrapServer._setup_routes(self)
  MooncakeKVBootstrapServer.close(self)
  MooncakeKVBootstrapServer.poll(self)
  MooncakeKVBootstrapServer.run(self)
  MooncakeKVManager.__init__(self,args,disaggregation_mode,server_args,is_mla_backend)
  MooncakeKVManager._bind_server_socket(self)
  MooncakeKVManager._connect(self,endpoint,is_ipv6)
  MooncakeKVManager._handle_aux_data(self,msg)
  MooncakeKVManager._handle_node_failure(self,failed_bootstrap_addr)
  MooncakeKVManager._register_to_bootstrap(self)
  MooncakeKVManager._transfer_data(self,mooncake_session_id,transfer_blocks)
  MooncakeKVManager.add_transfer_request(self,bootstrap_room,kv_indices,index_slice,is_last,aux_index)
  MooncakeKVManager.bootstrap_thread()
  MooncakeKVManager.check_status(self,bootstrap_room)
  MooncakeKVManager.decode_thread()
  MooncakeKVManager.get_session_id(self)
  MooncakeKVManager.heartbeat_checker()
  MooncakeKVManager.init_engine(self)
  MooncakeKVManager.process_layer(src_ptr,dst_ptr,item_len)
  MooncakeKVManager.process_layer_tp_aware(layer_params)
  MooncakeKVManager.process_layers(layers_params)
  MooncakeKVManager.record_failure(self,bootstrap_room,failure_reason)
  MooncakeKVManager.register_buffer_to_engine(self)
  MooncakeKVManager.send_aux(self,req,prefill_aux_index,dst_aux_ptrs)
  MooncakeKVManager.send_aux_data_to_endpoint(self,remote,dst_port,room,buffer_index,aux_index,data)
  MooncakeKVManager.send_aux_tcp(self,req,prefill_aux_index,dst_aux_ptrs)
  MooncakeKVManager.send_kvcache(self,mooncake_session_id,prefill_kv_indices,dst_kv_ptrs,dst_kv_indices,executor)
  MooncakeKVManager.send_kvcache_slice(self,mooncake_session_id,prefill_kv_indices,dst_kv_ptrs,dst_kv_indices,dst_tp_rank,dst_attn_tp_size,dst_kv_item_len,executor)
  MooncakeKVManager.set_transfer_blocks(src_ptr,dst_ptr,item_len)
  MooncakeKVManager.start_decode_thread(self)
  MooncakeKVManager.start_prefill_thread(self)
  MooncakeKVManager.sync_status_to_decode_endpoint(self,remote,dst_port,room,status,prefill_rank)
  MooncakeKVManager.transfer_worker(self,queue,executor)
  MooncakeKVManager.update_status(self,bootstrap_room,status)
  MooncakeKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room,data_parallel_rank)
  MooncakeKVReceiver._connect(cls,endpoint,is_ipv6)
  MooncakeKVReceiver._connect_to_bootstrap_server(cls,bootstrap_info)
  MooncakeKVReceiver._get_bootstrap_info_from_server(self,engine_rank,target_dp_group,target_pp_rank)
  MooncakeKVReceiver._get_prefill_parallel_info_from_server(self)
  MooncakeKVReceiver._register_kv_args(self)
  MooncakeKVReceiver.abort(self)
  MooncakeKVReceiver.clear(self)
  MooncakeKVReceiver.failure_exception(self)
  MooncakeKVReceiver.init(self,kv_indices,aux_index)
  MooncakeKVReceiver.poll(self)
  MooncakeKVSender.__init__(self,mgr,bootstrap_addr,bootstrap_room,dest_tp_ranks,pp_rank)
  MooncakeKVSender.abort(self)
  MooncakeKVSender.clear(self)
  MooncakeKVSender.failure_exception(self)
  MooncakeKVSender.init(self,num_kv_indices,aux_index)
  MooncakeKVSender.poll(self)
  MooncakeKVSender.send(self,kv_indices)
  TransferInfo.from_zmq(cls,msg)
  async MooncakeKVBootstrapServer._handle_health_check(self,request)
  async MooncakeKVBootstrapServer._handle_route(self,request)
  async MooncakeKVBootstrapServer._handle_route_get(self,request)
  async MooncakeKVBootstrapServer._handle_route_put(self,request)
disaggregation/mooncake/transfer_engine.py:
  MooncakeTransferEngine.__init__(self,hostname,gpu_id,ib_device)
  MooncakeTransferEngine.batch_deregister(self,ptrs)
  MooncakeTransferEngine.batch_register(self,ptrs,lengths)
  MooncakeTransferEngine.batch_transfer_sync(self,session_id,buffers,peer_buffer_addresses,lengths)
  MooncakeTransferEngine.deregister(self,ptr)
  MooncakeTransferEngine.get_session_id(self)
  MooncakeTransferEngine.initialize(self,hostname,device_name)
  MooncakeTransferEngine.register(self,ptr,length)
  MooncakeTransferEngine.transfer_sync(self,session_id,buffer,peer_buffer_address,length)
disaggregation/nixl/conn.py:
  KVArgsRegisterInfo.from_zmq(cls,msg)
  NixlKVManager.__init__(self,args,disaggregation_mode,server_args,is_mla_backend)
  NixlKVManager._add_remote_peer(self,decode_kv_args)
  NixlKVManager._bind_server_socket(self)
  NixlKVManager._start_bootstrap_thread(self)
  NixlKVManager.add_transfer_request(self,bootstrap_room,kv_indices,index_slice,is_last,chunk_id,aux_index)
  NixlKVManager.bootstrap_thread()
  NixlKVManager.check_status(self,bootstrap_room)
  NixlKVManager.check_transfer_done(self,room)
  NixlKVManager.register_buffer_to_engine(self)
  NixlKVManager.send_aux(self,peer_name,prefill_aux_index,dst_aux_ptrs,dst_aux_index,notif)
  NixlKVManager.send_kvcache(self,peer_name,prefill_kv_indices,dst_kv_ptrs,dst_kv_indices,dst_gpu_id,notif)
  NixlKVManager.update_status(self,bootstrap_room,status)
  NixlKVManager.update_transfer_status(self)
  NixlKVReceiver.__init__(self,mgr,bootstrap_addr,bootstrap_room,data_parallel_rank)
  NixlKVReceiver._register_kv_args(self)
  NixlKVReceiver.failure_exception(self)
  NixlKVReceiver.init(self,kv_indices,aux_index)
  NixlKVReceiver.poll(self)
  NixlKVSender.__init__(self,mgr,bootstrap_addr,bootstrap_room,dest_tp_ranks,pp_rank)
  NixlKVSender.failure_exception(self)
  NixlKVSender.init(self,num_kv_indices,aux_index)
  NixlKVSender.poll(self)
  NixlKVSender.send(self,kv_indices)
  TransferInfo.from_zmq(cls,msg)
  TransferInfo.is_dummy(self)
  TransferStatus.is_done(self)
disaggregation/prefill.py:
  PrefillBootstrapQueue.__init__(self,token_to_kv_pool,draft_token_to_kv_pool,req_to_metadata_buffer_idx_allocator,metadata_buffers,tp_rank,tp_size,gpu_id,bootstrap_port,gloo_group,max_total_num_tokens,decode_tp_size,decode_dp_size,scheduler,pp_rank,pp_size,transfer_backend)
  PrefillBootstrapQueue._check_if_req_exceed_kv_capacity(self,req)
  PrefillBootstrapQueue._init_kv_manager(self)
  PrefillBootstrapQueue._process_req(self,req)
  PrefillBootstrapQueue.add(self,req,num_kv_heads)
  PrefillBootstrapQueue.extend(self,reqs,num_kv_heads)
  PrefillBootstrapQueue.pop_bootstrapped(self,return_failed_reqs,rids_to_check)
  SchedulerDisaggregationPrefillMixin.event_loop_normal_disagg_prefill(self)
  SchedulerDisaggregationPrefillMixin.event_loop_overlap_disagg_prefill(self)
  SchedulerDisaggregationPrefillMixin.event_loop_pp_disagg_prefill(self)
  SchedulerDisaggregationPrefillMixin.get_transferred_rids(self)
  SchedulerDisaggregationPrefillMixin.process_batch_result_disagg_prefill(self,batch,result,launch_done)
  SchedulerDisaggregationPrefillMixin.process_disagg_prefill_inflight_queue(self,rids_to_check)
  SchedulerDisaggregationPrefillMixin.process_prefill_chunk(self)
  SchedulerDisaggregationPrefillMixin.recv_pyobj_from_prev_stage(self)
  SchedulerDisaggregationPrefillMixin.send_kv_chunk(self,req,last_chunk,end_idx)
  SchedulerDisaggregationPrefillMixin.send_pyobj_to_next_stage(self,data)
disaggregation/utils.py:
  MetadataBuffers.__init__(self,size,hidden_size,dtype,max_top_logprobs_num,custom_mem_pool)
  MetadataBuffers.get_buf(self,idx)
  MetadataBuffers.get_buf_infos(self)
  MetadataBuffers.set_buf(self,req)
  PDRegistryRequest.__post_init__(self)
  ReqToMetadataIdxAllocator.__init__(self,size)
  ReqToMetadataIdxAllocator.alloc(self)
  ReqToMetadataIdxAllocator.available_size(self)
  ReqToMetadataIdxAllocator.free(self,free_index)
  get_kv_class(transfer_backend,class_type)
  is_mla_backend(target_kv_pool)
  kv_to_page_indices(kv_indices,page_size)
  kv_to_page_num(num_kv_indices,page_size)
  poll_and_all_reduce(pollers,gloo_group)
  prepare_abort(req,error_message,status_code)
  register_disaggregation_server(mode,server_port,bootstrap_port,pdlb_url)
distributed/communication_op.py:
  broadcast_tensor_dict(tensor_dict,src)
  tensor_model_parallel_all_gather(input_,dim)
  tensor_model_parallel_all_reduce(input_)
  tensor_model_parallel_gather(input_,dst,dim)
distributed/device_communicators/cuda_wrapper.py:
  CudaRTLibrary.CUDART_CHECK(self,result)
  CudaRTLibrary.__init__(self,so_file)
  CudaRTLibrary.cudaDeviceReset(self)
  CudaRTLibrary.cudaDeviceSynchronize(self)
  CudaRTLibrary.cudaFree(self,devPtr)
  CudaRTLibrary.cudaGetErrorString(self,error)
  CudaRTLibrary.cudaIpcGetMemHandle(self,devPtr)
  CudaRTLibrary.cudaIpcOpenMemHandle(self,handle)
  CudaRTLibrary.cudaMalloc(self,size)
  CudaRTLibrary.cudaMemcpy(self,dst,src,count)
  CudaRTLibrary.cudaMemset(self,devPtr,value,count)
  CudaRTLibrary.cudaSetDevice(self,device)
  find_loaded_library(lib_name)
distributed/device_communicators/custom_all_reduce.py:
  CustomAllreduce.__del__(self)
  CustomAllreduce.__init__(self,group,device,max_size)
  CustomAllreduce._gather_ipc_meta(self,shard_data)
  CustomAllreduce._get_ipc_meta(self,inp)
  CustomAllreduce.all_reduce(self,inp,out,registered)
  CustomAllreduce.all_reduce_reg(self,inp,out)
  CustomAllreduce.all_reduce_unreg(self,inp,out)
  CustomAllreduce.capture(self)
  CustomAllreduce.close(self)
  CustomAllreduce.create_shared_buffer(size_in_bytes,group)
  CustomAllreduce.custom_all_reduce(self,input)
  CustomAllreduce.free_shared_buffer(pointers,group)
  CustomAllreduce.register_buffer(self,inp)
  CustomAllreduce.register_graph_buffers(self)
  CustomAllreduce.should_custom_ar(self,inp)
  _can_p2p(rank,world_size)
distributed/device_communicators/custom_all_reduce_utils.py:
  can_actually_p2p(batch_src,batch_tgt)
  consumer(batch_tgt,producer_queue,consumer_queue,result_queue,cuda_visible_devices)
  gpu_p2p_access_check(src,tgt)
  is_full_nvlink(physical_device_ids,world_size)
  is_weak_contiguous(inp)
  producer(batch_src,producer_queue,consumer_queue,result_queue,cuda_visible_devices)
  update_environment_variables(envs)
  with_nvml_context(fn)
  wrapper(*args,**kwargs)
distributed/device_communicators/hpu_communicator.py:
  HpuCommunicator.__init__(self,group)
  HpuCommunicator.all_gather(self,x,dim)
  HpuCommunicator.all_reduce(self,x)
distributed/device_communicators/npu_communicator.py:
  NpuCommunicator.__init__(self,group)
  NpuCommunicator.all_gather(self,x,dim)
  NpuCommunicator.all_reduce(self,x)
distributed/device_communicators/pymscclpp.py:
  PyMscclppCommunicator.__init__(self,group,device,max_bytes)
  PyMscclppCommunicator.all_reduce(self,tensor,op)
  PyMscclppCommunicator.change_state(self,enable)
  PyMscclppCommunicator.pre_tune_config(self,dtype)
  PyMscclppCommunicator.should_mscclpp_allreduce(self,inp,op)
  mscclpp_bench_time(func,test_niter,warmup_niter)
  mscclpp_convert_to_bytes(size_str)
  mscclpp_is_weak_contiguous(inp)
distributed/device_communicators/pynccl.py:
  PyNcclCommunicator.__init__(self,group,device,library_path)
  PyNcclCommunicator.all_gather(self,output_tensor,input_tensor,stream,sizes)
  PyNcclCommunicator.all_reduce(self,tensor,op,stream)
  PyNcclCommunicator.broadcast(self,tensor,src,stream)
  PyNcclCommunicator.change_state(self,enable,stream)
  PyNcclCommunicator.deregister_comm_window(self,window)
  PyNcclCommunicator.group_end(self)
  PyNcclCommunicator.group_start(self)
  PyNcclCommunicator.recv(self,tensor,src,stream)
  PyNcclCommunicator.reduce_scatter(self,output_tensor,input_tensor,op,stream,sizes)
  PyNcclCommunicator.register_comm_window_raw(self,ptr,size)
  PyNcclCommunicator.send(self,tensor,dst,stream)
distributed/device_communicators/pynccl_allocator.py:
  get_nccl_mem_pool()
  is_symmetric_memory_enabled()
  set_graph_pool_id(graph_pool_id)
  use_symmetric_memory.__enter__(self)
  use_symmetric_memory.__exit__(self,exc_type,exc_val,exc_tb)
  use_symmetric_memory.__init__(self,group_coordinator)
  use_symmetric_memory.tag(self,tensor)
distributed/device_communicators/pynccl_wrapper.py:
  NCCLLibrary.NCCL_CHECK(self,result)
  NCCLLibrary.__init__(self,so_file)
  NCCLLibrary.ncclAllGather(self,sendbuff,recvbuff,count,datatype,comm,stream)
  NCCLLibrary.ncclAllReduce(self,sendbuff,recvbuff,count,datatype,op,comm,stream)
  NCCLLibrary.ncclBroadcast(self,sendbuff,recvbuff,count,datatype,root,comm,stream)
  NCCLLibrary.ncclCommDestroy(self,comm)
  NCCLLibrary.ncclCommInitRank(self,world_size,unique_id,rank)
  NCCLLibrary.ncclCommWindowDeregister(self,comm,window)
  NCCLLibrary.ncclCommWindowRegister(self,comm,buff,size,win_flags)
  NCCLLibrary.ncclGetErrorString(self,result)
  NCCLLibrary.ncclGetRawVersion(self)
  NCCLLibrary.ncclGetUniqueId(self)
  NCCLLibrary.ncclGetVersion(self)
  NCCLLibrary.ncclGroupEnd(self)
  NCCLLibrary.ncclGroupStart(self)
  NCCLLibrary.ncclRecv(self,recvbuff,count,datatype,src,comm,stream)
  NCCLLibrary.ncclReduce(self,sendbuff,recvbuff,count,datatype,op,root,comm,stream)
  NCCLLibrary.ncclReduceScatter(self,sendbuff,recvbuff,count,datatype,op,comm,stream)
  NCCLLibrary.ncclSend(self,sendbuff,count,datatype,dest,comm,stream)
  find_nccl_library()
  ncclDataTypeEnum.from_torch(cls,dtype)
  ncclRedOpTypeEnum.from_torch(cls,op)
distributed/device_communicators/quick_all_reduce.py:
  QuickAllReduce.__del__(self)
  QuickAllReduce.__init__(self,group,device)
  QuickAllReduce.close(self)
  QuickAllReduce.create_shared_buffer(self)
  QuickAllReduce.init_quick_all_reduce(self)
  QuickAllReduce.quick_all_reduce(self,inp,out)
  QuickAllReduce.should_quick_allreduce(self,inp)
  qr_rocm_arch_available()
distributed/device_communicators/shm_broadcast.py:
  MessageQueue.__init__(self,n_reader,n_local_reader,local_reader_ranks,max_chunk_bytes,max_chunks,connect_ip)
  MessageQueue.acquire_read(self)
  MessageQueue.acquire_write(self)
  MessageQueue.broadcast_object(self,obj)
  MessageQueue.create_from_handle(handle,rank)
  MessageQueue.create_from_process_group(pg,max_chunk_bytes,max_chunks,writer_rank)
  MessageQueue.dequeue(self)
  MessageQueue.enqueue(self,obj)
  MessageQueue.export_handle(self)
  MessageQueue.wait_until_ready(self)
  ShmRingBuffer.__del__(self)
  ShmRingBuffer.__init__(self,n_reader,max_chunk_bytes,max_chunks,name)
  ShmRingBuffer.__reduce__(self)
  ShmRingBuffer.get_data(self,current_idx)
  ShmRingBuffer.get_metadata(self,current_idx)
distributed/device_communicators/xpu_communicator.py:
  XpuCommunicator.__init__(self,group)
  XpuCommunicator.all_reduce(self,x)
  XpuCommunicator.gather(self,input_,rank_in_group,dst,dim)
distributed/naive_distributed.py:
  NaiveDistributed.__init__(self,rank,world_size,rendezvous)
  NaiveDistributed._get_path(interesting_rank)
  NaiveDistributed._read_one(interesting_rank)
  NaiveDistributed.all_gather_object(self,obj)
  NaiveDistributed.barrier(self)
  NaiveDistributed.get_rank(self)
  NaiveDistributed.get_world_size(self)
  NaiveDistributed.scatter(self,tensor,scatter_list,src)
  get_naive_distributed()
  set_naive_distributed(instance)
distributed/parallel_state.py:
  GroupCoordinator.__init__(self,group_ranks,local_rank,torch_distributed_backend,use_pynccl,use_pymscclpp,use_custom_allreduce,use_hpu_communicator,use_xpu_communicator,use_npu_communicator,use_message_queue_broadcaster,group_name)
  GroupCoordinator.__repr__(self)
  GroupCoordinator._all_gather_into_tensor(self,output,input)
  GroupCoordinator._all_gather_single(input_,sizes)
  GroupCoordinator._all_reduce_in_place(self,input_)
  GroupCoordinator._all_reduce_out_place(self,input_,outplace_all_reduce_method)
  GroupCoordinator.all_gather(self,input_,dim,output_tensor_list)
  GroupCoordinator.all_gather_into_tensor(self,output,input)
  GroupCoordinator.all_gather_object(self,obj)
  GroupCoordinator.all_gatherv(self,input_,sizes)
  GroupCoordinator.all_reduce(self,input_)
  GroupCoordinator.barrier(self)
  GroupCoordinator.broadcast(self,input_,src)
  GroupCoordinator.broadcast_object(self,obj,src)
  GroupCoordinator.broadcast_object_list(self,obj_list,src,group)
  GroupCoordinator.broadcast_tensor_dict(self,tensor_dict,src,group,metadata_group)
  GroupCoordinator.destroy(self)
  GroupCoordinator.first_rank(self)
  GroupCoordinator.gather(self,input_,dst,dim)
  GroupCoordinator.graph_capture(self,graph_capture_context)
  GroupCoordinator.is_first_rank(self)
  GroupCoordinator.is_last_rank(self)
  GroupCoordinator.last_rank(self)
  GroupCoordinator.next_rank(self)
  GroupCoordinator.prev_rank(self)
  GroupCoordinator.recv(self,size,dtype,src)
  GroupCoordinator.recv_object(self,src)
  GroupCoordinator.recv_tensor_dict(self,src,all_gather_group)
  GroupCoordinator.reduce_scatter(self,output,input_list)
  GroupCoordinator.reduce_scatter_tensor(self,output,input)
  GroupCoordinator.reduce_scatterv(self,input_,output,sizes)
  GroupCoordinator.send(self,tensor,dst)
  GroupCoordinator.send_object(self,obj,dst)
  GroupCoordinator.send_tensor_dict(self,tensor_dict,dst,all_gather_group)
  _get_unique_name(name)
  _register_group(group)
  _split_tensor_dict(tensor_dict)
  cleanup_dist_env_and_memory(shutdown_ray)
  destroy_distributed_environment()
  destroy_model_parallel()
  ensure_model_parallel_initialized(tensor_model_parallel_size,expert_model_parallel_size,pipeline_model_parallel_size,backend)
  get_moe_ep_group()
  get_moe_expert_parallel_rank()
  get_moe_expert_parallel_world_size()
  get_moe_tensor_parallel_rank()
  get_moe_tensor_parallel_world_size()
  get_moe_tp_group()
  get_pp_group()
  get_tensor_model_parallel_rank()
  get_tensor_model_parallel_world_size()
  get_tp_group()
  get_world_group()
  graph_capture()
  in_the_same_node_as(pg,source_rank)
  init_distributed_environment(world_size,rank,distributed_init_method,local_rank,backend,timeout)
  init_model_parallel_group(group_ranks,local_rank,backend,use_custom_allreduce,use_message_queue_broadcaster,group_name,use_mscclpp_allreduce)
  init_world_group(ranks,local_rank,backend)
  initialize_model_parallel(tensor_model_parallel_size,expert_model_parallel_size,pipeline_model_parallel_size,backend,duplicate_tp_group)
  inplace_all_reduce(tensor,group_name)
  inplace_all_reduce_fake(tensor,group_name)
  model_parallel_is_initialized()
  monkey_patch_vllm_parallel_state(reverse)
  outplace_all_reduce(tensor,group_name,outplace_all_reduce_method)
  outplace_all_reduce_fake(tensor,group_name,outplace_all_reduce_method)
  patch_tensor_parallel_group(tp_group)
  reg_all_gather_into_tensor(output,input,group_name)
  reg_all_gather_into_tensor_fake(output,input,group_name)
  set_custom_all_reduce(enable)
  set_mscclpp_all_reduce(enable)
  set_pdmux_status(enable_prefill_multiplexing)
distributed/utils.py:
  StatelessProcessGroup.__post_init__(self)
  StatelessProcessGroup.all_gather_obj(self,obj)
  StatelessProcessGroup.barrier(self)
  StatelessProcessGroup.broadcast_obj(self,obj,src)
  StatelessProcessGroup.create(host,port,rank,world_size,data_expiration_seconds)
  StatelessProcessGroup.expire_data(self)
  StatelessProcessGroup.recv_obj(self,src)
  StatelessProcessGroup.send_obj(self,obj,dst)
  divide(numerator,denominator)
  ensure_divisibility(numerator,denominator)
  get_pp_indices(num_hidden_layers,pp_rank,pp_size)
  split_tensor_along_last_dim(tensor,num_partitions,contiguous_split_chunks)
entrypoints/EngineBase.py:
  EngineBase.flush_cache(self)
  EngineBase.generate(self,prompt,sampling_params,input_ids,image_data,return_logprob,logprob_start_len,top_logprobs_num,token_ids_logprob,lora_path,custom_logit_processor,return_hidden_states,stream,bootstrap_host,bootstrap_port,bootstrap_room,data_parallel_rank)
  EngineBase.load_lora_adapter(self,lora_name,lora_path)
  EngineBase.release_memory_occupation(self)
  EngineBase.resume_memory_occupation(self)
  EngineBase.shutdown(self)
  EngineBase.unload_lora_adapter(self,lora_name)
  EngineBase.update_weights_from_tensor(self,named_tensors,load_format,flush_cache)
entrypoints/context.py:
  ConversationContext.append_output(self,output)
  ConversationContext.need_builtin_tool_call(self)
  ConversationContext.render_for_completion(self)
  HarmonyContext.__init__(self,messages,tool_sessions)
  HarmonyContext.append_output(self,output)
  HarmonyContext.messages(self)
  HarmonyContext.need_builtin_tool_call(self)
  HarmonyContext.render_for_completion(self)
  SimpleContext.__init__(self)
  SimpleContext.append_output(self,output)
  SimpleContext.need_builtin_tool_call(self)
  SimpleContext.render_for_completion(self)
  StreamingHarmonyContext.__init__(self,*args,**kwargs)
  StreamingHarmonyContext.append_output(self,output)
  StreamingHarmonyContext.is_assistant_action_turn(self)
  StreamingHarmonyContext.is_expecting_start(self)
  StreamingHarmonyContext.messages(self)
  StreamingHarmonyContext.render_for_completion(self)
  async ConversationContext.call_tool(self)
  async HarmonyContext.call_python_tool(self,tool_session,last_msg)
  async HarmonyContext.call_search_tool(self,tool_session,last_msg)
  async HarmonyContext.call_tool(self)
  async SimpleContext.call_tool(self)
entrypoints/engine.py:
  Engine.__enter__(self)
  Engine.__exit__(self,exc_type,exc_value,traceback)
  Engine.__init__(self,**kwargs)
  Engine.collective_rpc(self,method,**kwargs)
  Engine.dump_expert_distribution_record(self)
  Engine.encode(self,prompt,image_data,audio_data,video_data)
  Engine.flush_cache(self)
  Engine.freeze_gc(self)
  Engine.generate(self,prompt,sampling_params,input_ids,image_data,audio_data,video_data,return_logprob,logprob_start_len,top_logprobs_num,token_ids_logprob,lora_path,custom_logit_processor,return_hidden_states,stream,bootstrap_host,bootstrap_port,bootstrap_room,data_parallel_rank)
  Engine.generator_wrapper()
  Engine.get_server_info(self)
  Engine.get_weights_by_name(self,name,truncate_size)
  Engine.init_weights_update_group(self,master_address,master_port,rank_offset,world_size,group_name,backend)
  Engine.load_lora_adapter(self,lora_name,lora_path,pinned)
  Engine.release_memory_occupation(self,tags)
  Engine.rerank(self,prompt)
  Engine.resume_memory_occupation(self,tags)
  Engine.save_remote_model(self,**kwargs)
  Engine.save_sharded_model(self,**kwargs)
  Engine.score(self,query,items,label_token_ids,apply_softmax,item_first)
  Engine.shutdown(self)
  Engine.start_expert_distribution_record(self)
  Engine.start_profile(self)
  Engine.stop_expert_distribution_record(self)
  Engine.stop_profile(self)
  Engine.unload_lora_adapter(self,lora_name)
  Engine.update_weights_from_disk(self,model_path,load_format)
  Engine.update_weights_from_distributed(self,names,dtypes,shapes,group_name,flush_cache)
  Engine.update_weights_from_tensor(self,named_tensors,load_format,flush_cache)
  _launch_subprocesses(server_args,port_args)
  _set_envs_and_config(server_args)
  async Engine.async_encode(self,prompt,image_data,audio_data,video_data)
  async Engine.async_generate(self,prompt,sampling_params,input_ids,image_data,audio_data,video_data,return_logprob,logprob_start_len,top_logprobs_num,token_ids_logprob,lora_path,custom_logit_processor,return_hidden_states,stream,bootstrap_host,bootstrap_port,bootstrap_room,data_parallel_rank)
  async Engine.async_score(self,query,items,label_token_ids,apply_softmax,item_first)
  launch_phase_sigquit_handler(signum,frame)
entrypoints/harmony_utils.py:
  get_developer_message(instructions,tools)
  get_encoding()
  get_stop_tokens_for_assistant_actions()
  get_streamable_parser_for_assistant()
  get_system_message(model_identity,reasoning_effort,start_date,browser_description,python_description)
  get_user_message(content)
  parse_chat_input(chat_msg)
  parse_output_into_messages(token_ids)
  parse_output_message(message)
  parse_remaining_state(parser)
  parse_response_input(response_msg,prev_responses)
  parse_response_output(output)
  render_for_completion(messages)
entrypoints/http_server.py:
  _create_error_response(e)
  _execute_server_warmup(server_args,pipe_finish_writer)
  _update_weight_version_if_provided(weight_version)
  _wait_and_warmup(server_args,pipe_finish_writer,launch_callback)
  async abort_request(obj,request)
  async available_models()
  async classify_request(obj,request)
  async clear_hicache_storage_backend()
  async close_session(obj,request)
  async configure_logging(obj,request)
  async continue_generation(request)
  async dump_expert_distribution_record_async()
  async encode_request(obj,request)
  async flush_cache()
  async freeze_gc_async()
  async gen()
  async generate_from_file_request(file,request)
  async generate_request(obj,request)
  async get_load()
  async get_model_info()
  async get_server_info()
  async get_weight_version()
  async get_weights_by_name(obj,request)
  async health_generate(request)
  async init_weights_update_group(obj,request)
  async lifespan(fast_api_app)
  async load_lora_adapter(obj,request)
  async open_session(obj,request)
  async openai_v1_chat_completions(request,raw_request)
  async openai_v1_completions(request,raw_request)
  async openai_v1_embeddings(request,raw_request)
  async parse_function_call_request(obj,request)
  async pause_generation(request)
  async release_memory_occupation(obj,request)
  async resume_memory_occupation(obj,request)
  async retrieve_model(model)
  async sagemaker_chat_completions(request,raw_request)
  async sagemaker_health()
  async separate_reasoning_request(obj,request)
  async set_internal_state(obj,request)
  async slow_down(obj,request)
  async start_expert_distribution_record_async()
  async start_profile_async(obj)
  async stop_expert_distribution_record_async()
  async stop_profile_async()
  async stream_results()
  async unload_lora_adapter(obj,request)
  async update_weight_version(obj,request)
  async update_weights_from_disk(obj,request)
  async update_weights_from_distributed(obj,request)
  async update_weights_from_tensor(obj,request)
  async v1_cancel_responses(response_id,raw_request)
  async v1_rerank_request(request,raw_request)
  async v1_responses_request(request,raw_request)
  async v1_retrieve_responses(response_id,raw_request)
  async v1_score_request(request,raw_request)
  async validate_json_request(raw_request)
  async validation_exception_handler(request,exc)
  async validation_exception_handler(request,exc)
  async vertex_generate(vertex_req,raw_request)
  launch_server(server_args,pipe_finish_writer,launch_callback)
  set_global_state(global_state)
entrypoints/http_server_engine.py:
  HttpServerEngineAdapter.__init__(self,**kwargs)
  HttpServerEngineAdapter._make_request(self,endpoint,payload)
  HttpServerEngineAdapter.flush_cache(self)
  HttpServerEngineAdapter.generate(self,prompt,sampling_params,input_ids,image_data,return_logprob,logprob_start_len,top_logprobs_num,token_ids_logprob,lora_path,custom_logit_processor)
  HttpServerEngineAdapter.release_memory_occupation(self)
  HttpServerEngineAdapter.resume_memory_occupation(self)
  HttpServerEngineAdapter.shutdown(self)
  HttpServerEngineAdapter.update_weights_from_tensor(self,named_tensors,load_format,flush_cache)
  launch_server_process(server_args)
entrypoints/openai/protocol.py:
  ChatCompletionMessageGenericParam._normalize_role(cls,v)
  ChatCompletionRequest.normalize_reasoning_inputs(cls,values)
  ChatCompletionRequest.set_json_schema(cls,values)
  ChatCompletionRequest.set_tool_choice_default(cls,values)
  ChatCompletionResponseChoice._serialize(self,handler)
  CompletionRequest.validate_max_tokens_positive(cls,v)
  CompletionResponseChoice._serialize(self,handler)
  CompletionResponseStreamChoice._serialize(self,handler)
  DeltaMessage._serialize(self,handler)
  ResponsesRequest.to_sampling_params(self,default_max_tokens,default_params)
  ResponsesResponse.from_request(cls,request,sampling_params,model_name,created_time,output,status,usage)
entrypoints/openai/serving_base.py:
  OpenAIServingBase.__init__(self,tokenizer_manager)
  OpenAIServingBase._convert_to_internal_request(self,request)
  OpenAIServingBase._generate_request_id_base(self,request)
  OpenAIServingBase._request_id_prefix(self)
  OpenAIServingBase._validate_request(self,_)
  OpenAIServingBase.create_error_response(self,message,err_type,status_code,param)
  OpenAIServingBase.create_streaming_error_response(self,message,err_type,status_code)
  async OpenAIServingBase._handle_non_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingBase._handle_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingBase.handle_request(self,request,raw_request)
entrypoints/openai/serving_chat.py:
  OpenAIServingChat.__init__(self,tokenizer_manager,template_manager)
  OpenAIServingChat._apply_conversation_template(self,request,is_multimodal)
  OpenAIServingChat._apply_jinja_template(self,request,tools,is_multimodal)
  OpenAIServingChat._build_chat_response(self,request,ret,created)
  OpenAIServingChat._build_sampling_params(self,request,stop,tool_call_constraint)
  OpenAIServingChat._check_for_unstreamed_tool_args(self,parser,content,request,index)
  OpenAIServingChat._convert_to_internal_request(self,request)
  OpenAIServingChat._get_enable_thinking_from_request(self,request)
  OpenAIServingChat._process_logprobs_tokens(self,logprobs,use_token_index)
  OpenAIServingChat._process_messages(self,request,is_multimodal)
  OpenAIServingChat._process_reasoning_stream(self,index,delta,reasoning_parser_dict,content,request)
  OpenAIServingChat._process_response_logprobs(self,ret_item)
  OpenAIServingChat._process_streaming_logprobs(self,content,n_prev_token)
  OpenAIServingChat._process_tool_calls(self,text,tools,tool_call_parser,finish_reason)
  OpenAIServingChat._request_id_prefix(self)
  OpenAIServingChat._validate_request(self,request)
  async OpenAIServingChat._generate_chat_stream(self,adapted_request,request,raw_request)
  async OpenAIServingChat._handle_non_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingChat._handle_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingChat._process_tool_call_stream(self,index,delta,parser_dict,content,request,has_tool_calls)
entrypoints/openai/serving_completions.py:
  OpenAIServingCompletion.__init__(self,tokenizer_manager,template_manager)
  OpenAIServingCompletion._build_completion_response(self,request,ret,created)
  OpenAIServingCompletion._build_sampling_params(self,request)
  OpenAIServingCompletion._convert_to_internal_request(self,request)
  OpenAIServingCompletion._get_echo_text(self,request,index)
  OpenAIServingCompletion._prepare_echo_prompts(self,request)
  OpenAIServingCompletion._request_id_prefix(self)
  OpenAIServingCompletion._validate_request(self,request)
  async OpenAIServingCompletion._generate_completion_stream(self,adapted_request,request,raw_request)
  async OpenAIServingCompletion._handle_non_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingCompletion._handle_streaming_request(self,adapted_request,request,raw_request)
entrypoints/openai/serving_embedding.py:
  OpenAIServingEmbedding.__init__(self,tokenizer_manager,template_manager)
  OpenAIServingEmbedding._build_embedding_response(self,ret)
  OpenAIServingEmbedding._convert_to_internal_request(self,request)
  OpenAIServingEmbedding._request_id_prefix(self)
  OpenAIServingEmbedding._validate_request(self,request)
  async OpenAIServingEmbedding._handle_non_streaming_request(self,adapted_request,request,raw_request)
entrypoints/openai/serving_rerank.py:
  OpenAIServingRerank._build_rerank_response(self,ret,request)
  OpenAIServingRerank._convert_to_internal_request(self,request)
  OpenAIServingRerank._request_id_prefix(self)
  OpenAIServingRerank._validate_request(self,request)
  async OpenAIServingRerank._handle_non_streaming_request(self,adapted_request,request,raw_request)
entrypoints/openai/serving_responses.py:
  OpenAIServingResponses.__init__(self,tokenizer_manager,template_manager,enable_prompt_tokens_details,enable_force_include_usage,tool_server)
  OpenAIServingResponses._construct_input_messages(self,request,prev_response)
  OpenAIServingResponses._construct_input_messages_with_harmony(self,request,prev_response)
  OpenAIServingResponses._make_invalid_id_error(self,response_id)
  OpenAIServingResponses._make_not_found_error(self,response_id)
  OpenAIServingResponses._make_request_with_harmony(self,request,prev_response)
  OpenAIServingResponses._make_response_output_items(self,request,final_output,tokenizer)
  OpenAIServingResponses._make_response_output_items_with_harmony(self,context)
  OpenAIServingResponses._request_id_prefix(self)
  OpenAIServingResponses._send_event(event)
  async OpenAIServingResponses._generate_with_builtin_tools(self,request_id,request_prompt,adapted_request,sampling_params,context,raw_request,priority,**kwargs)
  async OpenAIServingResponses._make_request(self,request,prev_response,tokenizer)
  async OpenAIServingResponses._run_background_request(self,request,sampling_params,result_generator,context,model_name,tokenizer,request_metadata,created_time,*args,**kwargs)
  async OpenAIServingResponses.cancel_responses(self,response_id)
  async OpenAIServingResponses.create_responses(self,request,raw_request)
  async OpenAIServingResponses.empty_async_generator()
  async OpenAIServingResponses.responses_full_generator(self,request,sampling_params,result_generator,context,model_name,tokenizer,request_metadata,created_time)
  async OpenAIServingResponses.responses_stream_generator(self,request,sampling_params,result_generator,context,model_name,tokenizer,request_metadata,created_time)
  async OpenAIServingResponses.retrieve_responses(self,response_id)
entrypoints/openai/serving_score.py:
  OpenAIServingScore._convert_to_internal_request(self,request)
  OpenAIServingScore._request_id_prefix(self)
  async OpenAIServingScore._handle_non_streaming_request(self,adapted_request,request,raw_request)
entrypoints/openai/tool_server.py:
  DemoToolServer.__init__(self)
  DemoToolServer.get_tool_description(self,tool_name)
  DemoToolServer.has_tool(self,tool_name)
  MCPToolServer.__init__(self)
  MCPToolServer.get_tool_description(self,tool_name)
  MCPToolServer.has_tool(self,tool_name)
  ToolServer.get_tool_description(self,tool_name)
  ToolServer.get_tool_session(self,tool_name)
  ToolServer.has_tool(self,tool_name)
  async DemoToolServer.get_tool_session(self,tool_name)
  async MCPToolServer.add_tool_server(self,server_url)
  async MCPToolServer.get_tool_session(self,tool_name)
  async list_server_and_tools(server_url)
  post_process_tools_description(list_tools_result)
  trim_schema(schema)
entrypoints/openai/usage_processor.py:
  UsageProcessor._details_if_cached(count)
  UsageProcessor.calculate_response_usage(responses,n_choices,enable_cache_report)
  UsageProcessor.calculate_streaming_usage(prompt_tokens,completion_tokens,cached_tokens,n_choices,enable_cache_report)
  UsageProcessor.calculate_token_usage(prompt_tokens,completion_tokens,cached_tokens)
entrypoints/openai/utils.py:
  append_token_logprobs(token_logprobs)
  append_top_logprobs(top_logprobs)
  process_hidden_states_from_ret(ret_item,request)
  to_openai_style_logprobs(input_token_logprobs,output_token_logprobs,input_top_logprobs,output_top_logprobs)
entrypoints/tool.py:
  HarmonyBrowserTool.__init__(self)
  HarmonyBrowserTool.tool_config(self)
  HarmonyPythonTool.__init__(self)
  HarmonyPythonTool.tool_config(self)
  async HarmonyBrowserTool.get_result(self,context)
  async HarmonyPythonTool.get_result(self,context)
  async Tool.get_result(self,context)
eplb/eplb_algorithms/__init__.py:
  compute_algorithm(raw_algorithm,num_groups,num_nodes)
  rebalance_experts(tokens_per_expert,num_physical_experts,num_local_physical_experts,num_groups,num_nodes,algorithm)
eplb/eplb_algorithms/deepseek.py:
  balanced_packing(weight,num_packs)
  inverse(perm)
  rebalance_experts(weight,num_replicas,num_groups,num_nodes,num_gpus,enable_hierarchical)
  rebalance_experts_hierarchical(weight,num_physical_experts,num_groups,num_nodes,num_gpus)
  replicate_experts(weight,num_phy)
eplb/eplb_algorithms/deepseek_vec.py:
  decode_rebalance_experts(tokens_per_expert,num_physical_experts,num_local_physical_experts)
  key_func(rank)
  make_redundant_experts_chunkwise(tokens_per_expert,num_physical_experts,num_local_physical_experts,num_physical_experts_per_chunk)
  pack_groups(tokens_per_group,num_nodes)
  prefill_rebalance_experts(tokens_per_expert,num_physical_experts,num_local_physical_experts,num_groups,num_nodes)
  rebalance_experts(tokens_per_expert,num_physical_experts,num_local_physical_experts,num_groups,num_nodes,enable_hierarchical)
eplb/eplb_manager.py:
  EPLBManager.__init__(self,model_runner)
  EPLBManager._check_rebalance_needed(self,average_utilization_rate_over_window)
  EPLBManager._compute_update_layer_ids_chunks(self)
  EPLBManager._entrypoint(self)
  EPLBManager.on_forward_pass_end(self)
  EPLBManager.rebalance(self)
  _chunk_list(items,chunk_size)
eplb/eplb_simulator/reader.py:
  read_mode_per_pass(dir_data)
eplb/expert_distribution.py:
  ExpertDistributionRecorder._on_not_implemented(self)
  ExpertDistributionRecorder.disable_this_region(self)
  ExpertDistributionRecorder.dump_record(self,output_mode)
  ExpertDistributionRecorder.init_new(server_args,expert_location_metadata,rank)
  ExpertDistributionRecorder.on_deepep_dispatch_low_latency(self,local_physical_count_of_layer)
  ExpertDistributionRecorder.on_deepep_dispatch_normal(self,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  ExpertDistributionRecorder.on_select_experts(self,topk_ids)
  ExpertDistributionRecorder.recording(self)
  ExpertDistributionRecorder.start_record(self)
  ExpertDistributionRecorder.stop_record(self)
  ExpertDistributionRecorder.with_current_layer(self,layer_idx)
  ExpertDistributionRecorder.with_debug_name(self,debug_name)
  ExpertDistributionRecorder.with_forward_pass(self,forward_pass_id,forward_batch)
  _Accumulator.__init__(self,server_args,expert_location_metadata,rank)
  _Accumulator.append(self,forward_pass_id,gatherer_key,single_pass_data)
  _Accumulator.dump(self,output_mode)
  _Accumulator.get_class(server_args)
  _Accumulator.get_single_pass_gatherer_key(self,debug_name)
  _Accumulator.get_single_pass_gatherer_keys(self)
  _Accumulator.init_new(server_args,expert_location_metadata,rank)
  _Accumulator.reset(self)
  _Buffer.append(self,value)
  _Buffer.get_all(self)
  _Buffer.init_new(item_shape,buffer_size,dtype,device)
  _Buffer.reset(self)
  _CircularBuffer.__init__(self,item_shape,buffer_size,dtype,device)
  _CircularBuffer.append(self,value)
  _CircularBuffer.get_all(self)
  _CircularBuffer.reset(self)
  _DeepepLowLatencySinglePassGatherer.__init__(self,*args,**kwargs)
  _DeepepLowLatencySinglePassGatherer.on_deepep_dispatch_low_latency(self,layer_idx,local_physical_count_of_layer)
  _DeepepNormalSinglePassGatherer.__init__(self,*args,**kwargs)
  _DeepepNormalSinglePassGatherer.collect(self)
  _DeepepNormalSinglePassGatherer.on_deepep_dispatch_normal(self,layer_idx,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  _DequeCollection.__init__(self,maxlens)
  _DequeCollection.append(self,value)
  _DequeCollection.clear(self)
  _DequeCollection.mean(self)
  _DetailAccumulator.__init__(self,*args,**kwargs)
  _DetailAccumulator._process_object(obj)
  _DetailAccumulator.append(self,forward_pass_id,gatherer_key,single_pass_data)
  _DetailAccumulator.dump(self,output_mode)
  _DetailAccumulator.get_single_pass_gatherer_key(self,debug_name)
  _DetailAccumulator.get_single_pass_gatherer_keys(self)
  _DetailAccumulator.reset(self)
  _DetailSinglePassGatherer.__init__(self,server_args,expert_location_metadata,rank)
  _DetailSinglePassGatherer.collect(self)
  _DetailSinglePassGatherer.on_deepep_dispatch_normal(self,layer_idx,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  _DetailSinglePassGatherer.on_forward_pass_start(self,forward_batch)
  _DetailSinglePassGatherer.on_select_experts(self,layer_idx,topk_ids)
  _DetailSinglePassGatherer.reset(self)
  _ExpertDistributionRecorderReal.__init__(self,server_args,expert_location_metadata,rank)
  _ExpertDistributionRecorderReal._on_forward_pass_end(self,forward_pass_id)
  _ExpertDistributionRecorderReal._on_forward_pass_start(self,forward_batch)
  _ExpertDistributionRecorderReal._on_hook(self,hook_name,**kwargs)
  _ExpertDistributionRecorderReal._reset(self)
  _ExpertDistributionRecorderReal.disable_this_region(self)
  _ExpertDistributionRecorderReal.dump_record(self,output_mode)
  _ExpertDistributionRecorderReal.on_deepep_dispatch_low_latency(self,local_physical_count_of_layer)
  _ExpertDistributionRecorderReal.on_deepep_dispatch_normal(self,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  _ExpertDistributionRecorderReal.on_select_experts(self,topk_ids)
  _ExpertDistributionRecorderReal.recording(self)
  _ExpertDistributionRecorderReal.start_record(self)
  _ExpertDistributionRecorderReal.stop_record(self)
  _ExpertDistributionRecorderReal.with_current_layer(self,layer_idx)
  _ExpertDistributionRecorderReal.with_debug_name(self,debug_name)
  _ExpertDistributionRecorderReal.with_forward_pass(self,forward_pass_id,forward_batch)
  _InfiniteBuffer.__init__(self,item_shape,dtype,device)
  _InfiniteBuffer.append(self,value)
  _InfiniteBuffer.get_all(self)
  _InfiniteBuffer.reset(self)
  _LayerBasedCpuSinglePassGatherer.__init__(self,*args,**kwargs)
  _LayerBasedCpuSinglePassGatherer._collect_objects(self,pad_len)
  _LayerBasedCpuSinglePassGatherer._on_layer_data(self,layer_idx,objects)
  _LayerBasedCpuSinglePassGatherer.reset(self)
  _LayerBasedGpuSinglePassGatherer.__init__(self,*args,enable_global_physical_experts,**kwargs)
  _LayerBasedGpuSinglePassGatherer.collect(self)
  _LayerBasedGpuSinglePassGatherer.reset(self)
  _SelectExpertsSinglePassGatherer.__init__(self,*args,**kwargs)
  _SelectExpertsSinglePassGatherer.on_select_experts(self,layer_idx,topk_ids)
  _SinglePassGatherer.__init__(self,expert_location_metadata,rank)
  _SinglePassGatherer.collect(self)
  _SinglePassGatherer.init_new(server_args,expert_location_metadata,rank)
  _SinglePassGatherer.on_deepep_dispatch_low_latency(self,layer_idx,local_physical_count_of_layer)
  _SinglePassGatherer.on_deepep_dispatch_normal(self,layer_idx,local_physical_count_of_layer,num_tokens_per_rank,num_tokens_per_rdma_rank,num_tokens_per_expert)
  _SinglePassGatherer.on_forward_pass_start(self,forward_batch)
  _SinglePassGatherer.on_select_experts(self,layer_idx,topk_ids)
  _SinglePassGatherer.reset(self)
  _StatAccumulator.__init__(self,*args,**kwargs)
  _StatAccumulator._get_global_average_utilization_rate(self)
  _StatAccumulator.append(self,forward_pass_id,gatherer_key,single_pass_data)
  _StatAccumulator.dump(self,output_mode)
  _StatAccumulator.reset(self)
  _UtilizationRateAccumulatorMixin.__init__(self,*args,**kwargs)
  _UtilizationRateAccumulatorMixin._append_utilization_rate(self,forward_pass_id,single_pass_global_physical_count)
  _UtilizationRateAccumulatorMixin.append(self,forward_pass_id,gatherer_key,single_pass_data)
  _UtilizationRateAccumulatorMixin.reset(self)
  _convert_global_physical_count_to_logical_count(global_physical_count,num_layers,num_logical_experts,physical_to_logical_map)
  _convert_local_to_global_physical_count(local_physical_count,rank,num_local_physical_experts,num_physical_experts)
  _dump_to_file(name,data)
  _list_sum(a,b)
  compute_gpu_physical_count(physical_count_of_whatever,num_gpu)
  compute_utilization_rate(gpu_physical_count_of_batch)
  get_global_expert_distribution_recorder()
  set_global_expert_distribution_recorder(value)
eplb/expert_location.py:
  ExpertLocationMetadata.__post_init__(self)
  ExpertLocationMetadata._init_common(server_args,model_config)
  ExpertLocationMetadata._init_raw(server_args,ep_size,physical_to_logical_map,logical_to_all_physical_map)
  ExpertLocationMetadata.ep_size(self)
  ExpertLocationMetadata.init_by_eplb(server_args,model_config,logical_count)
  ExpertLocationMetadata.init_by_mapping(server_args,model_config,physical_to_logical_map)
  ExpertLocationMetadata.init_trivial(server_args,model_config)
  ExpertLocationMetadata.logical_to_all_physical(self,layer_id,logical_expert_id)
  ExpertLocationMetadata.num_layers(self)
  ExpertLocationMetadata.num_local_physical_experts(self)
  ExpertLocationMetadata.num_logical_experts(self)
  ExpertLocationMetadata.num_physical_experts(self)
  ExpertLocationMetadata.update(self,other,update_layer_ids)
  ModelConfigForExpertLocation.from_model_config(model_config)
  _compute_gpu_id_of_physical_expert(physical_expert_id,num_local_physical_experts)
  _compute_logical_to_all_physical_map(physical_to_logical_map,num_logical_experts)
  _fair_choices(arr,k,r)
  _logical_to_all_physical_raw(logical_to_all_physical_map,layer_id,logical_expert_id)
  _pad_nested_array(arr,pad_value)
  compute_initial_expert_location_metadata(server_args,model_config)
  compute_logical_to_rank_dispatch_physical_map(logical_to_all_physical_map,num_gpus,num_physical_experts,ep_rank,seed)
  get_global_expert_location_metadata()
  set_global_expert_location_metadata(value)
eplb/expert_location_dispatch.py:
  ExpertLocationDispatchInfo.init_new(cls,layer_id)
  _topk_ids_logical_to_physical_dynamic(topk_ids,info)
  _topk_ids_logical_to_physical_static(topk_ids,info)
  topk_ids_logical_to_physical(topk_ids,info)
  transform_select_experts_inputs(router_logits,correction_bias,info)
eplb/expert_location_updater.py:
  ExpertLocationUpdater.__init__(self)
  ExpertLocationUpdater.update(self,routed_experts_weights_of_layer,new_expert_location_metadata,update_layer_ids,nnodes,rank)
  _ChunkUtils.__init__(self,chunk_values,element_values)
  _ChunkUtils._chunk_index_from_element_index(num_elements,num_chunks,element_index)
  _ChunkUtils._element_slice_from_chunk_index(num_elements,num_chunks,chunk_index)
  _ChunkUtils.chunk_value_from_element_value(self,element_value)
  _ChunkUtils.element_values_from_chunk_value(self,chunk_value)
  _compute_comm_info(logical_expert_id)
  _create_isend_ops(p2p_op_infos)
  _create_isend_ops_of_logical_expert_id(logical_expert_id,src_expert_location,p2p_op_infos)
  _create_p2p_recv_and_buffer2weight_copy(buffer2weight_copy_infos,p2p_op_infos,logical_expert_id,src_rank,dst_expert_location)
  _deduplicate_ordered(arr)
  _entrypoint()
  _execute_buffer2weight_copies(buffer2weight_copy_infos)
  _execute_p2p_ops(p2p_op_infos)
  _get_canary_value(meta,layer_id)
  _get_direction_from_op(op)
  _get_local_expert_location(expert_location)
  _get_tensor(tensors,tensor_index,expert_location)
  _group_by(items,keyfunc)
  _handle_recv(buffer2weight_copy_infos,p2p_op_infos)
  _handle_recv_of_dst_expert_location(dst_expert_location,buffer2weight_copy_infos,p2p_op_infos)
  _log_p2p_op_metrics(p2p_op_infos,num_gpu_per_node,world_size,self_node_id)
  _update_expert_weights(**kwargs)
  _update_expert_weights_raw(routed_experts_weights_of_layer,old_expert_location_metadata,new_expert_location_metadata,update_layer_ids,nnodes,rank)
  _update_expert_weights_with_canary(routed_experts_weights_of_layer,old_expert_location_metadata,new_expert_location_metadata,update_layer_ids,nnodes,rank)
  create_temp_buffers(sample_tensors)
  update_expert_weights_single_layer(routed_experts_weights,temp_buffers,old_physical_to_logical_map,new_physical_to_logical_map,num_local_physical_experts,num_gpu_per_node,rank,world_size,debug,log_metrics)
function_call/base_format_detector.py:
  BaseFormatDetector.__init__(self)
  BaseFormatDetector._ends_with_partial_token(self,buffer,bot_token)
  BaseFormatDetector._get_tool_indices(self,tools)
  BaseFormatDetector.build_ebnf(self,tools)
  BaseFormatDetector.detect_and_parse(self,text,tools)
  BaseFormatDetector.has_tool_call(self,text)
  BaseFormatDetector.parse_base_json(self,action,tools)
  BaseFormatDetector.parse_streaming_increment(self,new_text,tools)
  BaseFormatDetector.structure_info(self)
  BaseFormatDetector.supports_structural_tag(self)
function_call/deepseekv31_detector.py:
  DeepSeekV31Detector.__init__(self)
  DeepSeekV31Detector.build_ebnf(self,tools)
  DeepSeekV31Detector.detect_and_parse(self,text,tools)
  DeepSeekV31Detector.has_tool_call(self,text)
  DeepSeekV31Detector.parse_streaming_increment(self,new_text,tools)
  DeepSeekV31Detector.structure_info(self)
function_call/deepseekv3_detector.py:
  DeepSeekV3Detector.__init__(self)
  DeepSeekV3Detector.build_ebnf(self,tools)
  DeepSeekV3Detector.detect_and_parse(self,text,tools)
  DeepSeekV3Detector.has_tool_call(self,text)
  DeepSeekV3Detector.parse_streaming_increment(self,new_text,tools)
  DeepSeekV3Detector.structure_info(self)
function_call/ebnf_composer.py:
  EBNFComposer._handle_enum(prop,function_format)
  EBNFComposer._handle_type(prop,function_format)
  EBNFComposer.build_ebnf(tools,function_format,sequence_start_token,sequence_end_token,individual_call_start_token,individual_call_end_token,tool_call_separator,call_rule_fmt,key_value_rule_fmt,key_value_separator)
  EBNFComposer.format_enum_val(v)
  EBNFComposer.get_type_mapping(function_format)
  EBNFComposer.get_value_rule(prop,function_format)
function_call/function_call_parser.py:
  FunctionCallParser.__init__(self,tools,tool_call_parser)
  FunctionCallParser.get_ebnf(self,tool_choice)
  FunctionCallParser.get_structure_constraint(self,tool_choice)
  FunctionCallParser.get_structure_tag(self)
  FunctionCallParser.has_tool_call(self,text)
  FunctionCallParser.parse_non_stream(self,full_text)
  FunctionCallParser.parse_stream_chunk(self,chunk_text)
function_call/glm4_moe_detector.py:
  Glm4MoeDetector.__init__(self)
  Glm4MoeDetector.build_ebnf(self,tools)
  Glm4MoeDetector.detect_and_parse(self,text,tools)
  Glm4MoeDetector.has_tool_call(self,text)
  Glm4MoeDetector.parse_streaming_increment(self,new_text,tools)
  Glm4MoeDetector.structure_info(self)
  Glm4MoeDetector.supports_structural_tag(self)
  get_argument_type(func_name,arg_key,defined_tools)
  parse_arguments(json_value)
function_call/gpt_oss_detector.py:
  GptOssDetector.__init__(self)
  GptOssDetector._extract_tool_call_from_event(self,content,tool_indices,tool_index)
  GptOssDetector.build_ebnf(self,tools)
  GptOssDetector.detect_and_parse(self,text,tools)
  GptOssDetector.has_tool_call(self,text)
  GptOssDetector.parse_streaming_increment(self,new_text,tools)
  GptOssDetector.structure_info(self)
function_call/kimik2_detector.py:
  KimiK2Detector.__init__(self)
  KimiK2Detector.build_ebnf(self,tools)
  KimiK2Detector.detect_and_parse(self,text,tools)
  KimiK2Detector.get_info(name)
  KimiK2Detector.has_tool_call(self,text)
  KimiK2Detector.parse_streaming_increment(self,new_text,tools)
  KimiK2Detector.structure_info(self)
function_call/llama32_detector.py:
  Llama32Detector.__init__(self)
  Llama32Detector.build_ebnf(self,tools)
  Llama32Detector.detect_and_parse(self,text,tools)
  Llama32Detector.has_tool_call(self,text)
  Llama32Detector.structure_info(self)
function_call/mistral_detector.py:
  MistralDetector.__init__(self)
  MistralDetector._extract_json_array(self,text)
  MistralDetector.build_ebnf(self,tools)
  MistralDetector.detect_and_parse(self,text,tools)
  MistralDetector.has_tool_call(self,text)
  MistralDetector.structure_info(self)
function_call/pythonic_detector.py:
  PythonicDetector.__init__(self)
  PythonicDetector._find_matching_bracket(self,buffer,start)
  PythonicDetector._get_parameter_value(self,val)
  PythonicDetector._strip_and_split_buffer(self,buffer)
  PythonicDetector._text_strip(text)
  PythonicDetector.build_ebnf(self,tools)
  PythonicDetector.detect_and_parse(self,text,tools)
  PythonicDetector.has_tool_call(self,text)
  PythonicDetector.parse_streaming_increment(self,new_text,tools)
  PythonicDetector.structure_info(self)
  PythonicDetector.supports_structural_tag(self)
function_call/qwen25_detector.py:
  Qwen25Detector.__init__(self)
  Qwen25Detector.build_ebnf(self,tools)
  Qwen25Detector.detect_and_parse(self,text,tools)
  Qwen25Detector.has_tool_call(self,text)
  Qwen25Detector.parse_streaming_increment(self,new_text,tools)
  Qwen25Detector.structure_info(self)
function_call/qwen3_coder_detector.py:
  Qwen3CoderDetector.__init__(self)
  Qwen3CoderDetector._extract(self,text,tools)
  Qwen3CoderDetector._parse_and_stream_parameters(self,text_to_parse)
  Qwen3CoderDetector._parse_block(self,block,tools)
  Qwen3CoderDetector._reset_streaming_state(self)
  Qwen3CoderDetector.build_ebnf(self,tools)
  Qwen3CoderDetector.detect_and_parse(self,text,tools)
  Qwen3CoderDetector.has_tool_call(self,text)
  Qwen3CoderDetector.parse_streaming_increment(self,new_text,tools)
  Qwen3CoderDetector.structure_info(self)
  Qwen3CoderDetector.supports_structural_tag(self)
  _safe_val(raw)
function_call/step3_detector.py:
  Step3Detector.__init__(self)
  Step3Detector._parse_partial_tool_call(self,tools)
  Step3Detector._parse_steptml_invoke(self,text,tools)
  Step3Detector._reset_streaming_state(self)
  Step3Detector.build_ebnf(self,tools)
  Step3Detector.detect_and_parse(self,text,tools)
  Step3Detector.has_tool_call(self,text)
  Step3Detector.parse_streaming_increment(self,new_text,tools)
  Step3Detector.structure_info(self)
  Step3Detector.supports_structural_tag(self)
  get_argument_type(func_name,arg_key,defined_tools)
  parse_arguments(value)
function_call/utils.py:
  _find_common_prefix(s1,s2)
  _is_complete_json(input_str)
  _partial_json_loads(input_str,flags)
harmony_parser.py:
  CanonicalStrategy.__init__(self)
  CanonicalStrategy._extract_channel_type(self,header_text)
  CanonicalStrategy._is_commentary_filler_between_blocks(self,text,tokens,pos)
  CanonicalStrategy._is_standalone_structural_token(self,content)
  CanonicalStrategy._parse_block(self,text,tokens,start_pos)
  CanonicalStrategy._parse_partial_analysis(self,text,tokens,start_pos)
  CanonicalStrategy.parse(self,text)
  HarmonyParser.__init__(self)
  HarmonyParser.parse(self,chunk)
  TextStrategy.__init__(self)
  TextStrategy.parse(self,text)
  TextStrategy.set_buffer_context(self,buffer)
  iter_tokens(text,start_pos)
  prefix_hold(text,tokens)
hf_transformers_utils.py:
  attach_additional_stop_token_ids(tokenizer)
  check_gguf_file(model)
  download_from_hf(model_path,allow_patterns)
  get_config(model,trust_remote_code,revision,model_override_args,**kwargs)
  get_context_length(config)
  get_generation_config(model,trust_remote_code,revision,**kwargs)
  get_hf_text_config(config)
  get_processor(tokenizer_name,*args,tokenizer_mode,trust_remote_code,tokenizer_revision,use_fast,**kwargs)
  get_sparse_attention_config(model,sparse_attention_config_filename)
  get_tokenizer(tokenizer_name,*args,tokenizer_mode,trust_remote_code,tokenizer_revision,**kwargs)
  get_tokenizer_from_processor(processor)
host_shared_memory.py:
  HostSharedMemoryManager.__init__(self,base_name)
  HostSharedMemoryManager._malloc_raw(self,num_bytes)
  HostSharedMemoryManager.malloc(self,shape,dtype)
  get_host_shared_memory_manager()
  set_host_shared_memory_manager(instance)
jinja_template_utils.py:
  _is_attr_access(node,varname,key)
  _is_var_access(node,varname)
  _is_var_or_elems_access(node,varname,key)
  _try_extract_ast(chat_template)
  detect_jinja_template_content_format(chat_template)
  process_content_for_template_format(msg_dict,content_format,image_data,video_data,audio_data,modalities)
layers/activation.py:
  GeluAndMul.__init__(self,approximate)
  GeluAndMul.forward_cuda(self,x)
  GeluAndMul.forward_native(self,x)
  GeluAndMul.forward_npu(self,x)
  NewGELU.forward_cuda(self,x)
  NewGELU.forward_native(self,x)
  QuickGELU.forward_cuda(self,x)
  QuickGELU.forward_hip(self,x)
  QuickGELU.forward_native(self,x)
  QuickGELU.forward_npu(self,x)
  ReLU2.forward(self,x)
  ScaledActivation.__init__(self,act_module,intermediate_size,input_is_parallel,params_dtype)
  ScaledActivation.forward(self,x)
  ScaledActivation.weight_loader(self,param,loaded_weight)
  SiluAndMul.forward_cpu(self,x)
  SiluAndMul.forward_cuda(self,x)
  SiluAndMul.forward_native(self,x)
  SiluAndMul.forward_npu(self,x)
  get_act_fn(act_fn_name,quant_config,intermediate_size,input_is_parallel,params_dtype)
  get_cross_encoder_activation_function(config)
layers/amx_utils.py:
  PackWeightMethod.__init__(self,weight_names,transpose_dims)
  PackWeightMethod.process_weights_after_loading(self,module)
  _amx_process_weight_after_loading(module,weight_names,transpose_dims)
  amx_process_weight_after_loading(weight)
  dim_is_supported(weight)
layers/attention/aiter_backend.py:
  AiterAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  AiterAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  AiterAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AiterAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  AiterAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  AiterAttnBackend.init_forward_metadata(self,forward_batch)
  AiterAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AiterAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  AiterIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  AiterIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,encoder_lens,spec_info)
  AiterIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,encoder_lens,spec_info)
  AiterMlaIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  AiterMlaIndicesUpdaterPrefill.update(self,req_pool_indices,kv_lens,kv_lens_sum,extend_lens,max_q_len,max_kv_len,spec_info)
  AiterMlaIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,kv_lens,kv_lens_sum,extend_lens,max_q_len,max_kv_len,spec_info)
  AiterMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
  AiterMultiStepDraftBackend.call_fn(i,forward_batch)
  AiterMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  AiterMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AiterMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  AiterMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  AiterMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
layers/attention/ascend_backend.py:
  AscendAttnBackend.__init__(self,model_runner)
  AscendAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  AscendAttnBackend.forward_decode_graph(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  AscendAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AscendAttnBackend.gen_attention_mask(self,max_seq_len,dtype)
  AscendAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  AscendAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AscendAttnBackend.init_forward_metadata(self,forward_batch)
  AscendAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AscendAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
layers/attention/base_attn_backend.py:
  AttentionBackend.forward(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  AttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  AttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  AttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  AttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  AttentionBackend.init_forward_metadata(self,forward_batch)
  AttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  AttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  AttentionBackend.support_triton(self)
layers/attention/cutlass_mla_backend.py:
  CutlassMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  CutlassMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  CutlassMLABackend.get_cuda_graph_seq_len_fill_value(self)
  CutlassMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,block_kv_indices)
  CutlassMLABackend.init_forward_metadata(self,forward_batch)
  CutlassMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  CutlassMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  CutlassMLADecodeMetadata.__init__(self,workspace,block_kv_indices)
layers/attention/double_sparsity_backend.py:
  DoubleSparseAttnBackend.__init__(self,model_runner)
  DoubleSparseAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  DoubleSparseAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  DoubleSparseAttnBackend.init_forward_metadata(self,forward_batch)
layers/attention/dual_chunk_flashattention_backend.py:
  DualChunkFlashAttentionBackend.__init__(self,model_runner)
  DualChunkFlashAttentionBackend._do_flash_attn(self,query_states,key_states,value_states,softmax_scale,causal,max_seqlen_k,stage,vertical_indices,slash_indices,vertical_indices_count,slash_indices_count,mergehead_softmax_scale,sparse_attn_enabled)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_decoding(self,query,query_succ,query_inter,key_cache,value_cache,block_table,cache_seqlens,softmax_scale,causal,chunk_size,local_size,original_max_position_embeddings,decode_meta)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_decoding_with_exp_sums(self,query,key_cache,value_cache,block_table,cache_seqlens,softmax_scale,causal)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_prefill(self,q,q_succ,q_inter,q_succ_critical,q_inter_critical,k,v,cu_seqlens_q,cu_seqlens_k,orig_seq_lens,scaling_factor,softmax_scale,causal,window_size,block_table,chunk_size,local_size)
  DualChunkFlashAttentionBackend._dual_chunk_flash_attn_prefill_func(self,q,q_succ,q_inter,q_succ_critical,q_inter_critical,k,v,block_table,softmax_scale,chunk_size,local_size,scaling_factor,k_length,sparse_attn_enabled,heads_vertical_size,heads_slash_size,group_size)
  DualChunkFlashAttentionBackend._merge_attn_outputs(self,flash_results,return_lse)
  DualChunkFlashAttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  DualChunkFlashAttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  DualChunkFlashAttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  DualChunkFlashAttentionBackend.get_sparse_attention_config(self,layer_idx)
  DualChunkFlashAttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  DualChunkFlashAttentionBackend.init_forward_metadata(self,forward_batch)
  DualChunkFlashAttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  DualChunkFlashAttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu,out_cache_loc)
  _get_block(block_table,block_size,begin,end)
  _sum_all_diagonal_matrix(mat)
  _vertical_slash_sparse_attention(query,key,value,v_idx,s_idx,softmax_scale,causal,stage,block_size_M,block_size_N,vertical_indices_count,slash_indices_count)
layers/attention/flashattention_backend.py:
  FlashAttentionBackend.__init__(self,model_runner,skip_prefill,speculative_step_id,topk,speculative_num_steps)
  FlashAttentionBackend._init_local_attn_metadata(self,forwardbatch,metadata,device)
  FlashAttentionBackend._init_sliding_window_attn_spec_metadata(self,metadata,metadata_expand,metadata_swa)
  FlashAttentionBackend._update_local_attn_metadata_for_capture(self,metadata,bs)
  FlashAttentionBackend._update_local_attn_metadata_for_replay(self,metadata,bs)
  FlashAttentionBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,sinks)
  FlashAttentionBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,sinks)
  FlashAttentionBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashAttentionBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashAttentionBackend.init_forward_metadata(self,forward_batch)
  FlashAttentionBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashAttentionBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu,out_cache_loc)
  FlashAttentionMultiStepBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashAttentionMultiStepBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashAttentionMultiStepBackend.init_forward_metadata(self,forward_batch)
  FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  _prepare_swa_spec_page_table_kernel(dst_ptr,src_a_ptr,src_b_ptr,seq_len_a_ptr,seq_len_b_ptr,dst_stride_m,dst_stride_n,a_stride_m,a_stride_n,b_stride_m,b_stride_n,LEN_A,LEN_B,REPEAT_STEP,BLOCK_N)
  cdiv(a,b)
  make_local_attention_virtual_batches(attn_chunk_size,query_start_loc_np,seq_lens_np,block_table,page_size)
  merge_state_v2_wrapper(o,s_a,o_exp,s_b)
  normal_decode_set_metadata(cache_seqlens_int32,cu_seqlens_k,page_table,req_to_token,req_pool_indices,strided_indices,max_seq_pages,seq_lens,seq_len_delta,page_size)
  prepare_swa_spec_page_table_triton(page_table_dst,page_table_a,page_table_b,seq_len_a,seq_len_b,speculative_num_draft_tokens)
layers/attention/flashinfer_backend.py:
  FlashInferAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  FlashInferAttnBackend._get_wrapper_idx(self,layer)
  FlashInferAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashInferAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashInferAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashInferAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  FlashInferAttnBackend.init_forward_metadata(self,forward_batch)
  FlashInferAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashInferAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashInferIndicesUpdaterDecode.__init__(self,model_runner,attn_backend)
  FlashInferIndicesUpdaterDecode.call_begin_forward(self,wrapper,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,kv_indptr,kv_start_idx,spec_info,seq_lens_cpu,use_sliding_window_kv_pool)
  FlashInferIndicesUpdaterDecode.update(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_cross_attention(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterDecode.update_sliding_window(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,decode_wrappers,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  FlashInferIndicesUpdaterPrefill.call_begin_forward(self,wrapper_ragged,wrapper_paged,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,seq_lens,prefix_lens,kv_start_idx,kv_indptr,qo_indptr,use_ragged,spec_info,use_sliding_window_kv_pool)
  FlashInferIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_cross_attention(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_single_wrapper(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferIndicesUpdaterPrefill.update_sliding_window(self,req_pool_indices,seq_lens,seq_lens_cpu,seq_lens_sum,prefix_lens,prefill_wrappers,use_ragged,encoder_lens,spec_info)
  FlashInferMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  FlashInferMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashInferMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashInferMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashInferMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  fast_decode_plan(self,indptr,indices,last_page_len,num_qo_heads,num_kv_heads,head_dim,page_size,pos_encoding_mode,window_left,logits_soft_cap,q_data_type,kv_data_type,data_type,sm_scale,rope_scale,rope_theta,non_blocking)
  should_use_tensor_core(kv_cache_dtype,num_attention_heads,num_kv_heads)
layers/attention/flashinfer_mla_backend.py:
  FlashInferMLAAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,q_indptr_decode_buf)
  FlashInferMLAAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  FlashInferMLAAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope)
  FlashInferMLAAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  FlashInferMLAAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  FlashInferMLAAttnBackend.init_forward_metadata(self,forward_batch)
  FlashInferMLAAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashInferMLAAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashInferMLAAttnBackend.init_mha_chunk_metadata(self,forward_batch)
  FlashInferMLAIndicesUpdaterDecode.__init__(self,model_runner,attn_backend)
  FlashInferMLAIndicesUpdaterDecode.call_begin_forward(self,wrapper,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,q_indptr,kv_indptr,init_metadata_replay,spec_info,**fast_decode_kwargs)
  FlashInferMLAIndicesUpdaterDecode.update(self,req_pool_indices,seq_lens,seq_lens_sum,decode_wrapper,init_metadata_replay,spec_info,**fast_decode_kwargs)
  FlashInferMLAIndicesUpdaterPrefill.__init__(self,model_runner,attn_backend)
  FlashInferMLAIndicesUpdaterPrefill.call_begin_forward(self,wrapper_ragged,wrapper_paged,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,seq_lens,prefix_lens,kv_indptr,qo_indptr,use_ragged,spec_info)
  FlashInferMLAIndicesUpdaterPrefill.update(self,req_pool_indices,seq_lens,seq_lens_sum,prefix_lens,prefill_wrapper_paged,use_ragged,spec_info)
  FlashInferMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashInferMLAMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  FlashInferMLAMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  FlashInferMhaChunkKVRunner.__init__(self,model_runner,attn_backend)
  FlashInferMhaChunkKVRunner.forward(self,q,k,v,layer,forward_batch)
  FlashInferMhaChunkKVRunner.update_prefix_chunks(self,num_prefix_chunks)
  FlashInferMhaChunkKVRunner.update_wrapper(self,forward_batch)
  fast_mla_decode_plan(self,qo_indptr_cpu,kv_indptr_cpu,kv_indices,kv_len_arr_cpu,num_heads,head_dim_ckv,head_dim_kpe,page_size,causal,sm_scale,q_data_type,kv_data_type)
layers/attention/flashmla_backend.py:
  FlashMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf)
  FlashMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashMLABackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  FlashMLABackend.get_cuda_graph_seq_len_fill_value(self)
  FlashMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,block_kv_indices)
  FlashMLABackend.init_forward_metadata(self,forward_batch)
  FlashMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  FlashMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  FlashMLADecodeMetadata.__init__(self,flashmla_metadata,num_splits,block_kv_indices)
  FlashMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashMLAMultiStepDraftBackend.call_fn(i,forward_batch)
  FlashMLAMultiStepDraftBackend.common_template(self,forward_batch,call_fn)
  FlashMLAMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  FlashMLAMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
layers/attention/hybrid_attn_backend.py:
  HybridAttnBackend.__init__(self,model_runner,prefill_backend,decode_backend)
  HybridAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  HybridAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  HybridAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  HybridAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  HybridAttnBackend.init_forward_metadata(self,forward_batch)
  HybridAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  HybridAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
layers/attention/intel_amx_backend.py:
  IntelAMXAttnBackend.__init__(self,model_runner)
  IntelAMXAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  IntelAMXAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  IntelAMXAttnBackend.init_forward_metadata(self,forward_batch)
  IntelAMXAttnBackend.support_triton(self)
layers/attention/merge_state.py:
  _supported_dtypes(o)
  _supported_headdim(o)
  merge_state(prefix_output,prefix_lse,suffix_output,suffix_lse,output,output_lse)
layers/attention/tbo_backend.py:
  TboAttnBackend.__init__(self,primary,children)
  TboAttnBackend._init_forward_metadata_cuda_graph_children(self,fn_name,bs,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info,capture_num_tokens,replay_seq_lens_sum,replay_seq_lens_cpu)
  TboAttnBackend.forward_decode(self,*args,**kwargs)
  TboAttnBackend.forward_extend(self,*args,**kwargs)
  TboAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TboAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TboAttnBackend.init_forward_metadata(self,forward_batch)
  TboAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TboAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TboAttnBackend.init_new(cls,creator)
  _init_forward_metadata_cuda_graph_split(fn_name,seq_slice,output_bs,bs,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info,capture_num_tokens,replay_seq_lens_sum,replay_seq_lens_cpu)
layers/attention/torch_native_backend.py:
  TorchNativeAttnBackend.__init__(self,model_runner)
  TorchNativeAttnBackend._run_sdpa_forward_decode(self,query,output,k_cache,v_cache,req_to_token,req_pool_indices,seq_lens,scaling,enable_gqa,causal)
  TorchNativeAttnBackend._run_sdpa_forward_extend(self,query,output,k_cache,v_cache,req_to_token,req_pool_indices,seq_lens,extend_prefix_lens,extend_seq_lens,scaling,enable_gqa,causal)
  TorchNativeAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  TorchNativeAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  TorchNativeAttnBackend.init_forward_metadata(self,forward_batch)
  TorchNativeAttnBackend.support_triton(self)
layers/attention/triton_backend.py:
  TritonAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  TritonAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,sinks)
  TritonAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,sinks)
  TritonAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TritonAttnBackend.get_num_kv_splits(self,num_kv_splits,seq_lens)
  TritonAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TritonAttnBackend.init_forward_metadata(self,forward_batch)
  TritonAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TritonAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TritonMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  TritonMultiStepDraftBackend.call_fn(i,forward_batch)
  TritonMultiStepDraftBackend.common_template(self,forward_batch,kv_indices_buffer,call_fn)
  TritonMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TritonMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  TritonMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  TritonMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
  get_num_kv_splits_triton(num_kv_splits_ptr,seq_lens_ptr,num_seq,num_group,num_head,num_kv_head,max_kv_splits,device_core_count,MAX_NUM_SEQ)
  logit_capping_mod(logit_capping_method,logit_cap)
  update_sliding_window_buffer(window_kv_indptr,req_to_token,sliding_window_size,seq_lens,req_pool_indices,bs,device,token_to_kv_pool_allocator)
  update_sliding_window_buffer_cuda_graph(window_kv_indptr,window_kv_indices,req_to_token,sliding_window_size,seq_lens,req_pool_indices,bs,token_to_kv_pool_allocator)
layers/attention/triton_ops/decode_attention.py:
  _decode_att_m_fwd(q,k_buffer,v_buffer,att_out,att_lse,kv_indptr,kv_indices,num_kv_splits,max_kv_splits,sm_scale,logit_cap,xai_temperature_len)
  _decode_grouped_att_m_fwd(q,k_buffer,v_buffer,att_out,att_lse,kv_indptr,kv_indices,num_kv_splits,max_kv_splits,sm_scale,logit_cap,xai_temperature_len)
  _decode_softmax_reducev_fwd(logits,lse,q,o,v_buffer,kv_indptr,num_kv_splits,max_kv_splits,sinks)
  _fwd_grouped_kernel_stage1(Q,K_Buffer,V_Buffer,sm_scale,kv_indptr,kv_indices,Att_Out,Att_Lse,num_kv_splits,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,kv_group_num,q_head_num,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_N,BLOCK_H,MIN_BLOCK_KV,logit_cap,xai_temperature_len,Lk,Lv)
  _fwd_kernel_stage1(Q,K_Buffer,V_Buffer,sm_scale,kv_indptr,kv_indices,Att_Out,Att_Lse,num_kv_splits,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,kv_group_num,BLOCK_DMODEL,BLOCK_DV,BLOCK_N,MIN_BLOCK_KV,logit_cap,Lk,Lv,xai_temperature_len)
  _fwd_kernel_stage2(Mid_O,Mid_O_1,O,kv_indptr,num_kv_splits,sink_ptr,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_obs,stride_oh,MAX_KV_SPLITS,MIN_BLOCK_KV,BLOCK_DV,Lv,HAS_SINK)
  decode_attention_fwd(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
  decode_attention_fwd_grouped(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
  decode_attention_fwd_normal(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,attn_logits,attn_lse,num_kv_splits,max_kv_splits,sm_scale,logit_cap,sinks,xai_temperature_len)
  tanh(x)
layers/attention/triton_ops/double_sparsity_attention.py:
  _fwd_kernel(Q_Extend,K_Extend,V_Extend,O_Extend,K_Buffer,V_Buffer,Req_to_tokens,B_req_idx,B_Seq_Len,B_Start_Loc_Extend,B_Seq_Len_Extend,sm_scale,kv_group_num,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,stride_req_to_tokens_b,logit_cap,Lq,Lv,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_M,BLOCK_N)
  _fwd_kernel_flash_decode_stage1(Q,K,V,sm_scale,Req_to_tokens,B_req_idx,B_Seqlen,Mid_O,Mid_O_LogExpSum,stride_req_to_tokens_b,stride_req_to_tokens_s,stride_qbs,stride_qh,stride_qd,stride_kbs,stride_kh,stride_kd,stride_vbs,stride_vh,stride_vd,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_od,stride_mid_o_eb,stride_mid_o_eh,stride_mid_o_es,gqa_group_size,BLOCK_SEQ,BLOCK_DMODEL,BLOCK_N)
  _fwd_kernel_flash_decode_stage2(B_Seqlen,Mid_O,Mid_O_LogExpSum,O,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_od,stride_mid_o_eb,stride_mid_o_eh,stride_mid_o_es,stride_obs,stride_oh,stride_od,BLOCK_SEQ,BLOCK_DMODEL)
  _sparse_fwd_kernel_flash_decode_stage1(Q_Label,K_Label_Buffer,sm_scale,Req_to_tokens,B_Seqlen,Att_Out,stride_req_to_tokens_b,stride_qbs,stride_qh,stride_buf_kbs,stride_buf_kh,att_stride_h,att_stride_b,kv_group_num,BLOCK_DMODEL,BLOCK_N,logit_cap)
  _sparse_fwd_kernel_flash_decode_stage2(Q,K,V,sm_scale,Req_to_tokens,Topk_token_indices,Mid_O,Mid_O_LogExpSum,Heavy_token_num,stride_req_to_tokens_b,stride_topk_token_indices_h,stride_topk_token_indices_b,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_o_eb,stride_mid_o_eh,gqa_group_size,BLOCK_SEQ,BLOCK_DMODEL,BLOCK_N)
  _sparse_fwd_kernel_flash_decode_stage3(Mid_O,Mid_O_LogExpSum,O,seq_len,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_mid_o_eb,stride_mid_o_eh,stride_obs,stride_oh,BLOCK_SEQ,BLOCK_DMODEL)
  extend_attention_fwd(q_extend,k_extend,v_extend,o_extend,k_buffer,v_buffer,req_to_tokens,b_req_idx,b_seq_len,b_seq_len_extend,b_start_loc_extend,max_len_extend,sm_scale,logit_cap)
  flash_decode_attention_fwd(q,k_buffer,v_buffer,o,req_to_token,b_req_idx,b_start_loc,b_seq_len,attn_logits,max_len_in_batch,sm_scale,logit_cap)
  flash_decode_sparse_attention_fwd(q,k_buffer,v_buffer,o,q_label,k_label_buffer,req_to_token,b_seq_len,max_len_in_batch,sm_scale,logit_cap,heavy_token_num,att_out_approx,mid_out,mid_o_logexpsum,BLOCK_SEQ)
  flash_decode_stage1(q,k,v,Req_to_tokens,B_req_idx,B_Seqlen,max_len_in_batch,mid_out,mid_out_logsumexp,block_seq)
  flash_decode_stage2(mid_out,mid_out_logexpsum,B_Seqlen,O,block_seq)
  sparse_flash_decode_stage1(q_label,k_label_buffer,att_out,Req_to_tokens,B_Seqlen,max_len_in_batch,sm_scale,logit_cap)
  sparse_flash_decode_stage2(q,k,v,Req_to_tokens,Topk_token_indices,heavy_token_num,mid_out,mid_out_logsumexp,block_seq,sm_scale)
  sparse_flash_decode_stage3(Seqlen,mid_out,mid_out_logexpsum,O,block_seq)
  tanh(x)
layers/attention/triton_ops/extend_attention.py:
  _fwd_kernel(Q_Extend,K_Extend,V_Extend,O_Extend,K_Buffer,V_Buffer,qo_indptr,kv_indptr,kv_indices,mask_ptr,mask_indptr,sink_ptr,window_kv_offset_ptr,sm_scale,kv_group_num,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,stride_buf_kbs,stride_buf_kh,stride_buf_vbs,stride_buf_vh,SLIDING_WINDOW_SIZE,logit_cap,xai_temperature_len,Lq,Lv,BLOCK_DMODEL,BLOCK_DPE,BLOCK_DV,BLOCK_M,BLOCK_N,USE_CUSTOM_MASK,IS_CAUSAL,SKIP_PREFIX_CUSTOM_MASK,STORE_TRANSPOSE,HAS_SINK)
  extend_attention_fwd(q_extend,k_extend,v_extend,o_extend,k_buffer,v_buffer,qo_indptr,kv_indptr,kv_indices,custom_mask,is_causal,mask_indptr,max_len_extend,sm_scale,logit_cap,skip_prefix_custom_mask,sliding_window_size,sinks,window_kv_offsets,xai_temperature_len)
  redundant_attention(q_extend,o_extend,k_buffer,v_buffer,b_req_idx,b_start_loc,b_seq_len,b_seq_len_prefix,max_len_in_batch)
  tanh(x)
layers/attention/triton_ops/merge_state.py:
  merge_state_kernel(output,output_lse,prefix_output,prefix_lse,suffix_output,suffix_lse,HEAD_SIZE,PADDED_HEAD_SIZE,OUTPUT_LSE)
  merge_state_triton(prefix_output,prefix_lse,suffix_output,suffix_lse,output,output_lse)
layers/attention/triton_ops/prefill_attention.py:
  _fwd_kernel(Q,K,V,sm_scale,B_Start_Loc,B_Seqlen,Out,stride_qbs,stride_qh,stride_kbs,stride_kh,stride_vbs,stride_vh,stride_obs,stride_oh,kv_group_num,BLOCK_M,BLOCK_DMODEL,BLOCK_N,IS_CAUSAL,Lk)
  context_attention_fwd(q,k,v,o,b_start_loc,b_seq_len,max_input_len,is_causal)
layers/attention/triton_ops/rocm_mla_decode_rope.py:
  _decode_grouped_att_m_fwd_rope(q,k_buffer,v_buffer,att_out,k_pe_tokens_out,kv_lora_rank,cos_sin_cache,positions,rotary_dim,kv_indptr,kv_indices,num_kv_splits,sm_scale,logit_cap,use_rope,is_neox_style)
  _fwd_grouped_kernel_stage1_rope(Q,K_Buffer,V_buffer,cos_sin_cache,positions,sm_scale,kv_indptr,kv_indices,Att_Out,k_pe_t_out,stride_qb,stride_qh,stride_buf_kbs,stride_buf_vbs,stride_mid_ob,stride_mid_oh,stride_mid_os,stride_kpe_tokens_out_b,stride_cos_sin_cache_s,stride_positions_b,rotary_dim,kv_lora_rank,qk_rope_head_dim,kv_group_num,q_head_num,BLOCK_C,BLOCK_R,BLOCK_N,BLOCK_H,NUM_KV_SPLITS,logit_cap,USE_ROPE,IS_NEOX_STYLE)
  decode_attention_fwd_grouped_rope(q,k_buffer,v_buffer,o,kv_indptr,kv_indices,k_pe_tokens,kv_lora_rank,rotary_dim,cos_sin_cache,positions,attn_logits,num_kv_splits,sm_scale,logit_cap,use_rope,is_neox_style)
  is_hip()
  tanh(x)
layers/attention/trtllm_mha_backend.py:
  TRTLLMHAAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,kv_last_page_len_buf,speculative_step_id)
  TRTLLMHAAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  TRTLLMHAAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache,**kwargs)
  TRTLLMHAAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  TRTLLMHAAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TRTLLMHAAttnBackend.init_forward_metadata(self,forward_batch)
  TRTLLMHAAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TRTLLMHAAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TRTLLMHAAttnMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
  TRTLLMHAAttnMultiStepDraftBackend.init_cuda_graph_state(self,max_bs,max_num_tokens)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata(self,forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(self,forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(self,forward_batch,bs)
layers/attention/trtllm_mla_backend.py:
  TRTLLMMLABackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf,q_indptr_decode_buf)
  TRTLLMMLABackend._calc_padded_blocks(self,max_seq_len)
  TRTLLMMLABackend._create_block_kv_indices(self,batch_size,max_blocks,req_pool_indices,seq_lens,device)
  TRTLLMMLABackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache,q_rope,k_rope,cos_sin_cache,is_neox)
  TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value(self)
  TRTLLMMLABackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  TRTLLMMLABackend.init_forward_metadata(self,forward_batch)
  TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  TRTLLMMLABackend.quantize_and_rope_for_fp8(self,q_nope,q_rope,k_nope,k_rope,forward_batch,cos_sin_cache,is_neox)
  TRTLLMMLAMultiStepDraftBackend.__init__(self,model_runner,topk,speculative_num_steps)
layers/attention/utils.py:
  create_flashinfer_kv_indices_triton(req_to_token_ptr,req_pool_indices_ptr,page_kernel_lens_ptr,kv_indptr,kv_start_idx,kv_indices_ptr,req_to_token_ptr_stride)
  create_flashmla_kv_indices_triton(req_to_token_ptr,req_pool_indices_ptr,page_kernel_lens_ptr,kv_start_idx,kv_indices_ptr,req_to_token_ptr_stride,kv_indices_ptr_stride,NUM_PAGE_PER_BLOCK,PAGED_SIZE)
layers/attention/vision.py:
  SingletonCache.empty(self)
  SingletonCache.get_data(self)
  SingletonCache.set_data(self,value)
  VisionAttention.__init__(self,embed_dim,num_heads,projection_size,use_qkv_parallel,qkv_backend,quant_config,dropout,softmax_in_single_precision,flatten_batch,prefix,proj_bias,num_dummy_heads,qkv_bias,qk_normalization,layer_norm_eps,customized_position_embedding_applier,**kwargs)
  VisionAttention._apply_qk_norm(self,q,k)
  VisionAttention._determine_attention_backend(self,passed_backend)
  VisionAttention.forward(self,x,cu_seqlens,position_embeddings,attention_mask,**kwargs)
  VisionFlash3Attention.__init__(self,**kwargs)
  VisionFlash3Attention.forward(self,q,k,v,cu_seqlens,bsz,seq_len,**kwargs)
  VisionSdpaAttention.__init__(self,head_dim,num_heads,num_kv_heads,dropout,flatten_batch,softmax_in_single_precision,**kwargs)
  VisionSdpaAttention._generate_mask_cache(s,flatten_batch,cu_seqlens)
  VisionSdpaAttention.forward(self,q,k,v,bsz,cu_seqlens,attention_mask,**kwargs)
  VisionSdpaAttention.generate_patch_attention_mask(self,s,cu_seqlens,flatten_batch)
  VisionTritonAttention.__init__(self,**kwargs)
  VisionTritonAttention.forward(self,q,k,v,cu_seqlens,bsz,seq_len,**kwargs)
  _get_cu_seqlens_for_shape(batch_size,seqlen,device)
layers/attention/vision_utils.py:
  pad_vit_attn_dummy_heads(config,name,loaded_weight)
  update_vit_attn_dummy_heads_config(config)
layers/attention/wave_backend.py:
  WaveAttnBackend.__init__(self,model_runner,skip_prefill,kv_indptr_buf)
  WaveAttnBackend.forward_decode(self,q,k,v,layer,forward_batch,save_kv_cache)
  WaveAttnBackend.forward_extend(self,q,k,v,layer,forward_batch,save_kv_cache)
  WaveAttnBackend.get_cuda_graph_seq_len_fill_value(self)
  WaveAttnBackend.get_num_kv_splits(self,num_kv_splits,seq_lens)
  WaveAttnBackend.init_cuda_graph_state(self,max_bs,max_num_tokens,kv_indices_buf)
  WaveAttnBackend.init_forward_metadata(self,forward_batch)
  WaveAttnBackend.init_forward_metadata_capture_cuda_graph(self,bs,num_tokens,req_pool_indices,seq_lens,encoder_lens,forward_mode,spec_info)
  WaveAttnBackend.init_forward_metadata_replay_cuda_graph(self,bs,req_pool_indices,seq_lens,seq_lens_sum,encoder_lens,forward_mode,spec_info,seq_lens_cpu)
  get_num_kv_splits_triton(num_kv_splits_ptr,seq_lens_ptr,num_seq,num_group,num_head,num_kv_head,max_kv_splits,device_core_count,MAX_NUM_SEQ)
layers/attention/wave_ops/decode_attention.py:
  decode_attention_fwd(q,k_buffer,v_buffer,o,b_req_idx,req_to_token,attn_logits,attn_logits_max,num_kv_splits,max_kv_splits,sm_scale,logit_cap)
  decode_attention_intermediate_arrays_shapes(num_seqs,head_size_kv,num_query_heads,max_kv_splits)
  decode_attention_wave(q,k_buffer,v_buffer,o,b_req_idx,req_to_token,attn_logits,attn_logits_max,num_kv_splits,max_kv_splits,sm_scale,logit_cap)
  get_wave_kernel(shape,max_kv_splits,input_dtype,output_dtype,logit_cap)
layers/attention/wave_ops/extend_attention.py:
  extend_attention_wave(q_extend,k_extend,v_extend,k_buffer,v_buffer,qo_indptr,kv_indptr,kv_indices,custom_mask,mask_indptr,max_seq_len,output,is_causal,layer_scaling,logit_cap)
  get_wave_kernel(shape,q_shape,k_shape,v_shape,k_cache_shape,v_cache_shape,o_shape,input_dtype,output_dtype,size_dtype,is_causal,logit_cap,layer_scaling)
layers/attention/wave_ops/prefill_attention.py:
  prefill_attention_wave(q,k,v,o,b_start_loc,b_seq_len,max_seq_len,is_causal)
layers/communicator.py:
  CommunicateContext.init_new(cls)
  CommunicateContext.is_same_group_size(self,a,b)
  CommunicateSimpleFn._scattered_to_tp_attn_full(hidden_states,forward_batch,context)
  CommunicateSimpleFn._trivial(hidden_states,forward_batch,context)
  CommunicateSimpleFn.get_fn(input_mode,output_mode,context)
  CommunicateSummableTensorPairFn._gather(hidden_states,residual,forward_batch,context,**kwargs)
  CommunicateSummableTensorPairFn._scatter(hidden_states,residual,forward_batch,context)
  CommunicateSummableTensorPairFn._scatter_hidden_states(hidden_states,residual,forward_batch,context,allow_reduce_scatter)
  CommunicateSummableTensorPairFn._trivial(hidden_states,residual,forward_batch,context,**kwargs)
  CommunicateSummableTensorPairFn.execute(cls,hidden_states_input_mode,residual_input_mode,output_mode,context,**kwargs)
  CommunicateSummableTensorPairFn.get_fn(hidden_states_input_mode,residual_input_mode,output_mode,context)
  CommunicateWithAllReduceAndLayerNormFn._gather_hidden_states_and_residual(hidden_states,residual,forward_batch,layernorm,context,residual_input_mode)
  CommunicateWithAllReduceAndLayerNormFn._scatter_hidden_states_and_residual(hidden_states,residual,forward_batch,layernorm,context,residual_input_mode)
  CommunicateWithAllReduceAndLayerNormFn._simple(hidden_states,residual,forward_batch,layernorm,context)
  CommunicateWithAllReduceAndLayerNormFn.get_fn(hidden_states_input_mode,residual_input_mode,hidden_states_output_mode,residual_output_mode,context)
  LayerCommunicator.__init__(self,layer_scatter_modes,input_layernorm,post_attention_layernorm,allow_reduce_scatter,is_last_layer)
  LayerCommunicator.postprocess_layer(self,hidden_states,residual,forward_batch)
  LayerCommunicator.prepare_attn(self,hidden_states,residual,forward_batch)
  LayerCommunicator.prepare_mlp(self,hidden_states,residual,forward_batch)
  LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer(self,forward_batch)
  LayerCommunicator.should_use_reduce_scatter(self,forward_batch)
  LayerScatterModes._compute_layer_input_mode(cls,context)
  LayerScatterModes._compute_layer_output_mode(cls,context)
  LayerScatterModes._compute_middle_residual_mode(cls,context)
  LayerScatterModes._compute_mlp_mode(cls,context)
  LayerScatterModes.init_new(cls,**kwargs)
  ScatterMode.model_input_output()
  _LayerModeComputationContext.previous_layer(self)
  enable_moe_dense_fully_dp()
layers/dp_attention.py:
  DpPaddingMode.get_default_mode_in_cuda_graph(cls)
  DpPaddingMode.get_dp_padding_mode(cls,global_num_tokens)
  DpPaddingMode.is_max_len(self)
  DpPaddingMode.is_sum_len(self)
  _DpGatheredBufferWrapper.get_dp_global_num_tokens(cls)
  _DpGatheredBufferWrapper.get_global_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_global_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.set_dp_buffer_len(cls,global_dp_buffer_len,local_dp_buffer_len,global_num_tokens)
  _DpGatheredBufferWrapper.set_metadata(cls,hidden_size,dtype,device)
  _dp_gather(global_tokens,local_tokens,forward_batch,is_partial)
  _dp_gather_via_all_gather(global_tokens,local_tokens,forward_batch,is_partial)
  _dp_gather_via_all_reduce(global_tokens,local_tokens,forward_batch,is_partial)
  attn_tp_all_gather(output_list,input)
  attn_tp_all_gather_into_tensor(output,input)
  attn_tp_reduce_scatter_tensor(output,input)
  compute_dp_attention_local_info(enable_dp_attention,tp_rank,tp_size,dp_size,moe_dense_tp_size)
  compute_dp_attention_world_info(enable_dp_attention,tp_rank,tp_size,dp_size)
  disable_dp_size()
  dp_gather_partial(global_tokens,local_tokens,forward_batch)
  dp_gather_replicate(global_tokens,local_tokens,forward_batch)
  dp_reduce_scatter_tensor(output,input)
  dp_scatter(local_tokens,global_tokens,forward_batch)
  get_attention_dp_rank()
  get_attention_dp_size()
  get_attention_tp_group()
  get_attention_tp_rank()
  get_attention_tp_size()
  get_dp_global_num_tokens()
  get_dp_local_info(forward_batch)
  get_global_dp_buffer()
  get_global_dp_buffer_len()
  get_local_attention_dp_rank()
  get_local_attention_dp_size()
  get_local_dp_buffer()
  get_local_dp_buffer_len()
  initialize_dp_attention(server_args,model_config)
  is_dp_attention_enabled()
  memcpy_triton(dst,src,dim,offset,sz,offset_src)
  memcpy_triton_kernel(dst_ptr,src_ptr,offset_ptr,sz_ptr,offset_src,chunk_size,BLOCK_SIZE)
  prod(x)
  set_dp_buffer_len(global_dp_buffer_len,local_dp_buffer_len,global_num_tokens)
layers/elementwise.py:
  FusedDualResidualRMSNorm.__call__(self,*args,**kwargs)
  FusedDualResidualRMSNorm.__init__(self,rmsnorm1,rmsnorm2)
  FusedDualResidualRMSNorm.forward(self,x,residual)
  FusedDualResidualRMSNorm.forward_cuda(self,x,residual,autotune)
  FusedDualResidualRMSNorm.forward_flashinfer(self,x,residual)
  FusedDualResidualRMSNorm.forward_native(self,x,residual)
  Softcap.__call__(self,*args,**kwargs)
  Softcap.__init__(self,softcap_const)
  Softcap.forward(self,x)
  Softcap.forward_cuda(self,x,autotune)
  Softcap.forward_native(self,x)
  experts_combine_kernel(out_hidden_states,moe_hidden_states,mlp_hidden_states,combine_k,hidden_dim,BLOCK_SIZE)
  experts_combine_triton(moe_hidden_states,mlp_hidden_states,output_buffer)
  fused_dual_residual_rmsnorm(x,residual,weight1,weight2,eps,autotune)
  fused_dual_residual_rmsnorm_kernel(output_ptr,mid_ptr,activ_ptr,residual_ptr,weight1_ptr,weight2_ptr,eps,hidden_dim,BLOCK_SIZE)
  fused_rmsnorm(x,weight,eps,autotune,inplace)
  fused_rmsnorm_kernel(output_ptr,activ_ptr,weight_ptr,eps,hidden_dim,BLOCK_SIZE)
  fused_softcap(x,softcap_const,autotune)
  fused_softcap_kernel(output_ptr,input_ptr,n_ele,softcap_const,BLOCK_SIZE)
  gelu_and_mul_kernel(out_hidden_states_ptr,out_scales_ptr,hidden_states_ptr,quant_max,static_scale,hidden_dim,BLOCK_SIZE)
  gelu_and_mul_triton(hidden_states,scales,quantize,out)
  silu_and_mul_kernel(out_hidden_states_ptr,out_scales_ptr,hidden_states_ptr,quant_max,static_scale,hidden_dim,BLOCK_SIZE)
  silu_and_mul_triton(hidden_states,scales,quantize,out)
layers/flashinfer_comm_fusion.py:
  FlashInferWorkspaceManager.__init__(self)
  FlashInferWorkspaceManager.cleanup(self)
  FlashInferWorkspaceManager.initialize(self,world_size,rank,max_token_num,hidden_dim,group,use_fp32_lamport)
  cleanup_flashinfer_workspace()
  ensure_workspace_initialized(max_token_num,hidden_dim,use_fp32_lamport)
  fake_flashinfer_allreduce_residual_rmsnorm(input_tensor,residual,weight,eps,max_token_num,use_oneshot,trigger_completion_at_end,fp32_acc)
  flashinfer_allreduce_residual_rmsnorm(input_tensor,residual,weight,eps,max_token_num,use_oneshot,trigger_completion_at_end,fp32_acc)
layers/layernorm.py:
  Gemma3RMSNorm.__init__(self,dim,eps)
  Gemma3RMSNorm._norm(self,x)
  Gemma3RMSNorm.extra_repr(self)
  Gemma3RMSNorm.forward_cuda(self,x)
  Gemma3RMSNorm.forward_native(self,x)
  Gemma3RMSNorm.forward_npu(self,x)
  GemmaRMSNorm.__init__(self,hidden_size,eps)
  GemmaRMSNorm.forward_cuda(self,x,residual)
  GemmaRMSNorm.forward_native(self,x,residual)
  GemmaRMSNorm.forward_npu(self,x,residual)
  RMSNorm.__init__(self,hidden_size,eps,var_hidden_size)
  RMSNorm.forward_aiter(self,x,residual)
  RMSNorm.forward_cpu(self,x,residual)
  RMSNorm.forward_cuda(self,x,residual)
  RMSNorm.forward_hip(self,x,residual)
  RMSNorm.forward_native(self,x,residual)
  RMSNorm.forward_npu(self,x,residual)
  RMSNorm.forward_with_allreduce_fusion(self,x,residual)
layers/linear.py:
  ColumnParallelLinear.__init__(self,input_size,output_size,bias,gather_output,skip_bias_add,params_dtype,quant_config,output_sizes,prefix,tp_rank,tp_size,use_presharded_weights)
  ColumnParallelLinear.extra_repr(self)
  ColumnParallelLinear.forward(self,input_)
  ColumnParallelLinear.weight_loader(self,param,loaded_weight)
  ColumnParallelLinear.weight_loader_v2(self,param,loaded_weight)
  LinearBase.__init__(self,input_size,output_size,skip_bias_add,params_dtype,quant_config,prefix)
  LinearBase.forward(self,x)
  MergedColumnParallelLinear.__init__(self,input_size,output_sizes,bias,gather_output,skip_bias_add,params_dtype,quant_config,prefix,tp_rank,tp_size,use_presharded_weights)
  MergedColumnParallelLinear._load_fused_module_from_checkpoint(self,param,loaded_weight)
  MergedColumnParallelLinear.weight_loader(self,param,loaded_weight,loaded_shard_id)
  MergedColumnParallelLinear.weight_loader_v2(self,param,loaded_weight,loaded_shard_id)
  QKVParallelLinear.__init__(self,hidden_size,head_size,total_num_heads,total_num_kv_heads,bias,skip_bias_add,params_dtype,quant_config,prefix,tp_rank,tp_size,load_presharded_attn)
  QKVParallelLinear._get_shard_offset_mapping(self,loaded_shard_id)
  QKVParallelLinear._get_shard_size_mapping(self,loaded_shard_id)
  QKVParallelLinear._load_fused_module_from_checkpoint(self,param,loaded_weight)
  QKVParallelLinear.weight_loader(self,param,loaded_weight,loaded_shard_id)
  QKVParallelLinear.weight_loader_v2(self,param,loaded_weight,loaded_shard_id)
  ReplicatedLinear.__init__(self,input_size,output_size,bias,skip_bias_add,params_dtype,quant_config,prefix)
  ReplicatedLinear.extra_repr(self)
  ReplicatedLinear.forward(self,x)
  ReplicatedLinear.weight_loader(self,param,loaded_weight)
  RowParallelLinear.__init__(self,input_size,output_size,bias,input_is_parallel,skip_bias_add,params_dtype,reduce_results,quant_config,prefix,tp_rank,tp_size,use_presharded_weights)
  RowParallelLinear.extra_repr(self)
  RowParallelLinear.forward(self,input_,skip_all_reduce)
  RowParallelLinear.weight_loader(self,param,loaded_weight)
  RowParallelLinear.weight_loader_v2(self,param,loaded_weight)
  adjust_bitsandbytes_4bit_shard(param,shard_offsets,loaded_shard_id)
  adjust_marlin_shard(param,shard_size,shard_offset)
  adjust_scalar_to_fused_array(param,loaded_weight,shard_id)
  adjust_shard_offsets(shard_offsets,loaded_weight,dim)
layers/logits_processor.py:
  LogitsMetadata.compute_dp_attention_metadata(self)
  LogitsMetadata.from_forward_batch(cls,forward_batch)
  LogitsProcessor.__init__(self,config,skip_all_gather,logit_scale)
  LogitsProcessor._get_logits(self,hidden_states,lm_head,logits_metadata,embedding_bias)
  LogitsProcessor.compute_temp_top_p_normalized_logprobs(last_logits,logits_metadata)
  LogitsProcessor.forward(self,input_ids,hidden_states,lm_head,logits_metadata,aux_hidden_states)
  LogitsProcessor.get_token_ids_logprobs(all_logprobs,logits_metadata)
  LogitsProcessor.get_top_logprobs(all_logprobs,logits_metadata)
  fused_softcap(full_logits,final_logit_softcapping)
  fused_softcap_kernel(full_logits_ptr,softcapping_value,n_elements,BLOCK_SIZE)
layers/moe/cutlass_moe.py:
  cutlass_fused_experts_fp8(a,w1_q,w2_q,w1_scale,w2_scale,topk_weights,topk_ids,a1_strides,c1_strides,a2_strides,c2_strides,workspace,a_ptrs,b_ptrs,out_ptrs,a_scales_ptrs,b_scales_ptrs,expert_offsets,problem_sizes1,problem_sizes2,use_fp8_blockscale)
  cutlass_moe_fp4(a,a1_gscale,w1_fp4,w1_blockscale,w1_alphas,a2_gscale,w2_fp4,w2_blockscale,w2_alphas,topk_weights,topk_ids,params,apply_router_weight_on_input)
layers/moe/cutlass_moe_params.py:
  CutlassMoEParams.__init__(self,cutlass_moe_type,device,num_experts,intermediate_size_per_partition,hidden_size)
  CutlassMoEParams.to_gemm1_args(self)
  CutlassMoEParams.to_gemm2_args(self)
layers/moe/cutlass_w4a8_moe.py:
  cutlass_w4a8_moe(start_expert_id,end_expert_id,total_num_experts,a,w1_q,w2_q,w1_scale,w2_scale,topk_weights,topk_ids_,local_topk_ids,a_strides1,b_strides1,c_strides1,a_strides2,b_strides2,c_strides2,s_strides13,s_strides2,expert_offsets,problem_sizes1,problem_sizes2,a1_scale,a2_scale,apply_router_weight_on_input)
layers/moe/ep_moe/kernels.py:
  _fwd_kernel_ep_gather(total_token_num,input_tensor,input_tensor_stride0,input_tensor_stride1,recv_topk_ids,recv_topk_ids_stride0,recv_topk_ids_stride1,recv_topk_weight,recv_topk_weight_stride0,recv_topk_weight_stride1,input_index,input_index_stride0,input_index_stride1,output_tensor,output_tensor_stride0,output_tensor_stride1,topk_num,BLOCK_D)
  _fwd_kernel_ep_scatter_1(num_recv_tokens_per_expert,expert_start_loc,m_indices,num_experts,BLOCK_E,BLOCK_EXPERT_NUM)
  _fwd_kernel_ep_scatter_2(total_token_num,expert_start_loc,recv_x,recv_x_stride0,recv_x_stride1,recv_x_scale,recv_x_scale_stride0,recv_x_scale_stride1,recv_topk,recv_topk_stride0,recv_topk_stride1,output_tensor,output_tensor_stride0,output_tensor_stride1,output_tensor_scale,output_tensor_scale_stride0,output_tensor_scale_stride1,output_index,output_index_stride0,output_index_stride1,topk_num,HIDDEN_SIZE,HIDDEN_SIZE_PAD,SCALE_HIDDEN_SIZE,SCALE_HIDDEN_SIZE_PAD)
  _silu_and_mul_post_quant_kernel(input_ptr,stride_input_0,stride_input_1,stride_input_2,output_ptr,stride_output_0,stride_output_1,stride_output_2,output_scale_ptr,stride_output_scale_0,stride_output_scale_1,stride_output_scale_2,masked_m_ptr,size_n,fp8_max,fp8_min,BLOCK_N,NUM_STAGE,SCALE_UE8M0)
  _tma_align_input_scale_kernel(input_scale_ptr,output_ptr,m,k_div_block_size,input_scale_stride_m,input_scale_stride_k,output_stride_m,output_stride_k,BLOCK_SIZE_K)
  compute_identity_kernel(top_k,hidden_states_ptr,expert_scales_ptr,num_tokens,output_ptr,hidden_dim,scales_stride,BLOCK_SIZE)
  compute_m_num_tiles_indptr(m_num_tiles_indptr,seg_indptr,batch_size,BLOCK_SIZE_M)
  compute_m_range(pid,batch_size,seg_indptr,weight_indices,m_num_tiles_indptr,BLOCK_SIZE_M)
  compute_masked_m_triton_kernel(seg_indptr,masked_m)
  compute_seg_indptr_triton_kernel(reorder_topk_ids,seg_indptr,num_toks)
  compute_src2dst_triton_kernel(reorder_ids,src2dst,num_toks,BLOCK_SIZE)
  deepep_compute_src2dst_triton_kernel(reorder_ids,src2dst,num_toks,num_minus_one,BLOCK_SIZE)
  deepep_permute_triton_kernel(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,topk,hidden_size,BLOCK_SIZE)
  deepep_post_reorder_triton_kernel(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,topk,hidden_size,BLOCK_SIZE)
  deepep_run_moe_deep_preprocess(topk_ids,num_experts)
  deepgemm_compute_src2dst_triton_kernel(topk_ids,reorder_ids,seg_indptr,src2dst,m_max,num_toks,BLOCK_SIZE)
  ep_gather(input_tensor,recv_topk_ids,recv_topk_weight,input_index,output_tensor)
  ep_scatter(recv_x,recv_x_scale,recv_topk,num_recv_tokens_per_expert,expert_start_loc,output_tensor,output_tensor_scale,m_indices,output_index,scale_ue8m0)
  fill_gateup_input_triton_kernel(input_ptr,scale_ptr,gateup_input_ptr,gateup_input_scale_ptr,src2dst_ptr,topk_ids_ptr,start_expert_id,end_expert_id,topk,m_max,hidden_size,scale_size,BLOCK_SIZE)
  gelu_and_mul_triton_kernel(gateup_output,down_input,hidden_size,reorder_topk_ids,scales,start_expert_id,end_expert_id,BLOCK_SIZE)
  get_tma_aligned_size(x,element_size)
  grouped_gemm_triton(a,b,c,batch_size,weight_column_major,seg_indptr,weight_indices,use_fp8_w8a8,scale_a,scale_b,block_shape,c_dtype,use_per_token_if_dynamic)
  grouped_gemm_triton_kernel(a,b,c,batch_size,N,K,seg_indptr,weight_indices,m_num_tiles_indptr,scale_a,scale_b,use_fp8_w8a8,group_n,group_k,a_stride_0,b_stride_0,b_stride_1,as_stride_0,as_stride_1,bs_stride_0,bs_stride_2,bs_stride_1,use_per_token_if_dynamic,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K)
  moe_ep_deepgemm_preprocess(topk_ids,num_experts,hidden_states,top_k,start_expert_id,end_expert_id,block_shape,output_dtype)
  post_reorder_triton_kernel(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,start_expert_id,end_expert_id,topk,hidden_size,dst_start,BLOCK_SIZE)
  post_reorder_triton_kernel_for_cutlass_moe(down_output_ptr,output_ptr,src2dst_ptr,topk_ids_ptr,topk_weights_ptr,num_experts,topk,hidden_size,dst_start,BLOCK_SIZE)
  pre_reorder_triton_kernel(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,start_expert_id,end_expert_id,topk,hidden_size,BLOCK_SIZE,use_per_token_if_dynamic)
  pre_reorder_triton_kernel_for_cutlass_moe(input_ptr,gateup_input_ptr,src2dst_ptr,topk_ids_ptr,a1_scales_ptr,num_experts,topk,hidden_size,BLOCK_SIZE)
  run_cutlass_moe_ep_preproess(local_topk_ids,local_num_experts)
  run_moe_ep_preproess(topk_ids,num_experts)
  silu_and_mul_masked_post_quant_fwd(input,output,output_scale,quant_group_size,masked_m,scale_ue8m0)
  silu_and_mul_triton_kernel(gateup_output,down_input,hidden_size,reorder_topk_ids,scales,start_expert_id,end_expert_id,BLOCK_SIZE)
  tanh(x)
  tma_align_input_scale(input_scale)
  zero_experts_compute_triton(expert_indices,expert_scales,num_experts,zero_expert_type,hidden_states)
layers/moe/ep_moe/layer.py:
  DeepEPMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,num_fused_shared_experts,params_dtype,quant_config,prefix,activation,routed_scaling_factor)
  DeepEPMoE.combine(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.dispatch(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.forward(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPMoE.forward_aiter(self,dispatch_output)
  DeepEPMoE.forward_deepgemm_contiguous(self,dispatch_output)
  DeepEPMoE.forward_deepgemm_masked(self,dispatch_output)
  DeepEPMoE.forward_npu(self,dispatch_output)
  DeepEPMoE.moe_impl(self,dispatch_output)
  EPMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,num_fused_shared_experts,params_dtype,quant_config,prefix,activation,routed_scaling_factor,gemm1_alpha,gemm1_clamp_limit,with_bias)
  EPMoE.forward(self,hidden_states,topk_output)
  EPMoE.forward_deepgemm(self,hidden_states,topk_output)
  _cast_to_e8m0_with_rounding_up(x)
  get_moe_impl_class(quant_config)
layers/moe/fused_moe_native.py:
  fused_moe_forward_native(layer,x,topk_output,moe_runner_config)
  moe_forward_native(layer,x,topk_output,moe_runner_config)
layers/moe/fused_moe_triton/__init__.py:
  get_config()
  override_config(config)
layers/moe/fused_moe_triton/fused_moe.py:
  _moe_sum_reduce_kernel(input_ptr,input_stride_0,input_stride_1,input_stride_2,output_ptr,output_stride_0,output_stride_1,token_num,topk_num,hidden_dim,routed_scaling_factor,BLOCK_M,BLOCK_DIM,NUM_STAGE)
  fused_experts(hidden_states,w1,w2,topk_output,moe_runner_config,b1,b2,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape)
  fused_experts_impl(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,inplace,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  fused_moe(hidden_states,w1,w2,topk_output,moe_runner_config,b1,b2,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape)
  fused_moe_kernel(a_ptr,b_ptr,bias_ptr,c_ptr,a_scale_ptr,b_scale_ptr,topk_weights_ptr,sorted_token_ids_ptr,expert_ids_ptr,num_tokens_post_padded_ptr,N,K,EM,num_valid_tokens,stride_am,stride_ak,stride_be,stride_bk,stride_bn,stride_bias_e,stride_bias_n,stride_cm,stride_cn,stride_asm,stride_ask,stride_bse,stride_bsk,stride_bsn,group_n,group_k,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M,MUL_ROUTED_WEIGHT,top_k,compute_type,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,per_channel_quant,even_Ks)
  fused_moe_kernel_gptq_awq(a_ptr,b_ptr,c_ptr,b_scale_ptr,b_zp_ptr,topk_weights_ptr,sorted_token_ids_ptr,expert_ids_ptr,num_tokens_post_padded_ptr,N,K,EM,num_valid_tokens,stride_am,stride_ak,stride_be,stride_bk,stride_bn,stride_cm,stride_cn,stride_bse,stride_bsk,stride_bsn,stride_bze,stride_bzk,stride_bzn,group_size,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M,MUL_ROUTED_WEIGHT,top_k,compute_type,has_zp,use_int4_w4a16,use_int8_w8a16,even_Ks)
  get_config_dtype_str(dtype,use_int8_w8a16,use_int4_w4a16,use_fp8_w8a8,use_int8_w8a8)
  get_config_file_name(E,N,dtype,block_shape)
  get_default_config(M,E,N,K,topk,dtype,is_marlin,block_shape)
  get_moe_configs(E,N,dtype,block_n,block_k)
  inplace_fused_experts(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  inplace_fused_experts_fake(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  invoke_fused_moe_kernel(A,B,bias,C,A_scale,B_scale,B_zp,topk_weights,topk_ids,sorted_token_ids,expert_ids,num_tokens_post_padded,mul_routed_weight,top_k,config,compute_type,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,block_shape,no_combine)
  moe_align_block_size(topk_ids,block_size,num_experts)
  moe_sum_reduce_torch_compile(x,out,routed_scaling_factor)
  moe_sum_reduce_triton(input,output,routed_scaling_factor)
  outplace_fused_experts(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  outplace_fused_experts_fake(hidden_states,w1,w2,topk_weights,topk_ids,b1,b2,activation,apply_router_weight_on_input,use_fp8_w8a8,use_int8_w8a8,use_int8_w8a16,use_int4_w4a16,per_channel_quant,w1_scale,w2_scale,w1_zp,w2_zp,a1_scale,a2_scale,block_shape,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_limit)
  swiglu_with_alpha_and_limit(x,gemm1_alpha,gemm1_limit)
  try_get_optimal_moe_config(w1_shape,w2_shape,top_k,dtype,M,is_marlin,block_shape)
  write_zeros_to_output(c_ptr,stride_cm,stride_cn,pid_n,N,offs_token,token_mask,BLOCK_SIZE_M,BLOCK_SIZE_N,compute_type)
layers/moe/fused_moe_triton/layer.py:
  FlashInferFP4MoE.__init__(self,*args,**kwargs)
  FlashInferFP4MoE._quantize_hidden_states_fp4(self,hidden_states)
  FlashInferFP4MoE.forward(self,hidden_states,topk_output)
  FlashInferFusedMoE.__init__(self,*args,**kwargs)
  FlashInferFusedMoE.forward(self,hidden_states,topk_output)
  FusedMoE.__init__(self,num_experts,hidden_size,intermediate_size,layer_id,top_k,num_fused_shared_experts,params_dtype,reduce_results,quant_config,prefix,activation,apply_router_weight_on_input,use_presharded_weights,inplace,no_combine,routed_scaling_factor,gemm1_alpha,gemm1_clamp_limit,use_weight_loader_fused,with_bias)
  FusedMoE._load_g_idx(self,shard_id,expert_data,shard_dim,loaded_weight,tp_rank)
  FusedMoE._load_model_weight_or_group_weight_scale(self,shard_dim,expert_data,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._load_per_channel_weight_scale(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank)
  FusedMoE._load_per_tensor_weight_scale(self,shard_id,param,loaded_weight,expert_id)
  FusedMoE._load_single_value(self,param,loaded_weight,expert_id)
  FusedMoE._load_w13(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._load_w2(self,expert_data,shard_dim,shard_id,loaded_weight,tp_rank,is_bias)
  FusedMoE._map_global_expert_id_to_local_expert_id(self,expert_id)
  FusedMoE._weight_loader_impl(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE._weight_loader_physical(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE.forward(self,hidden_states,topk_output)
  FusedMoE.make_expert_input_scale_params_mapping(cls,num_experts)
  FusedMoE.make_expert_params_mapping(cls,ckpt_gate_proj_name,ckpt_down_proj_name,ckpt_up_proj_name,num_experts)
  FusedMoE.make_expert_params_mapping_fused(cls,ckpt_gate_up_proj_name,ckpt_down_proj_name,ckpt_gate_up_proj_bias_name,ckpt_down_proj_bias_name)
  FusedMoE.make_expert_params_mapping_fused_mxfp4(cls,ckpt_gate_up_proj_name,ckpt_down_proj_name,ckpt_gate_up_proj_bias_name,ckpt_down_proj_bias_name,ckpt_gate_up_proj_scale_name,ckpt_down_proj_scale_name)
  FusedMoE.should_fuse_routed_scaling_factor_in_topk(self)
  FusedMoE.weight_loader(self,param,loaded_weight,weight_name,shard_id,expert_id)
  FusedMoE.weight_loader_fused(self,param,loaded_weight,weight_name,shard_id)
  _get_tile_tokens_dim(num_tokens,top_k,num_experts)
  _is_fp4_quantization_enabled()
  get_fused_moe_impl_class()
layers/moe/fused_moe_triton/triton_kernels_moe.py:
  quantize(w,dtype,dev,**opt)
  triton_kernel_fused_experts(hidden_states,w1,w2,routing_data,gather_indx,scatter_indx,inplace,activation,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
  triton_kernel_fused_experts_with_bias(hidden_states,w1,w1_pcg,b1,w2,w2_pcg,b2,routing_data,gather_indx,scatter_indx,inplace,activation,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape,gemm1_alpha,gemm1_clamp_limit)
  triton_kernel_moe_forward(hidden_states,w1,w2,topk_output,moe_runner_config,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
  triton_kernel_moe_with_bias_forward(hidden_states,w1,w1_pcg,b1,w2,w2_pcg,b2,topk_output,moe_runner_config,use_fp8_w8a8,per_channel_quant,global_num_experts,expert_map,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
layers/moe/rocm_moe_utils.py:
  rocm_aiter_asm_moe_tkw1_fake(hidden_states,w1,w2,topk_weights,topk_ids,fc1_scale,fc2_scale,fc1_smooth_scale,fc2_smooth_scale,a16,per_tensor_quant_scale,expert_mask,activation_method)
  rocm_aiter_asm_moe_tkw1_impl(hidden_states,w1,w2,topk_weights,topk_ids,fc1_scale,fc2_scale,fc1_smooth_scale,fc2_smooth_scale,a16,per_tensor_quant_scale,expert_mask,activation_method)
  rocm_fused_experts_tkw1(hidden_states,w1,w2,topk_weights,topk_ids,activation,apply_router_weight_on_input,use_fp8_w8a8,per_channel_quant,w1_scale,w2_scale,a1_scale,a2_scale,block_shape)
layers/moe/router.py:
  FusedMoeRouter.__call__(self,*args,**kwargs)
  FusedMoeRouter.__init__(self,router_linear,topk,moe_softcapping)
  FusedMoeRouter.forward(self,x,residual)
  FusedMoeRouter.forward_cuda(self,x,autotune)
  FusedMoeRouter.forward_vllm(self,x)
  fused_moe_router_impl(x,router_weight,topk,moe_softcapping,correction_bias)
  fused_moe_router_kernel(input_ptr,moe_router_weight_ptr,topk_weights_ptr,topk_ids_ptr,correction_bias_ptr,is_correction_bias,num_experts,topk,moe_softcapping,moe_renormalize,hidden_dim,BLOCK_SIZE)
  fused_moe_router_large_bs_impl(x,router_weight,topk,moe_softcapping,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K)
  fused_moe_router_large_bs_kernel(a_ptr,b_ptr,topk_weights_ptr,topk_ids_ptr,bs,num_experts,topk,moe_softcapping,moe_renormalize,K,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,stride_am,stride_bn)
  fused_moe_router_shim(moe_softcapping,hidden_states,gating_output,topk,renormalize,correction_bias)
layers/moe/token_dispatcher/base_dispatcher.py:
  BaseDispatcher.combine(self,*args,**kwargs)
  BaseDispatcher.dispatch(self,*args,**kwargs)
  DispatchOutput.format(self)
  DispatchOutputChecker.format_is_ascent_ll(dispatch_output)
  DispatchOutputChecker.format_is_deepep(dispatch_output)
  DispatchOutputChecker.format_is_deepep_ll(dispatch_output)
  DispatchOutputChecker.format_is_deepep_normal(dispatch_output)
  DispatchOutputChecker.format_is_standard(dispatch_output)
  DispatchOutputFormat.is_ascent_ll(self)
  DispatchOutputFormat.is_deepep(self)
  DispatchOutputFormat.is_deepep_ll(self)
  DispatchOutputFormat.is_deepep_normal(self)
  DispatchOutputFormat.is_standard(self)
layers/moe/token_dispatcher/deepep.py:
  AscendDeepEPLLOutput.format(self)
  DeepEPBuffer.clean_buffer(cls)
  DeepEPBuffer.get_deepep_buffer(cls,group,hidden_size,param_bytes,deepep_mode,num_max_dispatch_tokens_per_rank,num_experts)
  DeepEPBuffer.set_dispatch_mode_as_low_latency(cls)
  DeepEPBuffer.set_dispatch_mode_as_normal(cls)
  DeepEPConfig.__init__(self)
  DeepEPConfig.get_instance(cls)
  DeepEPDispatcher.__init__(self,group,router_topk,permute_fusion,num_experts,num_local_experts,hidden_size,params_dtype,deepep_mode,async_finish,return_recv_hook)
  DeepEPDispatcher._get_impl(self,forward_batch)
  DeepEPDispatcher._update_stage(self,old_stage,new_stage)
  DeepEPDispatcher.combine(self,*args,**kwargs)
  DeepEPDispatcher.combine_a(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPDispatcher.combine_b(self)
  DeepEPDispatcher.dispatch(self,*args,**kwargs)
  DeepEPDispatcher.dispatch_a(self,hidden_states,topk_idx,topk_weights,forward_batch)
  DeepEPDispatcher.dispatch_b(self)
  DeepEPLLOutput.format(self)
  DeepEPNormalOutput.format(self)
  _DeepEPDispatcherImplBase.__init__(self,group,router_topk,permute_fusion,num_experts,num_local_experts,hidden_size,params_dtype,deepep_mode)
  _DeepEPDispatcherImplBase._get_buffer(self)
  _DeepEPDispatcherImplBase.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplBase.combine_b(self,*args,**kwargs)
  _DeepEPDispatcherImplBase.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplBase.dispatch_b(self,*args,**kwargs)
  _DeepEPDispatcherImplLowLatency.__init__(self,return_recv_hook,**kwargs)
  _DeepEPDispatcherImplLowLatency._combine_core(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency._dispatch_core(self,hidden_states,topk_idx,use_fp8)
  _DeepEPDispatcherImplLowLatency._get_buffer(self)
  _DeepEPDispatcherImplLowLatency.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency.combine_b(self,hidden_states,event,hook)
  _DeepEPDispatcherImplLowLatency.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplLowLatency.dispatch_b(self,hidden_states,topk_idx,topk_weights,masked_m,expected_m,event,hook)
  _DeepEPDispatcherImplNormal.__init__(self,async_finish,**kwargs)
  _DeepEPDispatcherImplNormal._combine_core(self,x,previous_event)
  _DeepEPDispatcherImplNormal._dispatch_core(self,x,topk_idx,topk_weights,previous_event)
  _DeepEPDispatcherImplNormal._get_buffer(self)
  _DeepEPDispatcherImplNormal.combine_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplNormal.combine_b(self,output,previous_event)
  _DeepEPDispatcherImplNormal.dispatch_a(self,hidden_states,topk_idx,topk_weights)
  _DeepEPDispatcherImplNormal.dispatch_b(self,hidden_states,topk_idx,topk_weights,previous_event)
layers/moe/token_dispatcher/standard.py:
  StandardDispatchOutput.format(self)
layers/moe/topk.py:
  BypassedTopKOutput.format(self)
  StandardTopKOutput.format(self)
  TopK.__init__(self,top_k,use_grouped_topk,topk_group,num_expert_group,renormalize,num_fused_shared_experts,custom_routing_function,scoring_func,correction_bias,routed_scaling_factor,apply_routed_scaling_factor_on_output,force_topk)
  TopK.empty_topk_output(self,device)
  TopK.forward_cpu(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_cuda(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_native(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopK.forward_npu(self,hidden_states,router_logits,num_token_non_padded,expert_location_dispatch_info)
  TopKOutput.format(self)
  TopKOutputChecker.format_is_bypassed(topk_output)
  TopKOutputChecker.format_is_standard(topk_output)
  TopKOutputChecker.format_is_triton_kernel(topk_output)
  TopKOutputFormat.is_bypassed(self)
  TopKOutputFormat.is_standard(self)
  TopKOutputFormat.is_triton_kernel(self)
  TritonKernelTopKOutput.format(self)
  _biased_grouped_topk_postprocess(topk_ids,expert_location_dispatch_info,num_token_non_padded)
  _mask_topk_ids_padded_region(topk_ids,num_token_non_padded)
  apply_topk_weights_cpu(need_apply,topk_weights,inputs)
  biased_grouped_topk_cpu(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,compiled,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  biased_grouped_topk_gpu(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  biased_grouped_topk_impl(hidden_states,gating_output,correction_bias,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  fused_topk(hidden_states,gating_output,topk,renormalize,num_token_non_padded,expert_location_dispatch_info)
  fused_topk_cpu(hidden_states,gating_output,topk,renormalize,num_token_non_padded,expert_location_dispatch_info,correction_bias)
  fused_topk_torch_native(hidden_states,gating_output,topk,renormalize,correction_bias)
  grouped_topk_cpu(hidden_states,gating_output,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  grouped_topk_gpu(hidden_states,gating_output,topk,renormalize,num_expert_group,topk_group,num_fused_shared_experts,routed_scaling_factor,num_token_non_padded,expert_location_dispatch_info,apply_routed_scaling_factor_on_output)
  is_power_of_two(n)
  select_experts(hidden_states,router_logits,topk_config,num_token_non_padded,expert_location_dispatch_info)
layers/moe/utils.py:
  DeepEPMode.enable_low_latency(self)
  DeepEPMode.enable_normal(self)
  DeepEPMode.is_auto(self)
  DeepEPMode.is_low_latency(self)
  DeepEPMode.is_normal(self)
  DeepEPMode.resolve(self,is_extend_in_batch)
  MoeA2ABackend._missing_(cls,value)
  MoeA2ABackend.is_deepep(self)
  MoeA2ABackend.is_none(self)
  MoeRunnerBackend.is_auto(self)
  MoeRunnerBackend.is_flashinfer_cutlass(self)
  MoeRunnerBackend.is_flashinfer_mxfp4(self)
  MoeRunnerBackend.is_flashinfer_trtllm(self)
  MoeRunnerBackend.is_triton(self)
  MoeRunnerBackend.is_triton_kernel(self)
  get_deepep_config()
  get_deepep_mode()
  get_moe_a2a_backend()
  get_moe_runner_backend()
  get_tbo_token_distribution_threshold()
  initialize_moe_config(server_args)
  is_tbo_enabled()
  should_use_flashinfer_cutlass_moe_fp4_allgather()
  should_use_flashinfer_trtllm_moe()
layers/multimodal.py:
  _as_uint32_words(t)
  _final_splitmix64(x)
  _fmix32(x,C1,C2)
  _rotl32(x,r)
  add_tree_reduce_u64_kernel(in_ptr,out_ptr,n_elems,CHUNK)
  gpu_tensor_hash(tensor,seed,tile_words,block_words,reduce_chunk,num_warps,num_stages,use_cg)
  hash_tiles32_kernel_blocked(in_ptr,out_ptr,n_u32,seed1,seed2,FM_C1,FM_C2,POS_A,POS_B,TILE,BLOCK,USE_CG)
layers/parameter.py:
  BasevLLMParameter.__init__(self,data,weight_loader)
  BasevLLMParameter.__new__(cls,data,**kwargs)
  BasevLLMParameter._assert_and_load(self,loaded_weight)
  BasevLLMParameter.load_column_parallel_weight(self,loaded_weight)
  BasevLLMParameter.load_merged_column_weight(self,loaded_weight,**kwargs)
  BasevLLMParameter.load_qkv_weight(self,loaded_weight,**kwargs)
  BasevLLMParameter.load_row_parallel_weight(self,loaded_weight)
  BasevLLMParameter.weight_loader(self)
  PackedColumnParameter.__init__(self,packed_factor,packed_dim,marlin_tile_size,**kwargs)
  PackedColumnParameter.adjust_shard_indexes_for_packing(self,shard_size,shard_offset)
  PackedColumnParameter.marlin_tile_size(self)
  PackedColumnParameter.packed_dim(self)
  PackedColumnParameter.packed_factor(self)
  PackedvLLMParameter.__init__(self,packed_factor,packed_dim,marlin_tile_size,**kwargs)
  PackedvLLMParameter.adjust_shard_indexes_for_packing(self,shard_size,shard_offset)
  PackedvLLMParameter.marlin_tile_size(self)
  PackedvLLMParameter.packed_dim(self)
  PackedvLLMParameter.packed_factor(self)
  PerTensorScaleParameter.__init__(self,**kwargs)
  PerTensorScaleParameter._load_into_shard_id(self,loaded_weight,shard_id,**kwargs)
  PerTensorScaleParameter._shard_id_as_int(self,shard_id)
  PerTensorScaleParameter.load_column_parallel_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_merged_column_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_qkv_weight(self,*args,**kwargs)
  PerTensorScaleParameter.load_row_parallel_weight(self,*args,**kwargs)
  RowvLLMParameter.__init__(self,input_dim,**kwargs)
  RowvLLMParameter.input_dim(self)
  RowvLLMParameter.load_row_parallel_weight(self,loaded_weight,tp_rank,use_presharded_weights)
  _ColumnvLLMParameter.__init__(self,output_dim,**kwargs)
  _ColumnvLLMParameter.load_column_parallel_weight(self,loaded_weight,tp_rank,use_presharded_weights)
  _ColumnvLLMParameter.load_merged_column_weight(self,loaded_weight,**kwargs)
  _ColumnvLLMParameter.load_qkv_weight(self,loaded_weight,tp_rank,use_presharded_weights,**kwargs)
  _ColumnvLLMParameter.output_dim(self)
  _adjust_shard_indexes_for_marlin(shard_size,shard_offset,marlin_tile_size)
  _adjust_shard_indexes_for_packing(shard_size,shard_offset,packed_factor,marlin_tile_size)
  permute_param_layout_(param,input_dim,output_dim,**kwargs)
layers/pooler.py:
  CrossEncodingPooler.__init__(self,config,classifier,pooler)
  CrossEncodingPooler.forward(self,hidden_states,forward_batch)
  Pooler.__init__(self,pooling_type,normalize)
  Pooler.forward(self,hidden_states,forward_batch)
layers/quantization/__init__.py:
  DummyConfig.override_quantization_method(self,*args,**kwargs)
  get_quantization_config(quantization)
  monkey_patch_isinstance_for_vllm_base_layer(reverse)
  monkey_patch_moe_apply(class_obj)
  monkey_patch_quant_configs()
  new_apply(self,layer,x,topk_output,activation,apply_router_weight_on_input,inplace,no_combine,routed_scaling_factor)
  patched_isinstance(obj,classinfo)
layers/quantization/awq.py:
  AWQConfig.__init__(self,weight_bits,group_size,zero_point,modules_to_not_convert)
  AWQConfig.__repr__(self)
  AWQConfig.from_config(cls,config)
  AWQConfig.get_config_filenames()
  AWQConfig.get_min_capability(cls)
  AWQConfig.get_name(self)
  AWQConfig.get_quant_method(self,layer,prefix)
  AWQConfig.get_scaled_act_names(self)
  AWQConfig.get_supported_act_dtypes(self)
  AWQLinearMethod.__init__(self,quant_config)
  AWQLinearMethod.apply(self,layer,x,bias)
  AWQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  AWQLinearMethod.process_weights_after_loading(self,layer)
  AWQMarlinConfig.__init__(self,weight_bits,group_size,zero_point,lm_head_quantized,modules_to_not_convert,full_config)
  AWQMarlinConfig.__repr__(self)
  AWQMarlinConfig.from_config(cls,config)
  AWQMarlinConfig.get_config_filenames(cls)
  AWQMarlinConfig.get_min_capability(cls)
  AWQMarlinConfig.get_name(cls)
  AWQMarlinConfig.get_quant_method(self,layer,prefix)
  AWQMarlinConfig.get_scaled_act_names(self)
  AWQMarlinConfig.get_supported_act_dtypes(cls)
  AWQMarlinConfig.is_awq_marlin_compatible(cls,quant_config)
  AWQMarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  AWQMarlinLinearMethod.__init__(self,quant_config)
  AWQMarlinLinearMethod.apply(self,layer,x,bias)
  AWQMarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  AWQMarlinLinearMethod.process_weights_after_loading(self,layer)
  AWQMoEMethod.__init__(self,quant_config)
  AWQMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  AWQMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  AWQMoEMethod.process_weights_after_loading(self,layer)
  is_layer_skipped_awq(prefix,modules_to_not_convert)
layers/quantization/awq_triton.py:
  awq_dequantize_kernel(qweight_ptr,scales_ptr,zeros_ptr,group_size,result_ptr,num_cols,num_rows,BLOCK_SIZE_X,BLOCK_SIZE_Y)
  awq_dequantize_triton(qweight,scales,zeros,block_size_x,block_size_y)
  awq_gemm_kernel(a_ptr,b_ptr,c_ptr,zeros_ptr,scales_ptr,M,N,K,group_size,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,SPLIT_K)
  awq_gemm_triton(input,qweight,scales,qzeros,split_k_iters,block_size_m,block_size_n,block_size_k)
layers/quantization/base_config.py:
  FusedMoEMethodBase.apply(self,layer,x,topk_output,moe_runner_config)
  FusedMoEMethodBase.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  LinearMethodBase.apply(self,layer,x,bias)
  LinearMethodBase.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  QuantizationConfig.__init__(self)
  QuantizationConfig.from_config(cls,config)
  QuantizationConfig.get_config_filenames()
  QuantizationConfig.get_from_keys(config,keys)
  QuantizationConfig.get_from_keys_or(config,keys,default)
  QuantizationConfig.get_min_capability(cls)
  QuantizationConfig.get_name(self)
  QuantizationConfig.get_quant_method(self,layer,prefix)
  QuantizationConfig.get_scaled_act_names(self)
  QuantizationConfig.get_supported_act_dtypes(self)
  QuantizationConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  QuantizeMethodBase.apply(self,layer,*args,**kwargs)
  QuantizeMethodBase.create_weights(self,layer,*weight_args,**extra_weight_attrs)
  QuantizeMethodBase.process_weights_after_loading(self,layer)
  method_has_implemented_embedding(method_class)
layers/quantization/blockwise_int8.py:
  BlockInt8Config.__init__(self,is_checkpoint_int8_serialized,activation_scheme,ignored_layers,weight_block_size)
  BlockInt8Config.from_config(cls,config)
  BlockInt8Config.get_config_filenames(cls)
  BlockInt8Config.get_min_capability(cls)
  BlockInt8Config.get_name(cls)
  BlockInt8Config.get_quant_method(self,layer,prefix)
  BlockInt8Config.get_scaled_act_names(self)
  BlockInt8Config.get_supported_act_dtypes(cls)
  BlockInt8LinearMethod.__init__(self,quant_config)
  BlockInt8LinearMethod.apply(self,layer,x,bias)
  BlockInt8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  BlockInt8LinearMethod.process_weights_after_loading(self,layer)
  BlockInt8MoEMethod.__init__(self,quant_config)
  BlockInt8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  BlockInt8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  BlockInt8MoEMethod.process_weights_after_loading(self,layer)
layers/quantization/compressed_tensors/compressed_tensors.py:
  CompressedTensorsConfig.__init__(self,target_scheme_map,ignore,quant_format,sparsity_scheme_map,sparsity_ignore_list,kv_cache_scheme,config,packed_modules_mapping)
  CompressedTensorsConfig._check_scheme_supported(self,min_capability,error)
  CompressedTensorsConfig._get_scheme_from_parts(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_dynamic_token_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_fp8_w8a16(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_fp8_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_static_tensor_w8a8(self,weight_quant,input_quant)
  CompressedTensorsConfig._is_wNa16_group_channel(self,weight_quant,input_quant)
  CompressedTensorsConfig._parse_sparsity_config(cls,config)
  CompressedTensorsConfig._quantization_scheme_map_from_config(cls,config)
  CompressedTensorsConfig.from_config(cls,config)
  CompressedTensorsConfig.get_cache_scale(self,name)
  CompressedTensorsConfig.get_config_filenames(cls)
  CompressedTensorsConfig.get_linear_method(self)
  CompressedTensorsConfig.get_min_capability(cls)
  CompressedTensorsConfig.get_name(self)
  CompressedTensorsConfig.get_quant_method(self,layer,prefix)
  CompressedTensorsConfig.get_scaled_act_names(self)
  CompressedTensorsConfig.get_scheme(self,layer,layer_name)
  CompressedTensorsConfig.get_supported_act_dtypes(cls)
  CompressedTensorsConfig.supports_cutlass_24(weight_quant,input_quant,sparsity_scheme)
  CompressedTensorsLinearMethod.__init__(self,quantization_config)
  CompressedTensorsLinearMethod.apply(self,layer,x,bias)
  CompressedTensorsLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  CompressedTensorsLinearMethod.process_weights_after_loading(self,layer)
  DeviceCapability.as_version_str(self)
  DeviceCapability.to_int(self)
layers/quantization/compressed_tensors/compressed_tensors_moe.py:
  CompressedTensorsMoEMethod.__new__(cls,*args,**kwargs)
  CompressedTensorsMoEMethod.get_moe_method(quant_config)
  CompressedTensorsW8A8Fp8MoEMethod.__init__(self,quant_config)
  CompressedTensorsW8A8Fp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  CompressedTensorsW8A8Fp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  CompressedTensorsW8A8Fp8MoEMethod.process_weights_after_loading(self,layer)
  CompressedTensorsWNA16MoEMethod.__init__(self,quant_config)
  CompressedTensorsWNA16MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  CompressedTensorsWNA16MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  CompressedTensorsWNA16MoEMethod.get_scale_perms(num_bits)
  CompressedTensorsWNA16MoEMethod.marlin_moe_permute_scales(s,size_k,size_n,group_size,num_bits)
  CompressedTensorsWNA16MoEMethod.marlin_permute_scales(s,size_k,size_n,group_size,num_bits)
  CompressedTensorsWNA16MoEMethod.process_weights_after_loading(self,layer)
  CompressedTensorsWNA16MoEMethod.replace_tensor(name,new_t)
layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py:
  CompressedTensorsScheme.apply_weights(self,layer,x,bias)
  CompressedTensorsScheme.create_weights(self,*args,**kwargs)
  CompressedTensorsScheme.get_min_capability(cls)
  CompressedTensorsScheme.process_weights_after_loading(self,layer)
layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py:
  CompressedTensorsW8A16Fp8.__init__(self,strategy,is_static_input_scheme)
  CompressedTensorsW8A16Fp8.apply_weights(self,layer,x,bias)
  CompressedTensorsW8A16Fp8.create_weights(self,layer,input_size,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  CompressedTensorsW8A16Fp8.get_min_capability(cls)
  CompressedTensorsW8A16Fp8.process_weights_after_loading(self,layer)
  apply_fp8_marlin_linear(*args,**kwargs)
  prepare_fp8_layer_for_marlin(*args,**kwargs)
layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py:
  CompressedTensorsW8A8Fp8.__init__(self,strategy,is_static_input_scheme)
  CompressedTensorsW8A8Fp8.apply_weights(self,layer,x,bias)
  CompressedTensorsW8A8Fp8.create_weights(self,layer,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  CompressedTensorsW8A8Fp8.get_min_capability(cls)
  CompressedTensorsW8A8Fp8.process_weights_after_loading(self,layer)
layers/quantization/compressed_tensors/utils.py:
  _find_first_match(value,targets,check_contains)
  _is_equal_or_regex_match(value,target,check_contains)
  _match_fused_layer(layer_name,target_layers,fused_mapping)
  check_equal_or_regex_match(layer_name,targets)
  find_matched_target(layer_name,module,targets,fused_mapping)
  is_activation_quantization_format(format)
  should_ignore_layer(layer_name,ignore,fused_mapping)
layers/quantization/deep_gemm_wrapper/compile_utils.py:
  _BaseWarmupExecutor.create(kernel_type,**kwargs)
  _BaseWarmupExecutor.execute(self,m)
  _GroupedContWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _GroupedContWarmupExecutor.execute(self,m)
  _GroupedMaskedWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _GroupedMaskedWarmupExecutor.execute(self,m)
  _NormalWarmupExecutor.__init__(self,max_m,n,k,num_groups)
  _NormalWarmupExecutor.execute(self,m)
  _compile_deep_gemm_one_type_all(kernel_type,n,k,num_groups,m_list)
  _empty_block_fp8(size)
  _empty_token_fp8(size)
  _maybe_compile_deep_gemm_one_type_all(kernel_type,n,k,num_groups)
  deep_gemm_execution_hook(m,n,k,num_groups,kernel_type)
  update_deep_gemm_config(gpu_id,server_args)
layers/quantization/deep_gemm_wrapper/configurer.py:
  _compute_enable_deep_gemm()
  _is_blackwell_arch()
layers/quantization/deep_gemm_wrapper/entrypoint.py:
  configure_deep_gemm_num_sms(num_sms)
  gemm_nt_f8f8bf16(lhs,rhs,out)
  grouped_gemm_nt_f8f8bf16_contig(lhs,rhs,out,m_indices)
  grouped_gemm_nt_f8f8bf16_masked(lhs,rhs,out,masked_m,expected_m)
  update_deep_gemm_config(gpu_id,server_args)
layers/quantization/fp8.py:
  Fp8Config.__init__(self,is_checkpoint_fp8_serialized,activation_scheme,ignored_layers,weight_block_size)
  Fp8Config.from_config(cls,config)
  Fp8Config.get_config_filenames(cls)
  Fp8Config.get_min_capability(cls)
  Fp8Config.get_name(cls)
  Fp8Config.get_quant_method(self,layer,prefix)
  Fp8Config.get_scaled_act_names(self)
  Fp8Config.get_supported_act_dtypes(cls)
  Fp8KVCacheMethod.__init__(self,quant_config)
  Fp8LinearMethod.__init__(self,quant_config)
  Fp8LinearMethod.apply(self,layer,x,bias)
  Fp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  Fp8LinearMethod.process_weights_after_loading(self,layer)
  Fp8MoEMethod.__init__(self,quant_config)
  Fp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  Fp8MoEMethod.apply_with_router_logits(self,layer,x,topk_output,moe_runner_config)
  Fp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  Fp8MoEMethod.maybe_apply_hip_fused_experts(self,layer,x,topk_output,activation,no_combine)
  Fp8MoEMethod.process_weights_after_loading(self,layer)
  Fp8MoEMethod.process_weights_hip_int4(self,layer)
  Fp8MoEMethod.process_weights_hip_scale_padding(self,layer)
  dummy_func(*args,**kwargs)
  get_tile_tokens_dim(num_tokens,top_k,num_experts)
layers/quantization/fp8_kernel.py:
  _per_group_transpose(data_ptr,trans_data_ptr,expert_offsets,k,M_ALIGNMENT,BLOCK_SIZE_M,BLOCK_SIZE_K)
  _per_tensor_quant_mla_fp8_stage1(x_ptr,x_s_ptr,head_size,x_stride_h,x_stride_s,eps,fp8_max,BLOCK_SIZE)
  _per_tensor_quant_mla_fp8_stage2(x_ptr,x_s_ptr,x_q_ptr,num_seq,head_size,x_stride_h,x_stride_s,fp8_min,fp8_max,BLOCK_SIZE)
  _per_token_group_quant_8bit(y_ptr,y_q_ptr,y_s_ptr,y_stride,N,eps,bit8_min,bit8_max,BLOCK)
  _per_token_group_quant_8bit_colmajor(y_ptr,y_q_ptr,y_s_ptr,group_size,y_num_columns,y_s_col_stride,eps,bit8_min,bit8_max,BLOCK,SCALE_UE8M0)
  _per_token_group_quant_8bit_fuse_silu_and_mul(x,group_size,dst_dtype,column_major_scales,scale_tma_aligned,scale_ue8m0,masked_m)
  _per_token_group_quant_8bit_raw(x,group_size,eps,dtype,column_major_scales,scale_tma_aligned,scale_ue8m0)
  _per_token_group_quant_fp8_hopper_moe_mn_major(a,expert_offsets,problem_sizes,a_fp8,sfa,K,BLOCK_K,M_ALIGNMENT,BLOCK_M)
  _per_token_group_quant_mla_deep_gemm_masked_fp8(y_ptr,y_q_ptr,y_s_ptr,masked_m_ptr,group_size,y_stride_b,y_stride_t,y_q_stride_b,y_q_stride_t,y_s_stride_b,y_s_stride_g,eps,fp8_min,fp8_max,NUM_GROUP,BLOCK)
  _static_quant_fp8(y_ptr,y_q_ptr,y_s_ptr,y_s_repeat_ptr,y_stride,N,fp8_min,fp8_max,BLOCK,REPEAT_SCALE)
  _w8a8_block_fp8_matmul(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  _w8a8_block_fp8_matmul_unrolledx4(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  create_per_token_group_quant_fp8_output_scale(x_shape,device,group_size,column_major_scales,scale_tma_aligned,scale_ue8m0)
  deep_gemm_fp8_fp8_bf16_nt(A,As,B,Bs,C)
  deep_gemm_fp8_fp8_bf16_nt_fake(A,As,B,Bs,C)
  get_w8a8_block_fp8_configs(N,K,block_n,block_k)
  grid(META)
  is_fp8_fnuz()
  is_weak_contiguous(x)
  per_group_transpose(a,expert_offsets,M_ALIGNMENT)
  per_tensor_quant_mla_fp8(x,x_s_out,eps)
  per_token_group_quant_8bit(x,group_size,dst_dtype,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  per_token_group_quant_fp8_hopper_moe_mn_major(A,expert_offsets,problem_sizes,group_size,expert_tokens_alignment)
  per_token_group_quant_mla_deep_gemm_masked_fp8(x,group_size,eps,dtype)
  prepare_block_fp8_matmul_inputs(A,B,As,Bs,block_size,output_dtype)
  scaled_fp8_quant(input,scale,num_token_padding,use_per_token_if_dynamic)
  scaled_fp8_quant(input,scale,num_token_padding,use_per_token_if_dynamic)
  scaled_mm_kernel(a_ptr,b_ptr,scale_a_ptr,scale_b_ptr,c_ptr,bias_ptr,M,N,K,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,ACCUMULATOR_DTYPE,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,BLOCK_SIZE_SCALE_A,BLOCK_SIZE_SCALE_B)
  select_w8a8_block_fp8_matmul_kernel(M,N,META)
  select_w8a8_block_fp8_matmul_kernel(M,N,META)
  sglang_per_token_group_quant_8bit(x,group_size,dst_dtype,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  sglang_per_token_group_quant_fp8(x,group_size,eps,column_major_scales,scale_tma_aligned,scale_ue8m0,fuse_silu_and_mul,masked_m)
  sglang_per_token_quant_fp8(x,dtype)
  static_quant_fp8(x,x_s,repeat_scale)
  triton_scaled_mm(input,weight,scale_a,scale_b,out_dtype,bias,block_size_m,block_size_n,block_size_k,use_heuristic)
  use_w8a8_block_fp8_matmul_unrolledx4(M,N,META)
  w8a8_block_fp8_matmul(A,B,As,Bs,block_size,output_dtype)
  w8a8_block_fp8_matmul_deepgemm(A,B,As,Bs,block_size,output_dtype)
  w8a8_block_fp8_matmul_triton(A,B,As,Bs,block_size,output_dtype)
layers/quantization/fp8_utils.py:
  _apply_fallback_scaled_mm(qinput,weight,x_scale,weight_scale,input_2d_shape,output_shape,bias,input_dtype)
  _check_ue8m0(name,x)
  _process_scaled_mm_output(output,input_2d_shape,output_shape)
  _requant_weight_ue8m0(weight,weight_scale_inv,weight_block_size)
  _transform_scale(sf,mn)
  aiter_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  apply_fp8_linear(input,weight,weight_scale,input_scale,input_scale_ub,bias,cutlass_fp8_supported,use_per_token_if_dynamic,pad_output,compressed_tensor_quant)
  block_quant_dequant(x_q_block,x_s,block_size,dtype)
  block_quant_to_tensor_quant(x_q_block,x_s,block_size)
  can_auto_enable_marlin_fp8()
  ceil_to_ue8m0(x)
  channel_quant_to_tensor_quant(x_q_channel,x_s)
  cutlass_block_fp8_supported()
  cutlass_fp8_supported()
  cutlass_w8a8_block_fp8_linear_with_fallback(input,weight,block_size,weight_scale,input_scale,bias)
  deepgemm_w8a8_block_fp8_linear_with_fallback(input,weight,block_size,weight_scale,input_scale,bias)
  dequant_mxfp4(w_block,w_scale,out_dtype)
  dispatch_w8a8_block_fp8_linear()
  flashinfer_gemm_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  input_to_float8(x,dtype)
  normalize_e4m3fn_to_e4m3fnuz(weight,weight_scale,input_scale)
  per_block_cast_to_fp8(x)
  requant_weight_ue8m0_inplace(weight,weight_scale_inv,weight_block_size)
  triton_w8a8_block_fp8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  use_rowwise_torch_scaled_mm()
layers/quantization/fpgemm_fp8.py:
  FBGEMMFp8Config.__init__(self,ignore_list,input_scale_ub)
  FBGEMMFp8Config.from_config(cls,config)
  FBGEMMFp8Config.get_config_filenames(cls)
  FBGEMMFp8Config.get_min_capability(cls)
  FBGEMMFp8Config.get_name(cls)
  FBGEMMFp8Config.get_quant_method(self,layer,prefix)
  FBGEMMFp8Config.get_scaled_act_names(self)
  FBGEMMFp8Config.get_supported_act_dtypes(cls)
  FBGEMMFp8LinearMethod.__init__(self,quant_config)
  FBGEMMFp8LinearMethod.apply(self,layer,x,bias)
  FBGEMMFp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  FBGEMMFp8LinearMethod.process_weights_after_loading(self,layer)
layers/quantization/gptq.py:
  GPTQConfig.__init__(self,weight_bits,group_size,desc_act,lm_head_quantized,dynamic)
  GPTQConfig.__repr__(self)
  GPTQConfig.from_config(cls,config)
  GPTQConfig.get_config_filenames(cls)
  GPTQConfig.get_min_capability(cls)
  GPTQConfig.get_name(cls)
  GPTQConfig.get_quant_method(self,layer,prefix)
  GPTQConfig.get_scaled_act_names(self)
  GPTQConfig.get_supported_act_dtypes(cls)
  GPTQLinearMethod.__init__(self,quant_config)
  GPTQLinearMethod.apply(self,layer,x,bias)
  GPTQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  GPTQLinearMethod.process_weights_after_loading(self,layer)
  GPTQMarlinConfig.__init__(self,weight_bits,group_size,desc_act,is_sym,lm_head_quantized,dynamic,full_config)
  GPTQMarlinConfig.__repr__(self)
  GPTQMarlinConfig.from_config(cls,config)
  GPTQMarlinConfig.get_config_filenames(cls)
  GPTQMarlinConfig.get_min_capability(cls)
  GPTQMarlinConfig.get_name(cls)
  GPTQMarlinConfig.get_quant_method(self,layer,prefix)
  GPTQMarlinConfig.get_scaled_act_names(self)
  GPTQMarlinConfig.get_supported_act_dtypes(cls)
  GPTQMarlinConfig.is_gptq_marlin_compatible(cls,quant_config)
  GPTQMarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  GPTQMarlinLinearMethod.__init__(self,quant_config)
  GPTQMarlinLinearMethod._get_weight_params(layer)
  GPTQMarlinLinearMethod._transform_param(layer,name,fn)
  GPTQMarlinLinearMethod.apply(self,layer,x,bias)
  GPTQMarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  GPTQMarlinLinearMethod.process_weights_after_loading(self,layer)
  GPTQMarlinLinearMethod.transform_w_q(x)
  GPTQMarlinLinearMethod.transform_w_s(x)
  GPTQMarlinMoEMethod.__init__(self,quant_config)
  GPTQMarlinMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  GPTQMarlinMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  GPTQMarlinMoEMethod.process_weights_after_loading(self,layer)
  check_marlin_format(hf_quant_cfg)
  gptq_marlin_moe_repack(b_q_weight,perm,size_k,size_n,num_bits)
layers/quantization/int8_kernel.py:
  _per_token_group_quant_int8(y_ptr,y_q_ptr,y_s_ptr,y_stride,N,eps,int8_min,int8_max,BLOCK)
  _per_token_quant_int8(x_ptr,xq_ptr,scale_ptr,x_sum_ptr,stride_x,stride_xq,N,CAL_SUM,BLOCK)
  _w8a8_block_int8_matmul(A,B,C,As,Bs,M,N,K,group_n,group_k,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,stride_As_m,stride_As_k,stride_Bs_k,stride_Bs_n,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,GROUP_SIZE_M)
  get_w8a8_block_int8_configs(N,K,block_n,block_k)
  grid(META)
  per_token_group_quant_int8(x,group_size,eps,dtype)
  per_token_quant_int8(x,scale_dtype,cal_sum)
  sglang_per_token_group_quant_int8(x,group_size,eps,dtype)
  w8a8_block_int8_matmul(A,B,As,Bs,block_size,output_dtype)
layers/quantization/int8_utils.py:
  apply_w8a8_block_int8_linear(input,weight,block_size,weight_scale,input_scale,bias)
  block_dequant(x_q_block,x_s,block_size)
  input_to_int8(x,dtype)
layers/quantization/kv_cache.py:
  BaseKVCacheMethod.__init__(self,quant_config)
  BaseKVCacheMethod.apply(self,layer)
  BaseKVCacheMethod.create_weights(self,layer)
  BaseKVCacheMethod.process_weights_after_loading(self,layer)
layers/quantization/marlin_utils.py:
  MarlinConfig.__init__(self,group_size,lm_head_quantized)
  MarlinConfig.__repr__(self)
  MarlinConfig.from_config(cls,config)
  MarlinConfig.get_config_filenames(cls)
  MarlinConfig.get_min_capability(cls)
  MarlinConfig.get_name(cls)
  MarlinConfig.get_quant_method(self,layer,prefix)
  MarlinConfig.get_supported_act_dtypes(cls)
  MarlinConfig.override_quantization_method(cls,hf_quant_cfg,user_quant)
  MarlinLinearMethod.__init__(self,quant_config)
  MarlinLinearMethod.apply(self,layer,x,bias)
  MarlinLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  MarlinLinearMethod.process_weights_after_loading(self,layer)
  _check_marlin_supported(quant_type,group_size,has_zp,device_capability)
  apply_awq_marlin_linear(input,weight,weight_scale,weight_zp,g_idx,g_idx_sort_indices,workspace,quant_type,output_size_per_partition,input_size_per_partition,bias,use_fp32_reduce)
  apply_gptq_marlin_linear(input,weight,weight_scale,weight_zp,g_idx,g_idx_sort_indices,workspace,wtype,output_size_per_partition,input_size_per_partition,is_k_full,bias,use_fp32_reduce)
  awq_to_marlin_zero_points(q_zp_packed,size_k,size_n,num_bits)
  check_marlin_supported(quant_type,group_size,has_zp,device_capability)
  check_marlin_supports_layer(layer,group_size)
  check_marlin_supports_shape(output_size_per_partition,input_size_per_partition,input_size,group_size)
  check_moe_marlin_supports_layer(layer,group_size)
  get_scale_perms()
  marlin_is_k_full(act_order,is_row_parallel)
  marlin_make_empty_g_idx(device)
  marlin_make_empty_zp(device)
  marlin_make_workspace(device,max_blocks_per_sm)
  marlin_moe_permute_scales(s,size_k,size_n,group_size)
  marlin_permute_bias(s)
  marlin_permute_scales(s,size_k,size_n,group_size)
  marlin_repeat_scales_on_all_ranks(act_order,group_size,is_row_parallel)
  marlin_sort_g_idx(g_idx)
  marlin_zero_points(zp,size_k,size_n,num_bits)
  maybe_warn_marlin_atomic_add(device,dtype)
  maybe_warn_marlin_atomic_add_env()
  moe_awq_to_marlin_zero_points(q_zp_packed,size_k,size_n,num_bits)
  query_marlin_supported_quant_types(has_zp,include_fp_type,device_capability)
  should_use_atomic_add_reduce(m,n,k,device,dtype)
  verify_marlin_supported(quant_type,group_size,has_zp)
  verify_marlin_supports_shape(output_size_per_partition,input_size_per_partition,input_size,group_size)
layers/quantization/marlin_utils_fp8.py:
  apply_fp8_marlin_linear(input,weight,weight_scale,workspace,size_n,size_k,bias,use_fp32_reduce)
  fp8_fused_exponent_bias_into_scales(scales)
  marlin_quant_fp8_torch(weight,group_size)
  pack_fp8_to_int32(fp8_tensor,size_k_first)
  prepare_fp8_layer_for_marlin(layer,size_k_first)
  prepare_moe_fp8_layer_for_marlin(layer,size_k_first)
layers/quantization/modelopt_quant.py:
  ModelOptFp4Config.__init__(self,is_checkpoint_nvfp4_serialized,kv_cache_quant_algo,group_size,exclude_modules)
  ModelOptFp4Config.from_config(cls,config)
  ModelOptFp4Config.get_config_filenames(cls)
  ModelOptFp4Config.get_min_capability(cls)
  ModelOptFp4Config.get_name(cls)
  ModelOptFp4Config.get_quant_method(self,layer,prefix)
  ModelOptFp4Config.get_scaled_act_names(self)
  ModelOptFp4Config.get_supported_act_dtypes(cls)
  ModelOptFp4Config.is_layer_excluded(self,prefix,exclude_modules)
  ModelOptFp4LinearMethod.__init__(self,quant_config)
  ModelOptFp4LinearMethod.apply(self,layer,x,bias)
  ModelOptFp4LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  ModelOptFp4LinearMethod.process_weights_after_loading(self,layer)
  ModelOptFp8Config.__init__(self,is_checkpoint_fp8_serialized,kv_cache_quant_method,exclude_modules)
  ModelOptFp8Config.from_config(cls,config)
  ModelOptFp8Config.get_config_filenames(cls)
  ModelOptFp8Config.get_min_capability(cls)
  ModelOptFp8Config.get_name(cls)
  ModelOptFp8Config.get_quant_method(self,layer,prefix)
  ModelOptFp8Config.get_scaled_act_names(self)
  ModelOptFp8Config.get_supported_act_dtypes(cls)
  ModelOptFp8KVCacheMethod.__init__(self,quant_config)
  ModelOptFp8LinearMethod.__init__(self,quant_config)
  ModelOptFp8LinearMethod.apply(self,layer,x,bias)
  ModelOptFp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,params_dtype,**extra_weight_attrs)
  ModelOptFp8LinearMethod.process_weights_after_loading(self,layer)
  ModelOptFp8MoEMethod.__init__(self,quant_config)
  ModelOptFp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  ModelOptFp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  ModelOptFp8MoEMethod.process_weights_after_loading(self,layer)
  ModelOptNvFp4FusedMoEMethod.__init__(self,quant_config)
  ModelOptNvFp4FusedMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  ModelOptNvFp4FusedMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe(self)
  ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first(self)
  ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel(self,gemm1_weights,gemm2_weights,gemm1_scales_linear_fp4_bytes,gemm2_scales_linear_fp4_bytes,hidden_size,intermediate_size,num_experts)
  ModelOptNvFp4FusedMoEMethod.process_weights_after_loading(self,layer)
  ModelOptNvFp4FusedMoEMethod.swizzle_blockscale(self,scale)
layers/quantization/moe_wna16.py:
  MoeWNA16Config.__init__(self,linear_quant_method,weight_bits,group_size,has_zp,lm_head_quantized,modules_to_not_convert,full_config)
  MoeWNA16Config.from_config(cls,config)
  MoeWNA16Config.get_config_filenames(cls)
  MoeWNA16Config.get_min_capability(cls)
  MoeWNA16Config.get_name(cls)
  MoeWNA16Config.get_quant_method(self,layer,prefix)
  MoeWNA16Config.get_scaled_act_names(self)
  MoeWNA16Config.get_supported_act_dtypes(cls)
  MoeWNA16Config.is_moe_wna16_compatible(cls,quant_config)
  MoeWNA16Config.override_quantization_method(cls,hf_quant_cfg,user_quant)
  MoeWNA16Method.__init__(self,quant_config)
  MoeWNA16Method.apply(self,layer,x,topk_output,moe_runner_config)
  MoeWNA16Method.convert_awq_tensor(tensor,tensor_type)
  MoeWNA16Method.convert_gptq_int4_qzeros(tensor)
  MoeWNA16Method.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  MoeWNA16Method.get_weight_loader(layer,weight_loader)
  MoeWNA16Method.moe_wna16_weight_loader(param,loaded_weight,weight_name,shard_id,expert_id)
  get_weight_perm(num_bits)
  is_layer_skipped_quant(prefix,modules_to_not_convert)
layers/quantization/mxfp4.py:
  Mxfp4Config.__init__(self,ignored_layers,is_checkpoint_mxfp4_serialized)
  Mxfp4Config.from_config(cls,config)
  Mxfp4Config.get_config_filenames(cls)
  Mxfp4Config.get_min_capability(cls)
  Mxfp4Config.get_name(cls)
  Mxfp4Config.get_quant_method(self,layer,prefix)
  Mxfp4Config.get_scaled_act_names(self)
  Mxfp4Config.get_supported_act_dtypes(cls)
  Mxfp4Config.is_static_cfg(self)
  Mxfp4DynamicQuantMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  Mxfp4DynamicQuantMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  Mxfp4DynamicQuantMoEMethod.mxfp4_quantize(self,w)
  Mxfp4DynamicQuantMoEMethod.process_weights_after_loading(self,layer)
  Mxfp4MoEMethod.__init__(self,prefix)
  Mxfp4MoEMethod._get_tile_tokens_dim(self,x,top_k)
  Mxfp4MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  Mxfp4MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,with_bias,**extra_weight_attrs)
  Mxfp4MoEMethod.process_weights_after_loading(self,layer)
  Mxfp4MoEMethod.swap_every_two_rows(x,axis)
  _dequant_mxfp4(x,scale,float_dtype)
  _dequant_mxfp4_fake(x,scale,float_dtype)
  _quant_dequant_mxfp4(x,scale_calculation_mode)
  _quant_dequant_mxfp4_fake(x,scale_calculation_mode)
  _swizzle_mxfp4(quant_tensor,scale,num_warps)
layers/quantization/mxfp4_tensor.py:
  MXFP4QuantizeUtil.cast_fp4(x)
  MXFP4QuantizeUtil.dequantize(cls,quantized_data,dtype,scale,block_sizes)
  MXFP4QuantizeUtil.fuse_uint4_to_uint8(x)
  MXFP4QuantizeUtil.quantize(cls,input,block_size)
  MXFP4QuantizeUtil.unfuse_uint8_to_uint4(x)
layers/quantization/petit.py:
  PetitNvFp4Config.__init__(self,is_checkpoint_nvfp4_serialized,kv_cache_quant_algo,group_size,exclude_modules)
  PetitNvFp4Config.from_config(cls,config)
  PetitNvFp4Config.get_config_filenames(cls)
  PetitNvFp4Config.get_min_capability(cls)
  PetitNvFp4Config.get_name(cls)
  PetitNvFp4Config.get_quant_method(self,layer,prefix)
  PetitNvFp4Config.get_scaled_act_names(self)
  PetitNvFp4Config.get_supported_act_dtypes(cls)
  PetitNvFp4Config.is_layer_excluded(self,prefix,exclude_modules)
  PetitNvFp4Config.is_petit_nvfp4_compatible(cls,quant_config)
  PetitNvFp4Config.override_quantization_method(cls,hf_quant_cfg,user_quant)
  PetitNvFp4LinearMethod.__init__(self,quant_config)
  PetitNvFp4LinearMethod.apply(self,layer,x,bias)
  PetitNvFp4LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  PetitNvFp4LinearMethod.process_weights_after_loading(self,layer)
layers/quantization/petit_utils.py:
  _check_petit_nvfp4_supported(quant_method,group_size)
  _check_petit_nvfp4_supported(quant_method,group_size)
  apply_petit_nvfp4_linear(input,weight,weight_scale,weight_scale_2,size_n,size_k,bias)
  apply_petit_nvfp4_linear(input,weight,weight_scale,weight_scale_2,size_n,size_k,bias)
  prepare_nvfp4_layer_for_petit(layer)
  prepare_nvfp4_layer_for_petit(layer)
  verify_petit_nvfp4_supported(quant_method,group_size)
layers/quantization/qoq.py:
  QoQConfig.__init__(self,weight_bits,group_size)
  QoQConfig.__repr__(self)
  QoQConfig.from_config(cls,config)
  QoQConfig.get_config_filenames(cls)
  QoQConfig.get_min_capability(cls)
  QoQConfig.get_name(cls)
  QoQConfig.get_quant_method(self,layer,prefix)
  QoQConfig.get_scaled_act_names(self)
  QoQConfig.get_supported_act_dtypes(cls)
  QoQLinearMethod.__init__(self,quant_config)
  QoQLinearMethod.apply(self,layer,x,bias)
  QoQLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  QoQLinearMethod.process_weights_after_loading(self,layer)
layers/quantization/quark/quark.py:
  QuarkConfig.__init__(self,quant_config,kv_cache_group,kv_cache_config,pack_method)
  QuarkConfig._check_scheme_supported(self,min_capability,error)
  QuarkConfig._find_matched_config(self,layer_name,module)
  QuarkConfig._get_scheme_from_config(self,config)
  QuarkConfig._is_mx_fp4(self,weight_quant,input_quant)
  QuarkConfig.from_config(cls,config)
  QuarkConfig.get_config_filenames(cls)
  QuarkConfig.get_linear_method(self)
  QuarkConfig.get_min_capability(cls)
  QuarkConfig.get_name(self)
  QuarkConfig.get_quant_method(self,layer,prefix)
  QuarkConfig.get_scaled_act_names(self)
  QuarkConfig.get_scheme(self,layer,layer_name)
  QuarkConfig.get_supported_act_dtypes(cls)
  QuarkKVCacheMethod.__init__(self,quant_config)
  QuarkKVCacheMethod.validate_kv_cache_config(kv_cache_config)
  QuarkLinearMethod.__init__(self,quantization_config)
  QuarkLinearMethod.apply(self,layer,x,bias)
  QuarkLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  QuarkLinearMethod.process_weights_after_loading(self,layer)
layers/quantization/quark/quark_moe.py:
  QuarkMoEMethod.__new__(cls,*args,**kwargs)
  QuarkMoEMethod.get_moe_method(quant_config,module,layer_name)
  QuarkW4A4MXFp4MoEMethod.__init__(self,weight_config,input_config)
  QuarkW4A4MXFp4MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  QuarkW4A4MXFp4MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size_per_partition,params_dtype,**extra_weight_attrs)
  QuarkW4A4MXFp4MoEMethod.process_weights_after_loading(self,layer)
layers/quantization/quark/schemes/quark_scheme.py:
  QuarkScheme.apply_weights(self,layer,x,bias)
  QuarkScheme.create_weights(self,*args,**kwargs)
  QuarkScheme.get_min_capability(cls)
  QuarkScheme.process_weights_after_loading(self,layer)
layers/quantization/quark/schemes/quark_w4a4_mxfp4.py:
  QuarkW4A4MXFP4.__init__(self,weight_quant_spec,input_quant_spec)
  QuarkW4A4MXFP4.apply_weights(self,layer,x,bias)
  QuarkW4A4MXFP4.create_weights(self,layer,output_partition_sizes,input_size_per_partition,params_dtype,weight_loader,**kwargs)
  QuarkW4A4MXFP4.get_min_capability(cls)
  QuarkW4A4MXFP4.process_weights_after_loading(self,layer)
layers/quantization/quark/utils.py:
  _is_equal_or_regex_match(value,target,check_contains)
  check_equal_or_regex_match(layer_name,targets)
  deep_compare(dict1,dict2)
  should_ignore_layer(layer_name,ignore,fused_mapping)
layers/quantization/unquant.py:
  UnquantizedEmbeddingMethod.apply(self,layer,x,bias)
  UnquantizedEmbeddingMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  UnquantizedEmbeddingMethod.embedding(self,layer,input_)
  UnquantizedFusedMoEMethod.__init__(self,use_triton_kernels)
  UnquantizedFusedMoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,with_bias,**extra_weight_attrs)
  UnquantizedFusedMoEMethod.forward_cpu(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_cuda(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_npu(self,layer,x,topk_output,moe_runner_config)
  UnquantizedFusedMoEMethod.forward_tpu(self,*args,**kwargs)
  UnquantizedFusedMoEMethod.process_weights_after_loading(self,layer)
  UnquantizedLinearMethod.apply(self,layer,x,bias)
  UnquantizedLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  UnquantizedLinearMethod.process_weights_after_loading(self,layer)
layers/quantization/utils.py:
  MockScalarTypes.__getattr__(self,name)
  all_close_1d(x)
  assert_fp8_all_close(a,b)
  convert_to_channelwise(weight_scale,logical_widths)
  get_dynamic_override(config,layer_name,key,default_value)
  get_linear_quant_method(config,layer,prefix,linear_method_cls)
  get_pack_factor(num_bits)
  get_scalar_types()
  gptq_quantize_weights(w,quant_type,group_size,act_order,test_perm)
  is_layer_skipped(prefix,ignored_layers,fused_mapping)
  override_config(config,prefix)
  pack_cols(q_w,num_bits,size_k,size_n)
  pack_rows(q_w,num_bits,size_k,size_n)
  per_tensor_dequantize(tensor,inv_scale)
  permute_rows(q_w,w_ref,group_size,test_perm)
  quantize_weights(w,quant_type,group_size,zero_points,ref_zero_points_after_scales)
  replace_parameter(mod,name,new)
  requantize_with_max_scale(weight,weight_scale,logical_widths)
  reshape_w(w)
  sort_weights(q_w,g_idx)
  unpack_cols(packed_q_w,num_bits,size_k,size_n)
  update_tensor_inplace(old,new)
layers/quantization/w4afp8.py:
  W4AFp8Config.__init__(self,is_checkpoint_fp8_serialized,is_checkpoint_w4afp8_serialized,linear_activation_scheme,moe_activation_scheme,ignored_layers,weight_block_size,group_size)
  W4AFp8Config.from_config(cls,config)
  W4AFp8Config.get_config_filenames(cls)
  W4AFp8Config.get_min_capability(cls)
  W4AFp8Config.get_name(cls)
  W4AFp8Config.get_quant_method(self,layer,prefix)
  W4AFp8Config.get_scaled_act_names(self)
  W4AFp8Config.get_supported_act_dtypes(cls)
  W4AFp8MoEMethod.__init__(self,quant_config)
  W4AFp8MoEMethod._interleave_scales(self,scales)
  W4AFp8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  W4AFp8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W4AFp8MoEMethod.process_weights_after_loading(self,layer)
layers/quantization/w8a8_fp8.py:
  W8A8FP8MoEMethod.__init__(self,quant_config)
  W8A8FP8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  W8A8FP8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W8A8FP8MoEMethod.process_weights_after_loading(self,layer)
  W8A8Fp8Config.__init__(self,is_checkpoint_fp8_serialized)
  W8A8Fp8Config.from_config(cls,config)
  W8A8Fp8Config.get_config_filenames(cls)
  W8A8Fp8Config.get_min_capability(cls)
  W8A8Fp8Config.get_name(self)
  W8A8Fp8Config.get_quant_method(self,layer,prefix)
  W8A8Fp8Config.get_scaled_act_names(self)
  W8A8Fp8Config.get_supported_act_dtypes(cls)
  W8A8Fp8LinearMethod.__init__(self,quantization_config)
  W8A8Fp8LinearMethod.apply(self,layer,x,bias)
  W8A8Fp8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  W8A8Fp8LinearMethod.process_weights_after_loading(self,layer)
layers/quantization/w8a8_int8.py:
  NPU_W8A8DynamicLinearMethod.__init__(self,quantization_config)
  NPU_W8A8DynamicLinearMethod.apply(self,layer,x,bias)
  NPU_W8A8DynamicLinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8DynamicLinearMethod.process_weights_after_loading(self,layer)
  NPU_W8A8DynamicLinearMethodImpl.__init__(self)
  NPU_W8A8DynamicLinearMethodImpl.apply(layer,x,bias,tp_rank)
  NPU_W8A8DynamicLinearMethodImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethod.__init__(self,quantization_config)
  NPU_W8A8LinearMethod.apply(self,layer,x,bias)
  NPU_W8A8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8LinearMethod.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethodImpl.__init__(self)
  NPU_W8A8LinearMethodImpl.apply(layer,x,bias)
  NPU_W8A8LinearMethodImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8LinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8LinearMethodImpl.process_weights_after_loading(self,layer)
  NPU_W8A8LinearMethodMTImpl.__init__(self)
  NPU_W8A8LinearMethodMTImpl.apply(layer,x,bias)
  NPU_W8A8LinearMethodMTImpl.get_perchannel_param(output_size,params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_weight(input_size,output_size,params_dtype)
  NPU_W8A8LinearMethodMTImpl.process_weights_after_loading(self,layer)
  NPU_W8A8MoEMethod.__init__(self,quantization_config)
  NPU_W8A8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  NPU_W8A8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  NPU_W8A8MoEMethod.process_weights_after_loading(self,layer)
  W8A8Int8Config.__init__(self,quant_config)
  W8A8Int8Config.from_config(cls,config)
  W8A8Int8Config.get_config_filenames(cls)
  W8A8Int8Config.get_min_capability(cls)
  W8A8Int8Config.get_name(self)
  W8A8Int8Config.get_quant_method(self,layer,prefix)
  W8A8Int8Config.get_scaled_act_names(self)
  W8A8Int8Config.get_supported_act_dtypes(cls)
  W8A8Int8Config.is_layer_skipped(self,prefix,fused_mapping)
  W8A8Int8LinearMethod.__init__(self,quantization_config)
  W8A8Int8LinearMethod.apply(self,layer,x,bias)
  W8A8Int8LinearMethod.create_weights(self,layer,input_size_per_partition,output_partition_sizes,input_size,output_size,params_dtype,**extra_weight_attrs)
  W8A8Int8LinearMethod.process_weights_after_loading(self,layer)
  W8A8Int8MoEMethod.__init__(self,quant_config)
  W8A8Int8MoEMethod.apply(self,layer,x,topk_output,moe_runner_config)
  W8A8Int8MoEMethod.create_weights(self,layer,num_experts,hidden_size,intermediate_size,params_dtype,**extra_weight_attrs)
  W8A8Int8MoEMethod.process_weights_after_loading(self,layer)
  _rmsnorm_forward_oot(self,x,residual)
  init(self,hidden_size,**extra_args)
  npu_fused_experts(hidden_states,w13,w13_scale,w2,w2_scale,topk_weights,topk_ids,top_k)
  npu_wrapper_rmsnorm_forward(func)
  npu_wrapper_rmsnorm_init(func)
layers/radix_attention.py:
  RadixAttention.__init__(self,num_heads,head_dim,scaling,num_kv_heads,layer_id,logit_cap,v_head_dim,sliding_window_size,is_cross_attention,pos_encoding_mode,logit_capping_method,quant_config,attn_type,use_irope,prefix)
  RadixAttention.forward(self,q,k,v,forward_batch,save_kv_cache,**kwargs)
layers/rotary_embedding.py:
  DeepseekScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype,extrapolation_factor,attn_factor,beta_fast,beta_slow,mscale,mscale_all_dim,device)
  DeepseekScalingRotaryEmbedding._compute_cos_sin_cache(self)
  DeepseekScalingRotaryEmbedding._compute_inv_freq(self,scaling_factor)
  DeepseekScalingRotaryEmbedding.forward_cpu(self,positions,query,key,offsets)
  DeepseekScalingRotaryEmbedding.forward_native(self,positions,query,key,offsets)
  DeepseekScalingRotaryEmbedding.forward_npu(self,positions,query,key,offsets)
  DualChunkRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,chunk_size,local_size)
  DualChunkRotaryEmbedding._apply_rotary_embedding(self,cos_sin,hidden_rot,hidden_pass)
  DualChunkRotaryEmbedding._compute_cos_sin_cache(self)
  DualChunkRotaryEmbedding._compute_inv_freq(self,base)
  DualChunkRotaryEmbedding.extra_repr(self)
  DualChunkRotaryEmbedding.forward(self,positions,query,key,offsets)
  DynamicNTKAlphaRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_alpha,dtype)
  DynamicNTKAlphaRotaryEmbedding._compute_cos_sin_cache(self)
  DynamicNTKScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype)
  DynamicNTKScalingRotaryEmbedding._compute_cos_sin_cache(self)
  LinearScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factors,dtype)
  LinearScalingRotaryEmbedding._compute_cos_sin_cache(self)
  LinearScalingRotaryEmbedding.scaling_factor_to_offset(self)
  Llama3RotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,scaling_factor,low_freq_factor,high_freq_factor,orig_max_position)
  Llama3RotaryEmbedding._compute_inv_freq(self,base)
  Llama4VisionRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype)
  Llama4VisionRotaryEmbedding._compute_cos_sin_cache(self)
  Llama4VisionRotaryEmbedding._compute_inv_freq(self,base)
  Llama4VisionRotaryEmbedding.forward(self,query,key)
  MRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype,mrope_section)
  MRotaryEmbedding.forward(self,positions,query,key)
  MRotaryEmbedding.get_next_input_positions(mrope_position_delta,context_len,seq_len)
  MRotaryEmbedding.get_rope_index(spatial_merge_size,image_token_id,video_token_id,vision_start_token_id,model_type,tokens_per_second,input_ids,image_grid_thw,video_grid_thw,second_per_grid_ts,**kwargs)
  MRotaryEmbedding.get_rope_index_glm4v(input_ids,hf_config,image_grid_thw,video_grid_thw,attention_mask,**kwargs)
  Phi3LongRoPEScaledRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,original_max_position_embeddings,base,is_neox_style,dtype,short_factor,long_factor,short_mscale,long_mscale)
  Phi3LongRoPEScaledRotaryEmbedding._compute_cos_sin_cache(self,max_position_embeddings,rescale_factors,mscale)
  Phi3LongRoPEScaledRotaryEmbedding._compute_inv_freq(self,rescale_factors)
  Phi3LongRoPEScaledRotaryEmbedding.forward(self,positions,query,key,offsets)
  RotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,dtype)
  RotaryEmbedding._compute_cos_sin_cache(self)
  RotaryEmbedding._compute_inv_freq(self,base)
  RotaryEmbedding.extra_repr(self)
  RotaryEmbedding.forward_cpu(self,positions,query,key,offsets)
  RotaryEmbedding.forward_cuda(self,positions,query,key,offsets,fused_set_kv_buffer_arg)
  RotaryEmbedding.forward_native(self,positions,query,key,offsets)
  RotaryEmbedding.forward_npu(self,positions,query,key,offsets)
  YaRNScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype,extrapolation_factor,attn_factor,beta_fast,beta_slow)
  YaRNScalingRotaryEmbedding._compute_cos_sin_cache(self)
  YaRNScalingRotaryEmbedding._compute_inv_freq(self,scaling_factor)
  _apply_rotary_emb(x,cos,sin,is_neox_style)
  _rotate_gptj(x)
  _rotate_neox(x)
  _yarn_find_correction_dim(num_rotations,dim,base,max_position_embeddings)
  _yarn_find_correction_range(low_rot,high_rot,dim,base,max_position_embeddings)
  _yarn_get_mscale(scale)
  _yarn_linear_ramp_mask(low,high,dim,dtype,device)
  apply_rotary_pos_emb_native(q,k,cos,sin,unsqueeze_dim)
  apply_rotary_pos_emb_npu(q,k,cos,sin,unsqueeze_dim)
  get_rope(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,dual_chunk_attention_config)
  get_rope_cpu(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,device)
  get_rope_wrapper(head_size,rotary_dim,max_position,base,is_neox_style,rope_scaling,dtype,partial_rotary_factor,device)
  rotate_half(x)
  yarn_get_mscale(scale,mscale)
layers/sampler.py:
  Sampler.__init__(self)
  Sampler.forward(self,logits_output,sampling_info,return_logprob,top_logprobs_nums,token_ids_logprobs)
  apply_custom_logit_processor(logits,sampling_batch_info,num_tokens_in_batch)
  get_token_ids_logprobs(logprobs,token_ids_logprobs)
  get_top_logprobs(logprobs,top_logprobs_nums)
  sampling_from_probs_torch(probs)
  top_k_top_p_min_p_sampling_from_probs_torch(probs,top_ks,top_ps,min_ps,need_min_p_sampling)
  top_p_normalize_probs_torch(probs,top_ps)
layers/torchao_utils.py:
  apply_torchao_config_to_model(model,torchao_config,filter_fn)
  get_gemlite_cache_path()
  proj_filter(module,fqn)
  save_gemlite_cache(print_error)
layers/utils.py:
  PPMissingLayer.__init__(self,*args,**kwargs)
  PPMissingLayer.forward(self,*args,**kwargs)
  get_layer_id(weight_name)
layers/vocab_parallel_embedding.py:
  ParallelLMHead.__init__(self,num_embeddings,embedding_dim,bias,params_dtype,org_num_embeddings,padding_size,quant_config,prefix,use_attn_tp_group,use_presharded_weights)
  ParallelLMHead.forward(self,input_)
  ParallelLMHead.tie_weights(self,embed_tokens)
  VocabParallelEmbedding.__init__(self,num_embeddings,embedding_dim,params_dtype,org_num_embeddings,padding_size,quant_config,prefix,enable_tp,use_attn_tp_group,use_presharded_weights)
  VocabParallelEmbedding._get_indices(cls,vocab_size_padded,org_vocab_size_padded,vocab_size,org_vocab_size,tp_rank,tp_size)
  VocabParallelEmbedding.extra_repr(self)
  VocabParallelEmbedding.forward(self,input_)
  VocabParallelEmbedding.get_sharded_to_full_mapping(self)
  VocabParallelEmbedding.weight_loader(self,param,loaded_weight)
  VocabParallelEmbeddingShardIndices.__post_init__(self)
  VocabParallelEmbeddingShardIndices.num_added_elements(self)
  VocabParallelEmbeddingShardIndices.num_added_elements_padded(self)
  VocabParallelEmbeddingShardIndices.num_added_vocab_padding(self)
  VocabParallelEmbeddingShardIndices.num_elements_padded(self)
  VocabParallelEmbeddingShardIndices.num_org_elements(self)
  VocabParallelEmbeddingShardIndices.num_org_elements_padded(self)
  VocabParallelEmbeddingShardIndices.num_org_vocab_padding(self)
  get_masked_input_and_mask(input_,org_vocab_start_index,org_vocab_end_index,num_org_vocab_padding,added_vocab_start_index,added_vocab_end_index)
  pad_vocab_size(vocab_size,pad_to)
  vocab_range_from_global_vocab_size(global_vocab_size,rank,world_size,offset)
  vocab_range_from_per_partition_vocab_size(per_partition_vocab_size,rank,offset)
lora/backend/base_backend.py:
  BaseLoRABackend.__init__(self,name,batch_info)
  BaseLoRABackend.run_gate_up_lora(self,x,gate_up_lora_a,gate_up_lora_b,*args,**kwargs)
  BaseLoRABackend.run_lora_a_sgemm(self,x,weights,*args,**kwargs)
  BaseLoRABackend.run_lora_b_sgemm(self,x,weights,*args,**kwargs)
  BaseLoRABackend.run_qkv_lora(self,x,qkv_lora_a,qkv_lora_b,*args,**kwargs)
  BaseLoRABackend.set_batch_info(self,batch_info)
  get_backend_from_name(name)
lora/backend/triton_backend.py:
  TritonLoRABackend.__init__(self,name,batch_info)
  TritonLoRABackend.run_gate_up_lora(self,x,gate_up_lora_a,gate_up_lora_b,base_output,*args,**kwargs)
  TritonLoRABackend.run_lora_a_sgemm(self,x,weights,*args,**kwargs)
  TritonLoRABackend.run_lora_b_sgemm(self,x,weights,base_output,*args,**kwargs)
  TritonLoRABackend.run_qkv_lora(self,x,qkv_lora_a,qkv_lora_b,output_offset,max_qkv_out_dim,base_output,*args,**kwargs)
lora/layers.py:
  BaseLayerWithLoRA.__init__(self,base_layer,lora_backend)
  BaseLayerWithLoRA.forward(self,x)
  BaseLayerWithLoRA.set_lora_info(self,*args)
  BaseLayerWithLoRA.slice_lora_a_weights(self,A,tp_rank)
  BaseLayerWithLoRA.slice_lora_b_weights(self,B,tp_rank)
  ColumnParallelLinearWithLoRA.__init__(self,base_layer,lora_backend)
  ColumnParallelLinearWithLoRA.apply_lora(self,base_output,x)
  ColumnParallelLinearWithLoRA.forward(self,input_)
  ColumnParallelLinearWithLoRA.set_lora_info(self,A_buffer,B_buffer)
  ColumnParallelLinearWithLoRA.slice_lora_a_weights(self,A,tp_rank)
  ColumnParallelLinearWithLoRA.slice_lora_b_weights(self,B,tp_rank)
  MergedColumnParallelLinearWithLoRA.__init__(self,base_layer,lora_backend)
  MergedColumnParallelLinearWithLoRA.apply_lora(self,base_output,x)
  MergedColumnParallelLinearWithLoRA.set_lora_info(self,A_buffer,B_buffer)
  MergedColumnParallelLinearWithLoRA.slice_lora_a_weights(self,A,tp_rank)
  MergedColumnParallelLinearWithLoRA.slice_lora_b_weights(self,B,tp_rank)
  QKVParallelLinearWithLoRA.__init__(self,base_layer,lora_backend)
  QKVParallelLinearWithLoRA.apply_lora(self,base_output,x)
  QKVParallelLinearWithLoRA.set_lora_info(self,A_buffer_qkv,B_buffer_qkv)
  QKVParallelLinearWithLoRA.slice_lora_a_weights(self,A,tp_rank)
  QKVParallelLinearWithLoRA.slice_lora_b_weights(self,B,tp_rank)
  RowParallelLinearWithLoRA.__init__(self,base_layer,lora_backend)
  RowParallelLinearWithLoRA.apply_lora(self,base_output,x)
  RowParallelLinearWithLoRA.forward(self,input_,skip_all_reduce)
  RowParallelLinearWithLoRA.set_lora_info(self,A_buffer,B_buffer)
  RowParallelLinearWithLoRA.slice_lora_a_weights(self,A,tp_rank)
  RowParallelLinearWithLoRA.slice_lora_b_weights(self,B,tp_rank)
  VocabParallelEmbeddingWithLoRA.__init__(self,base_layer,lora_backend)
  get_lora_layer(layer,lora_backend)
lora/lora.py:
  LoRAAdapter.__init__(self,uid,config,base_hf_config,load_config,lora_backend)
  LoRAAdapter.initialize_weights(self)
  LoRAAdapter.normalize_gate_up_proj(self,weight_names,weights)
  LoRAAdapter.normalize_qkv_proj(self,weight_names,weights)
  LoRALayer.__init__(self,config,base_hf_config)
lora/lora_config.py:
  LoRAConfig.__init__(self,path)
  LoRAConfig.get_lora_config(self,dummy)
lora/lora_manager.py:
  LoRAManager.__init__(self,base_model,base_hf_config,max_loras_per_batch,load_config,dtype,lora_backend,tp_size,tp_rank,max_lora_rank,target_modules,lora_paths)
  LoRAManager.create_lora_update_result(self,success,error_message)
  LoRAManager.init_cuda_graph_batch_info(self,max_bs_in_cuda_graph)
  LoRAManager.init_lora_adapters(self,lora_paths)
  LoRAManager.init_lora_modules(self)
  LoRAManager.init_lora_shapes(self,max_lora_rank,target_modules)
  LoRAManager.init_memory_pool(self)
  LoRAManager.init_state(self,max_lora_rank,target_modules,lora_paths)
  LoRAManager.load_lora_adapter(self,lora_ref)
  LoRAManager.load_lora_weights(self,lora_ref)
  LoRAManager.prepare_lora_batch(self,forward_batch)
  LoRAManager.set_lora_module(self,module_name,module)
  LoRAManager.transfer_adapter_info(weight_indices_out,lora_ranks_out,scalings_out)
  LoRAManager.unload_lora_adapter(self,lora_ref)
  LoRAManager.update_lora_info(self)
  LoRAManager.validate_lora_batch(self,lora_ids)
  LoRAManager.validate_new_adapter(self,lora_config,lora_ref)
lora/lora_registry.py:
  LoRARef.__post_init__(self)
  LoRARef.__str__(self)
  LoRARegistry.__init__(self,lora_paths)
  LoRARegistry._lookup(name)
  LoRARegistry._register_adapter(self,lora_ref)
  LoRARegistry.num_registered_loras(self)
  async LoRARegistry.acquire(self,lora_name)
  async LoRARegistry.register(self,lora_ref)
  async LoRARegistry.release(self,lora_id)
  async LoRARegistry.unregister(self,lora_name)
  async LoRARegistry.wait_for_unload(self,lora_id)
lora/mem_pool.py:
  EmptySlot.__new__(cls)
  EmptySlot.__repr__(self)
  LoRAMemoryPool.__init__(self,base_hf_config,max_loras_per_batch,dtype,tp_size,tp_rank,max_lora_rank,target_modules,base_model)
  LoRAMemoryPool._can_support(config)
  LoRAMemoryPool.can_support(self,config)
  LoRAMemoryPool.get_available_buffer_slot()
  LoRAMemoryPool.get_buffer_id(self,lora_uid)
  LoRAMemoryPool.get_lora_A_shape(self,module_name,base_model,max_lora_dim)
  LoRAMemoryPool.get_lora_B_shape(self,module_name,base_model,max_lora_dim)
  LoRAMemoryPool.get_tensor(self,target_module,layer_id,lora_type)
  LoRAMemoryPool.init_buffer(buffer,target_modules,get_lora_shape_fn)
  LoRAMemoryPool.init_buffers(self,base_model)
  LoRAMemoryPool.load_lora_weight_tensor(buffer_view,weight)
  LoRAMemoryPool.load_lora_weight_to_buffer(self,uid,buffer_id,lora_adapter,lora_modules)
  LoRAMemoryPool.prepare_lora_batch(self,cur_uids,lora_adapters,lora_modules,lora_refs)
lora/triton_ops/gate_up_lora_b.py:
  _gate_up_lora_b_kernel(x,weights,output,K,output_dim,x_stride_0,x_stride_1,w_stride_0,w_stride_1,w_stride_2,output_stride_0,output_stride_1,seg_lens,seg_indptr,weight_indices,lora_ranks,BLOCK_S,BLOCK_N,BLOCK_K,scalings)
  gate_up_lora_b_fwd(x,gate_up_lora_b,batch_info,output_dim,base_output)
lora/triton_ops/qkv_lora_b.py:
  _qkv_lora_b_kernel(x,weights,output,K,max_qkv_out_dim,x_stride_0,x_stride_1,w_stride_0,w_stride_1,w_stride_2,output_stride_0,output_stride_1,seg_lens,seg_indptr,weight_indices,lora_ranks,n_offs,BLOCK_S,BLOCK_N,BLOCK_K,scalings)
  qkv_lora_b_fwd(x,qkv_lora_b,batch_info,output_offset,max_qkv_out_dim,base_output)
lora/triton_ops/sgemm_lora_a.py:
  _sgemm_lora_a_kernel(x,weights,output,N,K,stack_num,x_stride_0,x_stride_1,w_stride_0,w_stride_1,w_stride_2,output_stride_0,output_stride_1,seg_lens,seg_indptr,weight_indices,lora_ranks,BLOCK_S,BLOCK_N,BLOCK_K)
  sgemm_lora_a_fwd(x,weights,batch_info,stack_num)
lora/triton_ops/sgemm_lora_b.py:
  _sgemm_lora_b_kernel(x,weights,output,N,K,x_stride_0,x_stride_1,w_stride_0,w_stride_1,w_stride_2,output_stride_0,output_stride_1,seg_lens,seg_indptr,weight_indices,lora_ranks,BLOCK_S,BLOCK_N,BLOCK_K,scalings)
  sgemm_lora_b_fwd(x,weights,batch_info,base_output)
lora/utils.py:
  get_hidden_dim(module_name,config,base_model)
  get_layer_id(name)
  get_normalized_target_modules(target_modules)
  get_stacked_multiply(module_name)
  get_target_module_name(full_module_name,target_modules)
managers/cache_controller.py:
  CacheOperation.__init__(self,host_indices,device_indices,node_id,priority)
  CacheOperation.__lt__(self,other)
  CacheOperation.merge(self,other)
  CacheOperation.split(self,factor)
  HiCacheController._3fs_zero_copy_page_get(self,operation,hash_values,host_indices)
  HiCacheController._3fs_zero_copy_page_set(self,hash_values,host_indices)
  HiCacheController.__init__(self,token_to_kv_pool_allocator,mem_pool_host,page_size,tp_group,load_cache_event,write_policy,io_backend,storage_backend,prefetch_threshold,model_name,storage_backend_extra_config)
  HiCacheController._generate_storage_config(self,model_name,storage_backend_extra_config)
  HiCacheController._generic_page_get(self,operation,hash_values,host_indices)
  HiCacheController._generic_page_set(self,hash_values,host_indices)
  HiCacheController._generic_storage_hit_query(self,operation)
  HiCacheController._mooncake_page_get(self,operation,hash_values,host_indices)
  HiCacheController._mooncake_page_set(self,hash_values,host_indices)
  HiCacheController._page_backup(self,operation)
  HiCacheController._page_transfer(self,operation)
  HiCacheController.backup_thread_func(self)
  HiCacheController.evict_device(self,device_indices,host_indices)
  HiCacheController.evict_host(self,host_indices,backup_only)
  HiCacheController.is_mooncake_backend(self)
  HiCacheController.load(self,host_indices,priority,node_id)
  HiCacheController.load_thread_func_layer_by_layer(self)
  HiCacheController.move_indices(self,host_indices,device_indices)
  HiCacheController.prefetch(self,request_id,host_indices,new_input_tokens,last_hash)
  HiCacheController.prefetch_io_aux_func(self)
  HiCacheController.prefetch_rate_limit_check(self)
  HiCacheController.prefetch_thread_func(self)
  HiCacheController.reset(self)
  HiCacheController.terminate_prefetch(self,operation)
  HiCacheController.write(self,device_indices,priority,node_id)
  HiCacheController.write_storage(self,host_indices,token_ids,hash_value)
  HiCacheController.write_thread_func_direct(self)
  LayerDoneCounter.__init__(self,num_layers)
  LayerDoneCounter.increment(self)
  LayerDoneCounter.next_producer(self)
  LayerDoneCounter.reset(self)
  LayerDoneCounter.set_consumer(self,index)
  LayerDoneCounter.update_producer(self)
  LayerDoneCounter.wait_until(self,threshold)
  PrefetchOperation.__init__(self,request_id,host_indices,token_ids,last_hash)
  PrefetchOperation.increment(self,num_tokens)
  PrefetchOperation.is_done(self)
  PrefetchOperation.mark_done(self)
  StorageOperation.__init__(self,host_indices,token_ids,last_hash,hash_value)
  StorageOperation.__lt__(self,other)
  TransferBuffer.__init__(self,stop_event,buffer_count,max_buffer_size)
  TransferBuffer.clear(self)
  TransferBuffer.empty(self)
  TransferBuffer.full(self)
  TransferBuffer.get(self,block,timeout)
  TransferBuffer.put(self,item,block,timeout)
managers/data_parallel_controller.py:
  DataParallelController.__init__(self,server_args,port_args,dp_balance_meta)
  DataParallelController.event_loop(self)
  DataParallelController.get_next_global_balance_id()
  DataParallelController.launch_dp_attention_schedulers(self,server_args,port_args)
  DataParallelController.launch_dp_schedulers(self,server_args,port_args)
  DataParallelController.launch_tensor_parallel_group(self,server_args,port_args,base_gpu_id,dp_rank)
  DataParallelController.launch_tensor_parallel_group_thread(self,server_args,port_args,base_gpu_id,dp_rank,ready_event)
  DataParallelController.minimum_tokens_scheduler(self,req)
  DataParallelController.round_robin_scheduler(self,req)
  DataParallelController.shortest_queue_scheduler(self,input_requests)
  LoadBalanceMethod.from_str(cls,method)
  run_data_parallel_controller_process(server_args,port_args,pipe_writer)
managers/detokenizer_manager.py:
  DetokenizerManager.__init__(self,server_args,port_args)
  DetokenizerManager.event_loop(self)
  DetokenizerManager.handle_batch_embedding_out(self,recv_obj)
  DetokenizerManager.handle_batch_token_id_out(self,recv_obj)
  DetokenizerManager.handle_freeze_gc_req(self,recv_req)
  DetokenizerManager.handle_multimodal_decode_req(self,recv_obj)
  DetokenizerManager.trim_matched_stop(self,output,finished_reason,no_stop_trim)
  LimitedCapacityDict.__init__(self,capacity,*args,**kwargs)
  LimitedCapacityDict.__setitem__(self,key,value)
  run_detokenizer_process(server_args,port_args)
managers/io_struct.py:
  BatchTokenizedEmbeddingReqInput.__getitem__(self,i)
  BatchTokenizedEmbeddingReqInput.__iter__(self)
  BatchTokenizedEmbeddingReqInput.__len__(self)
  BatchTokenizedGenerateReqInput.__getitem__(self,i)
  BatchTokenizedGenerateReqInput.__iter__(self)
  BatchTokenizedGenerateReqInput.__len__(self)
  EmbeddingReqInput.__getitem__(self,i)
  EmbeddingReqInput.contains_mm_input(self)
  EmbeddingReqInput.normalize_batch_and_arguments(self)
  EmbeddingReqInput.regenerate_rid(self)
  GenerateReqInput.__getitem__(self,i)
  GenerateReqInput._determine_batch_size(self)
  GenerateReqInput._expand_inputs(self,num)
  GenerateReqInput._handle_parallel_sampling(self)
  GenerateReqInput._normalize_audio_data(self,num)
  GenerateReqInput._normalize_batch_inputs(self)
  GenerateReqInput._normalize_custom_logit_processor(self,num)
  GenerateReqInput._normalize_image_data(self,num)
  GenerateReqInput._normalize_logprob_params(self,num)
  GenerateReqInput._normalize_lora_paths(self,num)
  GenerateReqInput._normalize_rid(self,num)
  GenerateReqInput._normalize_sampling_params(self,num)
  GenerateReqInput._normalize_single_inputs(self)
  GenerateReqInput._normalize_video_data(self,num)
  GenerateReqInput._validate_inputs(self)
  GenerateReqInput._validate_session_params(self)
  GenerateReqInput.contains_mm_input(self)
  GenerateReqInput.normalize_batch_and_arguments(self)
  GenerateReqInput.normalize_param(param,default_value,param_name)
  GenerateReqInput.regenerate_rid(self)
  LoadLoRAAdapterReqInput.to_ref(self)
  UnloadLoRAAdapterReqInput.to_ref(self)
managers/mm_utils.py:
  MultiModalityDataPaddingPattern.pad_input_tokens(self,input_ids,mm_inputs)
  MultiModalityDataPaddingPatternMultimodalTokens.pad_input_tokens(self,input_ids,mm_inputs)
  MultiModalityDataPaddingPatternTokenPairs.__init__(self,data_token_pairs,data_start_token_ids)
  MultiModalityDataPaddingPatternTokenPairs.pad_input_tokens(self,input_ids,mm_inputs)
  TransportProxyTensor.__getstate__(self)
  TransportProxyTensor.__new__(cls,data,name,fields,transport_mode,*args,**kwargs)
  TransportProxyTensor.__setstate__(self,state)
  TransportProxyTensor.fields(self)
  TransportProxyTensor.name(self)
  TransportProxyTensor.transport_mode(self)
  _adjust_embedding_length(embedding,mask,logger)
  _get_chunked_prefill_embedding(data_embedding_func,embedding_items,items_size,prefix_length,extend_length,items_offset_list)
  _get_multimodal_mask(input_ids,placeholder_tensor)
  _get_precomputed_embedding(items)
  data_hash(data)
  embed_mm_inputs(mm_inputs_list,extend_prefix_lens,extend_seq_lens,input_ids,input_embedding,multimodal_model,data_embedding_func_mapping,placeholder_tokens)
  general_mm_embed_routine(input_ids,forward_batch,language_model,multimodal_model,data_embedding_funcs,placeholder_tokens,**kwargs)
  get_embedding_and_mask(data_embedding_func,embedding_items,placeholder_tensor,input_ids,items_size,prefix_length,extend_length,items_offset_list)
  get_embedding_chunk(embedding,extend_prefix_len,extend_seq_len,items_offset)
  get_embedding_hash(embedding_items)
  get_multimodal_data_bounds(input_ids,pad_values,token_pairs)
  hash_feature(f)
  init_embedding_cache(max_size)
  tensor_hash(tensor_list)
managers/multimodal_processor.py:
  get_mm_processor(hf_config,server_args,processor,transport_mode)
  import_processors()
managers/schedule_batch.py:
  BaseFinishReason.__init__(self,is_error)
  BaseFinishReason.to_json(self)
  FINISH_ABORT.__init__(self,message,status_code,err_type)
  FINISH_ABORT.to_json(self)
  FINISH_LENGTH.__init__(self,length)
  FINISH_LENGTH.to_json(self)
  FINISH_MATCHED_STR.__init__(self,matched)
  FINISH_MATCHED_STR.to_json(self)
  FINISH_MATCHED_TOKEN.__init__(self,matched)
  FINISH_MATCHED_TOKEN.to_json(self)
  Modality.all()
  Modality.from_str(modality_str)
  MultimodalDataItem.__getattr__(self,name)
  MultimodalDataItem.__setitem__(self,key,value)
  MultimodalDataItem.from_dict(obj)
  MultimodalDataItem.is_audio(self)
  MultimodalDataItem.is_empty_list(l)
  MultimodalDataItem.is_image(self)
  MultimodalDataItem.is_modality(self,modality)
  MultimodalDataItem.is_valid(self)
  MultimodalDataItem.is_video(self)
  MultimodalDataItem.merge(self,other)
  MultimodalDataItem.set(self,key,value)
  MultimodalDataItem.set_pad_value(self)
  MultimodalDataItem.validate(self)
  MultimodalInputs.contains_audio_inputs(self)
  MultimodalInputs.contains_image_inputs(self)
  MultimodalInputs.contains_mm_input(self)
  MultimodalInputs.contains_video_inputs(self)
  MultimodalInputs.from_dict(obj)
  MultimodalInputs.merge(self,other)
  Req.__init__(self,rid,origin_input_text,origin_input_ids,sampling_params,return_logprob,top_logprobs_num,token_ids_logprob,stream,origin_input_ids_unpadded,lora_id,input_embeds,token_type_ids,session_id,custom_logit_processor,return_hidden_states,eos_token_ids,bootstrap_host,bootstrap_port,bootstrap_room,data_parallel_rank,vocab_size)
  Req.__repr__(self)
  Req.adjust_max_prefix_ids(self)
  Req.check_finished(self)
  Req.extend_image_inputs(self,image_inputs)
  Req.finished(self)
  Req.init_incremental_detokenize(self)
  Req.init_next_round_input(self,tree_cache)
  Req.load_kv_cache(self,req_to_token_pool,token_to_kv_pool_allocator)
  Req.log_time_stats(self)
  Req.offload_kv_cache(self,req_to_token_pool,token_to_kv_pool_allocator)
  Req.reset_for_retract(self)
  Req.seqlen(self)
  Req.set_finish_with_abort(self,error_msg)
  ScheduleBatch.__str__(self)
  ScheduleBatch._available_and_evictable_str(self)
  ScheduleBatch._evict_tree_cache_if_needed(self,num_tokens)
  ScheduleBatch._get_available_size()
  ScheduleBatch._is_available_size_sufficient(self,num_tokens)
  ScheduleBatch.alloc_paged_token_slots_decode(self,seq_lens,last_loc,backup_state)
  ScheduleBatch.alloc_paged_token_slots_extend(self,prefix_lens,seq_lens,last_loc,extend_num_tokens,backup_state)
  ScheduleBatch.alloc_req_slots(self,num_reqs)
  ScheduleBatch.alloc_token_slots(self,num_tokens,backup_state)
  ScheduleBatch.batch_size(self)
  ScheduleBatch.check_decode_mem(self,buf_multiplier)
  ScheduleBatch.copy(self)
  ScheduleBatch.filter_batch(self,chunked_req_to_exclude,keep_indices)
  ScheduleBatch.get_model_worker_batch(self,seq_lens_cpu_cache)
  ScheduleBatch.get_required_tokens(num_reqs)
  ScheduleBatch.init_new(cls,reqs,req_to_token_pool,token_to_kv_pool_allocator,tree_cache,model_config,enable_overlap,spec_algorithm,chunked_req)
  ScheduleBatch.is_empty(self)
  ScheduleBatch.merge_batch(self,other)
  ScheduleBatch.mix_with_running(self,running_batch)
  ScheduleBatch.new_page_count_next_decode(self)
  ScheduleBatch.prepare_encoder_info_decode(self)
  ScheduleBatch.prepare_encoder_info_extend(self,input_ids,seq_lens)
  ScheduleBatch.prepare_for_decode(self)
  ScheduleBatch.prepare_for_extend(self)
  ScheduleBatch.prepare_for_idle(self)
  ScheduleBatch.prepare_for_split_prefill(self)
  ScheduleBatch.retract_decode(self,server_args)
  get_last_loc(req_to_token,req_pool_indices_tensor,prefix_lens_tensor)
  get_last_loc_kernel(req_to_token,req_pool_indices_tensor,prefix_lens_tensor,result,num_tokens,req_to_token_stride,BLOCK_SIZE)
  get_last_loc_torch(req_to_token,req_pool_indices_tensor,prefix_lens_tensor)
  get_last_loc_triton(req_to_token,req_pool_indices_tensor,prefix_lens_tensor)
  write_req_to_token_pool_triton(req_to_token_ptr,req_pool_indices,pre_lens,seq_lens,extend_lens,out_cache_loc,req_to_token_ptr_stride)
managers/schedule_policy.py:
  PrefillAdder.__init__(self,page_size,tree_cache,token_to_kv_pool_allocator,running_batch,new_token_ratio,rem_input_tokens,rem_chunk_tokens,mixed_with_decode_tokens)
  PrefillAdder._lock_node(self,last_node)
  PrefillAdder._update_prefill_budget(self,prefix_len,extend_input_len,max_new_tokens)
  PrefillAdder.add_chunked_req(self,req)
  PrefillAdder.add_one_req(self,req,has_chunked_req)
  PrefillAdder.add_one_req_ignore_eos(self,req,has_chunked_req)
  PrefillAdder.add_req_state(r,insert_sort)
  PrefillAdder.budget_state(self)
  PrefillAdder.ceil_paged_tokens(self,tokens)
  PrefillAdder.cur_rem_tokens(self)
  PrefillAdder.rem_total_tokens(self)
  SchedulePolicy.__init__(self,policy,tree_cache,enable_hierarchical_cache)
  SchedulePolicy._calc_weight(cur_node,node_to_weight)
  SchedulePolicy._compute_prefix_matches(self,waiting_queue,policy)
  SchedulePolicy._determine_active_policy(self,waiting_queue)
  SchedulePolicy._get_dfs_priority(cur_node,node_to_priority,last_node_to_reqs,q)
  SchedulePolicy._sort_by_dfs_weight(waiting_queue,tree_cache)
  SchedulePolicy._sort_by_longest_output(waiting_queue)
  SchedulePolicy._sort_by_longest_prefix(waiting_queue,temporary_deprioritized)
  SchedulePolicy._sort_randomly(waiting_queue)
  SchedulePolicy._validate_and_adjust_policy(self,policy,tree_cache)
  SchedulePolicy.calc_priority(self,waiting_queue)
managers/scheduler.py:
  IdleSleeper.__init__(self,sockets)
  IdleSleeper.maybe_sleep(self)
  Scheduler.__init__(self,server_args,port_args,gpu_id,tp_rank,moe_ep_rank,pp_rank,dp_rank,dp_balance_meta)
  Scheduler._add_request_to_queue(self,req)
  Scheduler._extend_requests_to_queue(self,reqs,is_retracted)
  Scheduler._get_swa_token_info(self)
  Scheduler._get_token_info(self)
  Scheduler._pause_engine(self)
  Scheduler._prefetch_kvcache(self,req)
  Scheduler.abort_request(self,recv_req)
  Scheduler.check_memory(self)
  Scheduler.check_tree_cache(self)
  Scheduler.clear_hicache_storage_wrapped(self,recv_req)
  Scheduler.close_session(self,recv_req)
  Scheduler.current_scheduler_metrics_enabled(self)
  Scheduler.event_loop_normal(self)
  Scheduler.event_loop_overlap(self)
  Scheduler.event_loop_pp(self)
  Scheduler.expert_distribution_handle(self,recv_req)
  Scheduler.flush_cache(self)
  Scheduler.flush_cache_wrapped(self,recv_req)
  Scheduler.gather_dp_balance_info(holding_tokens_list)
  Scheduler.get_idle_batch(self)
  Scheduler.get_internal_state(self,recv_req)
  Scheduler.get_load(self)
  Scheduler.get_new_batch_prefill(self)
  Scheduler.get_next_batch_to_run(self)
  Scheduler.get_num_allocatable_reqs(self,running_bs)
  Scheduler.get_print_prefix(self)
  Scheduler.handle_batch_embedding_request(self,recv_req)
  Scheduler.handle_batch_generate_request(self,recv_req)
  Scheduler.handle_dp_balance_data(self,local_batch)
  Scheduler.handle_embedding_request(self,recv_req)
  Scheduler.handle_freeze_gc(self,recv_req)
  Scheduler.handle_generate_request(self,recv_req)
  Scheduler.handle_rpc_request(self,recv_req)
  Scheduler.init_disaggregation(self)
  Scheduler.init_memory_pool_and_cache(self)
  Scheduler.init_moe_config(self)
  Scheduler.init_tokenizer(self)
  Scheduler.load_lora_adapter(self,recv_req)
  Scheduler.maybe_send_health_check_signal(self)
  Scheduler.maybe_sleep_on_idle(self)
  Scheduler.move_ready_grammar_requests(self)
  Scheduler.open_session(self,recv_req)
  Scheduler.prepare_mlp_sync_batch(self,local_batch)
  Scheduler.prepare_mlp_sync_batch_raw(local_batch,dp_size,attn_tp_size,tp_group,get_idle_batch,disable_cuda_graph,spec_algorithm,speculative_num_draft_tokens,require_mlp_tp_gather,disable_overlap_schedule)
  Scheduler.process_batch_result(self,batch,result,launch_done)
  Scheduler.process_input_requests(self,recv_reqs)
  Scheduler.recv_requests(self)
  Scheduler.run_batch(self,batch)
  Scheduler.self_check_during_idle(self)
  Scheduler.set_internal_state(self,recv_req)
  Scheduler.set_next_batch_sampling_info_done(self,batch)
  Scheduler.slow_down(self,recv_req)
  Scheduler.unload_lora_adapter(self,recv_req)
  Scheduler.update_running_batch(self,batch)
  Scheduler.watchdog_thread(self)
  Scheduler.write_shared_dp_balance_info(new_recv_rid_lists,local_tokens)
  is_health_check_generate_req(recv_req)
  is_work_request(recv_req)
  run_scheduler_process(server_args,port_args,gpu_id,tp_rank,moe_ep_rank,pp_rank,dp_rank,pipe_writer,balance_meta)
managers/scheduler_input_blocker.py:
  SchedulerInputBlocker.__init__(self,noop)
  SchedulerInputBlocker._change_state(self,original,target)
  SchedulerInputBlocker._execute_block_req(self)
  SchedulerInputBlocker._execute_unblock_req(self)
  SchedulerInputBlocker._handle_arrive_unblock_barrier(self)
  SchedulerInputBlocker._handle_recv_req(self,recv_req)
  SchedulerInputBlocker.handle(self,recv_reqs)
  input_blocker_guard_region(send_to_scheduler)
managers/scheduler_metrics_mixin.py:
  KvMetrics.__init__(self)
  SchedulerMetricsMixin._emit_kv_metrics(self)
  SchedulerMetricsMixin._publish_kv_events(self)
  SchedulerMetricsMixin.init_kv_events(self,kv_events_config)
  SchedulerMetricsMixin.init_metrics(self,tp_rank,pp_rank,dp_rank)
  SchedulerMetricsMixin.log_decode_stats(self,can_run_cuda_graph,running_batch)
  SchedulerMetricsMixin.log_prefill_stats(self,adder,can_run_list,running_bs)
managers/scheduler_output_processor_mixin.py:
  SchedulerOutputProcessorMixin.add_input_logprob_return_values(self,i,req,output,logprob_pt,num_input_logprobs,last_prefill_chunk)
  SchedulerOutputProcessorMixin.add_logprob_return_values(self,i,req,pt,next_token_ids,num_input_logprobs,output)
  SchedulerOutputProcessorMixin.process_batch_result_decode(self,batch,result,launch_done)
  SchedulerOutputProcessorMixin.process_batch_result_prefill(self,batch,result,launch_done)
  SchedulerOutputProcessorMixin.stream_output(self,reqs,return_logprob,skip_req)
  SchedulerOutputProcessorMixin.stream_output_embedding(self,reqs)
  SchedulerOutputProcessorMixin.stream_output_generation(self,reqs,return_logprob,skip_req)
managers/scheduler_profiler_mixin.py:
  SchedulerProfilerMixin._profile_batch_predicate(self,batch)
  SchedulerProfilerMixin.init_profier(self)
  SchedulerProfilerMixin.init_profile(self,output_dir,start_step,num_steps,activities,with_stack,record_shapes,profile_by_stage,profile_id)
  SchedulerProfilerMixin.profile(self,recv_req)
  SchedulerProfilerMixin.start_profile(self,stage)
  SchedulerProfilerMixin.stop_profile(self,stage)
managers/scheduler_recv_skipper.py:
  SchedulerRecvSkipper.__init__(self,server_args)
  SchedulerRecvSkipper.handle(self,last_forward_mode)
  SchedulerRecvSkipper.maybe_create(server_args)
managers/scheduler_update_weights_mixin.py:
  SchedulerUpdateWeightsMixin.get_weights_by_name(self,recv_req)
  SchedulerUpdateWeightsMixin.init_weights_update_group(self,recv_req)
  SchedulerUpdateWeightsMixin.release_memory_occupation(self,recv_req)
  SchedulerUpdateWeightsMixin.resume_memory_occupation(self,recv_req)
  SchedulerUpdateWeightsMixin.save_remote_model(self,params)
  SchedulerUpdateWeightsMixin.save_sharded_model(self,params)
  SchedulerUpdateWeightsMixin.update_weights_from_disk(self,recv_req)
  SchedulerUpdateWeightsMixin.update_weights_from_distributed(self,recv_req)
  SchedulerUpdateWeightsMixin.update_weights_from_tensor(self,recv_req)
  _export_static_state(model)
  _import_static_state(model,static_params)
managers/session_controller.py:
  Session.__init__(self,capacity_of_str_len,session_id)
  Session.create_req(self,req,tokenizer)
  SessionReqNode.__init__(self,req,parent,childs)
  SessionReqNode.__str__(self)
  SessionReqNode._str_helper(self,prefix)
  SessionReqNode.abort(self)
  SessionReqNode.clear(self,req_dict)
  SessionReqNode.clear_childs(self,req_dict)
managers/template_manager.py:
  TemplateManager.__init__(self)
  TemplateManager._detect_reasoning_pattern(self,template)
  TemplateManager._load_explicit_chat_template(self,tokenizer_manager,chat_template_arg)
  TemplateManager._load_jinja_template(self,tokenizer_manager,template_path)
  TemplateManager._load_json_chat_template(self,template_path)
  TemplateManager._load_json_completion_template(self,template_path)
  TemplateManager._resolve_hf_chat_template(self,tokenizer_manager)
  TemplateManager.chat_template_name(self)
  TemplateManager.completion_template_name(self)
  TemplateManager.force_reasoning(self)
  TemplateManager.guess_chat_template_from_model_path(self,model_path)
  TemplateManager.initialize_templates(self,tokenizer_manager,model_path,chat_template,completion_template)
  TemplateManager.jinja_template_content_format(self)
  TemplateManager.load_chat_template(self,tokenizer_manager,chat_template_arg,model_path)
  TemplateManager.load_completion_template(self,completion_template_arg)
managers/tokenizer_manager.py:
  SignalHandler.__init__(self,tokenizer_manager)
  SignalHandler.running_phase_sigquit_handler(self,signum,frame)
  SignalHandler.sigterm_handler(self,signum,frame)
  TokenizerManager.__init__(self,server_args,port_args)
  TokenizerManager._create_tokenized_object(self,obj,input_text,input_ids,input_embeds,mm_inputs,token_type_ids)
  TokenizerManager._dump_data_to_file(self,data_list,filename,log_message)
  TokenizerManager._handle_abort_req(self,recv_obj)
  TokenizerManager._handle_batch_output(self,recv_obj)
  TokenizerManager._handle_open_session_req_output(self,recv_obj)
  TokenizerManager._handle_update_weights_from_disk_req_output(self,recv_obj)
  TokenizerManager._send_batch_request(self,obj,tokenized_objs,created_time)
  TokenizerManager._send_one_request(self,obj,tokenized_obj,created_time)
  TokenizerManager._upload_file_to_gcs(bucket_name,source_file_path,object_name)
  TokenizerManager._validate_batch_tokenization_constraints(self,batch_size,obj)
  TokenizerManager._validate_input_ids_in_vocab(self,input_ids,vocab_size)
  TokenizerManager._validate_one_request(self,obj,input_ids)
  TokenizerManager.abort_request(self,rid,abort_all)
  TokenizerManager.auto_create_handle_loop(self)
  TokenizerManager.background_task()
  TokenizerManager.collect_metrics(self,state,recv_obj,i)
  TokenizerManager.configure_logging(self,obj)
  TokenizerManager.convert_logprob_style(self,meta_info,state,top_logprobs_num,token_ids_logprob,return_text_in_logprobs,recv_obj,recv_obj_index)
  TokenizerManager.create_abort_task(self,obj)
  TokenizerManager.detokenize_logprob_tokens(self,token_logprobs_val,token_logprobs_idx,decode_to_text)
  TokenizerManager.detokenize_top_logprobs_tokens(self,token_logprobs_val,token_logprobs_idx,decode_to_text)
  TokenizerManager.dump_requests(self,state,out_dict)
  TokenizerManager.dump_requests_before_crash(self)
  TokenizerManager.get_log_request_metadata(self)
  TokenizerManager.record_request_for_crash_dump(self,state,out_dict)
  _Communicator.__init__(self,sender,fan_out)
  _Communicator.handle_recv(self,recv_obj)
  _determine_tensor_transport_mode(server_args)
  async TokenizerManager._batch_tokenize_and_process(self,batch_size,obj)
  async TokenizerManager._execute_profile(self,req)
  async TokenizerManager._handle_batch_request(self,obj,request,created_time)
  async TokenizerManager._tokenize_one_request(self,obj)
  async TokenizerManager._wait_for_model_update_from_disk(self,obj)
  async TokenizerManager._wait_one_response(self,obj,state,request)
  async TokenizerManager.abort_request()
  async TokenizerManager.clear_hicache_storage(self)
  async TokenizerManager.close_session(self,obj,request)
  async TokenizerManager.continue_generation(self)
  async TokenizerManager.dump_expert_distribution_record(self)
  async TokenizerManager.flush_cache(self)
  async TokenizerManager.freeze_gc(self)
  async TokenizerManager.generate_request(self,obj,request)
  async TokenizerManager.get_internal_state(self)
  async TokenizerManager.get_load(self)
  async TokenizerManager.get_weights_by_name(self,obj,request)
  async TokenizerManager.handle_loop(self)
  async TokenizerManager.init_weights_update_group(self,obj,request)
  async TokenizerManager.load_lora_adapter(self,obj,_)
  async TokenizerManager.open_session(self,obj,request)
  async TokenizerManager.pause_generation(self)
  async TokenizerManager.release_memory_occupation(self,obj,request)
  async TokenizerManager.resume_memory_occupation(self,obj,request)
  async TokenizerManager.score_request(self,query,items,label_token_ids,apply_softmax,item_first,request)
  async TokenizerManager.set_internal_state(self,obj)
  async TokenizerManager.sigterm_watchdog(self)
  async TokenizerManager.slow_down(self,obj,request)
  async TokenizerManager.start_expert_distribution_record(self)
  async TokenizerManager.start_profile(self,output_dir,start_step,num_steps,activities,with_stack,record_shapes,profile_by_stage)
  async TokenizerManager.stop_expert_distribution_record(self)
  async TokenizerManager.stop_profile(self)
  async TokenizerManager.unload_lora_adapter(self,obj,_)
  async TokenizerManager.update_weights_from_disk(self,obj,request)
  async TokenizerManager.update_weights_from_distributed(self,obj,request)
  async TokenizerManager.update_weights_from_tensor(self,obj,request)
  async _Communicator.__call__(self,obj)
  async print_exception_wrapper(func)
managers/tp_worker.py:
  TpModelWorker.__init__(self,server_args,gpu_id,tp_rank,moe_ep_rank,pp_rank,dp_rank,nccl_port,is_draft_worker,req_to_token_pool,token_to_kv_pool_allocator)
  TpModelWorker.can_run_lora_batch(self,lora_ids)
  TpModelWorker.forward_batch_embedding(self,model_worker_batch)
  TpModelWorker.forward_batch_generation(self,model_worker_batch,launch_done,skip_sample)
  TpModelWorker.get_attention_tp_cpu_group(self)
  TpModelWorker.get_attention_tp_group(self)
  TpModelWorker.get_memory_pool(self)
  TpModelWorker.get_pad_input_ids_func(self)
  TpModelWorker.get_tokens_per_layer_info(self)
  TpModelWorker.get_tp_group(self)
  TpModelWorker.get_weights_by_name(self,recv_req)
  TpModelWorker.get_worker_info(self)
  TpModelWorker.init_weights_update_group(self,recv_req)
  TpModelWorker.is_hybrid(self)
  TpModelWorker.load_lora_adapter(self,recv_req)
  TpModelWorker.register_hicache_layer_transfer_counter(self,counter)
  TpModelWorker.set_hicache_consumer(self,consumer_index)
  TpModelWorker.sliding_window_size(self)
  TpModelWorker.unload_lora_adapter(self,recv_req)
  TpModelWorker.update_weights_from_disk(self,recv_req)
  TpModelWorker.update_weights_from_distributed(self,recv_req)
  TpModelWorker.update_weights_from_tensor(self,recv_req)
managers/tp_worker_overlap_thread.py:
  TpModelWorkerClient.__delete__(self)
  TpModelWorkerClient.__init__(self,server_args,gpu_id,tp_rank,moe_ep_rank,pp_rank,dp_rank,nccl_port)
  TpModelWorkerClient.can_run_lora_batch(self,lora_ids)
  TpModelWorkerClient.forward_batch_generation(self,model_worker_batch)
  TpModelWorkerClient.forward_thread_func(self)
  TpModelWorkerClient.forward_thread_func_(self)
  TpModelWorkerClient.get_attention_tp_cpu_group(self)
  TpModelWorkerClient.get_attention_tp_group(self)
  TpModelWorkerClient.get_kv_cache(self)
  TpModelWorkerClient.get_memory_pool(self)
  TpModelWorkerClient.get_pad_input_ids_func(self)
  TpModelWorkerClient.get_tokens_per_layer_info(self)
  TpModelWorkerClient.get_tp_group(self)
  TpModelWorkerClient.get_weights_by_name(self,recv_req)
  TpModelWorkerClient.get_worker_info(self)
  TpModelWorkerClient.init_weights_update_group(self,recv_req)
  TpModelWorkerClient.is_hybrid(self)
  TpModelWorkerClient.load_lora_adapter(self,recv_req)
  TpModelWorkerClient.register_hicache_layer_transfer_counter(self,counter)
  TpModelWorkerClient.resolve_last_batch_result(self,launch_done)
  TpModelWorkerClient.set_hicache_consumer(self,consumer_index)
  TpModelWorkerClient.sliding_window_size(self)
  TpModelWorkerClient.unload_lora_adapter(self,recv_req)
  TpModelWorkerClient.update_weights_from_disk(self,recv_req)
  TpModelWorkerClient.update_weights_from_distributed(self,recv_req)
  TpModelWorkerClient.update_weights_from_tensor(self,recv_req)
  resolve_future_token_ids(input_ids,future_token_ids_map)
managers/utils.py:
  DPBalanceMeta.__getstate__(self)
  DPBalanceMeta.__init__(self,num_workers)
  DPBalanceMeta.__setstate__(self,state)
  DPBalanceMeta.destructor(self)
  DPBalanceMeta.get_shared_local_tokens(self)
  DPBalanceMeta.get_shared_onfly(self)
  DPBalanceMeta.set_shared_local_tokens(self,data)
  DPBalanceMeta.set_shared_onfly_info(self,data)
  get_logprob_dict_from_result(result)
  get_logprob_from_pp_outputs(next_pp_outputs)
  validate_input_length(req,max_req_input_len,allow_auto_truncate)
mem_cache/allocator.py:
  BaseTokenToKVPoolAllocator.__init__(self,size,page_size,dtype,device,kvcache,need_sort)
  BaseTokenToKVPoolAllocator.alloc(self,need_size)
  BaseTokenToKVPoolAllocator.alloc_decode(self,*args,**kwargs)
  BaseTokenToKVPoolAllocator.alloc_extend(self,*args,**kwargs)
  BaseTokenToKVPoolAllocator.available_size(self)
  BaseTokenToKVPoolAllocator.backup_state(self)
  BaseTokenToKVPoolAllocator.clear(self)
  BaseTokenToKVPoolAllocator.debug_print(self)
  BaseTokenToKVPoolAllocator.free(self,free_index)
  BaseTokenToKVPoolAllocator.free_group_begin(self)
  BaseTokenToKVPoolAllocator.free_group_end(self)
  BaseTokenToKVPoolAllocator.get_cpu_copy(self,*args,**kwargs)
  BaseTokenToKVPoolAllocator.get_kvcache(self)
  BaseTokenToKVPoolAllocator.load_cpu_copy(self,*args,**kwargs)
  BaseTokenToKVPoolAllocator.merge_and_sort_free(self)
  BaseTokenToKVPoolAllocator.restore_state(self,state)
  PagedTokenToKVPoolAllocator.__init__(self,size,page_size,dtype,device,kvcache,need_sort)
  PagedTokenToKVPoolAllocator.alloc(self,need_size)
  PagedTokenToKVPoolAllocator.alloc_decode(self,seq_lens,last_loc)
  PagedTokenToKVPoolAllocator.alloc_extend(self,prefix_lens,seq_lens,last_loc,extend_num_tokens)
  PagedTokenToKVPoolAllocator.clear(self)
  PagedTokenToKVPoolAllocator.free(self,free_index)
  PagedTokenToKVPoolAllocator.get_cpu_copy(self,indices)
  PagedTokenToKVPoolAllocator.load_cpu_copy(self,kv_cache_cpu,indices)
  SWATokenToKVPoolAllocator.__init__(self,size,size_swa,dtype,device,kvcache,need_sort)
  SWATokenToKVPoolAllocator.alloc(self,need_size)
  SWATokenToKVPoolAllocator.available_size(self)
  SWATokenToKVPoolAllocator.backup_state(self)
  SWATokenToKVPoolAllocator.clear(self)
  SWATokenToKVPoolAllocator.debug_print(self)
  SWATokenToKVPoolAllocator.free(self,free_index)
  SWATokenToKVPoolAllocator.free_swa(self,free_index)
  SWATokenToKVPoolAllocator.full_available_size(self)
  SWATokenToKVPoolAllocator.get_kvcache(self)
  SWATokenToKVPoolAllocator.restore_state(self,state)
  SWATokenToKVPoolAllocator.size_full(self)
  SWATokenToKVPoolAllocator.size_swa(self)
  SWATokenToKVPoolAllocator.swa_available_size(self)
  SWATokenToKVPoolAllocator.translate_loc_from_full_to_swa(self,kv_indices)
  TokenToKVPoolAllocator.__init__(self,size,dtype,device,kvcache,need_sort)
  TokenToKVPoolAllocator.alloc(self,need_size)
  TokenToKVPoolAllocator.available_size(self)
  TokenToKVPoolAllocator.clear(self)
  TokenToKVPoolAllocator.free(self,free_index)
  TokenToKVPoolAllocator.get_cpu_copy(self,indices)
  TokenToKVPoolAllocator.load_cpu_copy(self,kv_cache_cpu,indices)
  alloc_decode_kernel(seq_lens_ptr,last_loc_ptr,free_page_ptr,out_indices,ret_values,bs_upper,page_size)
  alloc_extend_kernel(pre_lens_ptr,seq_lens_ptr,last_loc_ptr,free_page_ptr,out_indices,ret_values,bs_upper,page_size,max_num_extend_tokens)
mem_cache/allocator_ascend.py:
  AscendPagedTokenToKVPoolAllocator.alloc_decode(self,seq_lens,last_loc)
  AscendPagedTokenToKVPoolAllocator.alloc_extend(self,prefix_lens,seq_lens,last_loc,extend_num_tokens)
  alloc_extend_kernel_ascend(prefix_lens,seq_lens,last_loc,free_pages,out_indices,page_size,device)
mem_cache/base_prefix_cache.py:
  BasePrefixCache.cache_finished_req(self,req,**kwargs)
  BasePrefixCache.cache_unfinished_req(self,req,**kwargs)
  BasePrefixCache.check_hicache_events(self)
  BasePrefixCache.dec_lock_ref(self,node,swa_uuid_for_lock)
  BasePrefixCache.evict(self,num_tokens)
  BasePrefixCache.evictable_size(self)
  BasePrefixCache.full_evictable_size(self)
  BasePrefixCache.full_protected_size(self)
  BasePrefixCache.inc_lock_ref(self,node)
  BasePrefixCache.init_load_back(self,last_host_node,host_hit_length)
  BasePrefixCache.match_prefix(self,key,**kwargs)
  BasePrefixCache.pretty_print(self)
  BasePrefixCache.protected_size(self)
  BasePrefixCache.ready_to_load_host_cache(self)
  BasePrefixCache.reset(self)
  BasePrefixCache.swa_evictable_size(self)
  BasePrefixCache.swa_protected_size(self)
  BasePrefixCache.take_events(self)
  BasePrefixCache.total_size(self)
mem_cache/chunk_cache.py:
  ChunkCache.__init__(self,req_to_token_pool,token_to_kv_pool_allocator,page_size)
  ChunkCache.cache_finished_req(self,req)
  ChunkCache.cache_unfinished_req(self,req,chunked)
  ChunkCache.dec_lock_ref(self,node,swa_uuid_for_lock)
  ChunkCache.evict(self,num_tokens)
  ChunkCache.inc_lock_ref(self,node)
  ChunkCache.match_prefix(self,**unused_kwargs)
  ChunkCache.pretty_print(self)
  ChunkCache.reset(self)
  SWAChunkCache.__init__(self,req_to_token_pool,token_to_kv_pool_allocator,page_size)
  SWAChunkCache.evict(self,num_tokens)
  SWAChunkCache.evict_swa(self,req,prelen,attention_chunk_size)
mem_cache/cpp_radix_tree/radix_tree.py:
  RadixTreeCpp.__init__(self,disabled,host_size,page_size,write_through_threshold)
  RadixTreeCpp.commit_loading_onboard(self,handle,success)
  RadixTreeCpp.commit_writing_through(self,handle,success)
  RadixTreeCpp.debug_print(self)
  RadixTreeCpp.evict(self,num_tokens)
  RadixTreeCpp.evictable_size(self)
  RadixTreeCpp.loading_onboard(self,host_node,new_device_indices)
  RadixTreeCpp.lock_ref(self,handle,lock)
  RadixTreeCpp.match_prefix(self,prefix)
  RadixTreeCpp.protected_size(self)
  RadixTreeCpp.reset(self)
  RadixTreeCpp.total_size(self)
  RadixTreeCpp.writing_through(self,key,indices)
mem_cache/hicache_storage.py:
  HiCacheFile.__init__(self,storage_config,file_path)
  HiCacheFile._get_suffixed_key(self,key)
  HiCacheFile.batch_get(self,keys,target_locations,target_sizes)
  HiCacheFile.batch_set(self,keys,values,target_locations,target_sizes)
  HiCacheFile.clear(self)
  HiCacheFile.delete(self,key)
  HiCacheFile.exists(self,key)
  HiCacheFile.get(self,key,target_location,target_sizes)
  HiCacheFile.set(self,key,value,target_location,target_sizes)
  HiCacheStorage.batch_exists(self,keys)
  HiCacheStorage.batch_get(self,keys,target_locations,target_sizes)
  HiCacheStorage.batch_set(self,keys,values,target_locations,target_sizes)
  HiCacheStorage.clear(self)
  HiCacheStorage.delete(self,key)
  HiCacheStorage.exists(self,key)
  HiCacheStorage.get(self,key,target_location,target_sizes)
  HiCacheStorage.set(self,key,value,target_location,target_sizes)
  get_hash_str(token_ids,prior_hash)
mem_cache/hiradix_cache.py:
  HiRadixCache.__init__(self,req_to_token_pool,token_to_kv_pool_allocator,tp_cache_group,page_size,hicache_ratio,hicache_size,hicache_write_policy,hicache_io_backend,hicache_mem_layout,hicache_storage_backend,hicache_storage_prefetch_policy,model_name,storage_backend_extra_config)
  HiRadixCache._collect_leaves_device(self)
  HiRadixCache._evict_backuped(self,node)
  HiRadixCache._evict_regular(self,node)
  HiRadixCache._inc_hit_count(self,node,chunked)
  HiRadixCache._insert_helper_host(self,node,key,host_value,hash_value)
  HiRadixCache._match_prefix_helper(self,node,key)
  HiRadixCache._split_node(self,key,child,split_len)
  HiRadixCache.can_terminate_prefetch(self,operation)
  HiRadixCache.check_backup_progress(self)
  HiRadixCache.check_hicache_events(self)
  HiRadixCache.check_prefetch_progress(self,req_id)
  HiRadixCache.check_revoked_prefetch(self)
  HiRadixCache.clear_storage_backend(self)
  HiRadixCache.evict(self,num_tokens)
  HiRadixCache.evict_host(self,num_tokens)
  HiRadixCache.evictable_size(self)
  HiRadixCache.get_height(self,node)
  HiRadixCache.init_load_back(self,last_node,host_hit_length,mem_quota)
  HiRadixCache.insert(self,key,value,chunked)
  HiRadixCache.is_leaf(node)
  HiRadixCache.load_back(self,node,mem_quota)
  HiRadixCache.loading_check(self)
  HiRadixCache.match_prefix(self,key,**kwargs)
  HiRadixCache.prefetch_from_storage(self,req_id,last_host_node,new_input_tokens,last_hash)
  HiRadixCache.ready_to_load_host_cache(self)
  HiRadixCache.reset(self)
  HiRadixCache.write_backup(self,node,write_back)
  HiRadixCache.write_backup_storage(self,node)
  HiRadixCache.writing_check(self,write_back)
mem_cache/lora_radix_cache.py:
  LoRAKey.__init__(self,lora_id,token_ids)
  LoRAKey.__len__(self)
  LoRARadixCache.__init__(self,req_to_token_pool,token_to_kv_pool_allocator,page_size,disable)
  LoRARadixCache._collect_leaves(self)
  LoRARadixCache._delete_leaf(self,node)
  LoRARadixCache._dfs_helper(node)
  LoRARadixCache._insert_helper(self,node,key,value)
  LoRARadixCache._match_prefix_helper(self,node,key)
  LoRARadixCache._print_helper(self,node,indent)
  LoRARadixCache._split_node(self,key,child,split_len)
  LoRARadixCache._total_size_helper(self)
  LoRARadixCache.all_values_flatten(self)
  LoRARadixCache.cache_finished_req(self,req)
  LoRARadixCache.cache_unfinished_req(self,req,chunked)
  LoRARadixCache.dec_lock_ref(self,node)
  LoRARadixCache.evict(self,num_tokens)
  LoRARadixCache.evictable_size(self)
  LoRARadixCache.inc_lock_ref(self,node)
  LoRARadixCache.insert(self,key,value)
  LoRARadixCache.match_prefix(self,key,**kwargs)
  LoRARadixCache.match_prefix_with_lora_id(self,key,**kwargs)
  LoRARadixCache.pretty_print(self)
  LoRARadixCache.protected_size(self)
  LoRARadixCache.reset(self)
  LoRARadixCache.total_size(self)
  LoRATreeNode.__init__(self,id)
  LoRATreeNode.__lt__(self,other)
  LoRATreeNode.evicted(self)
  _key_match(key0,key1)
  get_child_key(key)
mem_cache/memory_pool.py:
  AscendMLAPagedTokenToKVPool.__init__(self,size,page_size,dtype,kv_lora_rank,qk_rope_head_dim,layer_num,device,enable_memory_saver,start_layer,end_layer)
  AscendMLAPagedTokenToKVPool.get_contiguous_buf_infos(self)
  AscendMLAPagedTokenToKVPool.get_key_buffer(self,layer_id)
  AscendMLAPagedTokenToKVPool.get_kv_buffer(self,layer_id)
  AscendMLAPagedTokenToKVPool.get_kv_size_bytes(self)
  AscendMLAPagedTokenToKVPool.get_value_buffer(self,layer_id)
  AscendMLAPagedTokenToKVPool.set_kv_buffer(self,layer,loc,cache_k,cache_v)
  AscendTokenToKVPool._create_buffers(self)
  AscendTokenToKVPool.get_contiguous_buf_infos(self)
  AscendTokenToKVPool.set_kv_buffer(self,layer,loc,cache_k,cache_v,k_scale,v_scale)
  DoubleSparseTokenToKVPool.__init__(self,size,page_size,dtype,head_num,head_dim,layer_num,device,heavy_channel_num,enable_memory_saver,start_layer,end_layer)
  DoubleSparseTokenToKVPool.get_key_buffer(self,layer_id)
  DoubleSparseTokenToKVPool.get_kv_buffer(self,layer_id)
  DoubleSparseTokenToKVPool.get_label_buffer(self,layer_id)
  DoubleSparseTokenToKVPool.get_value_buffer(self,layer_id)
  DoubleSparseTokenToKVPool.set_kv_buffer(self,layer,loc,cache_k,cache_v,cache_label)
  KVCache.__init__(self,size,page_size,dtype,layer_num,device,enable_memory_saver,start_layer,end_layer)
  KVCache.get_cpu_copy(self,indices)
  KVCache.get_key_buffer(self,layer_id)
  KVCache.get_kv_buffer(self,layer_id)
  KVCache.get_value_buffer(self,layer_id)
  KVCache.load_cpu_copy(self,kv_cache_cpu,indices)
  KVCache.register_layer_transfer_counter(self,layer_transfer_counter)
  KVCache.set_kv_buffer(self,layer,loc,cache_k,cache_v)
  MHATokenToKVPool.__init__(self,size,page_size,dtype,head_num,head_dim,layer_num,device,enable_memory_saver,start_layer,end_layer)
  MHATokenToKVPool._clear_buffers(self)
  MHATokenToKVPool._create_buffers(self)
  MHATokenToKVPool._get_key_buffer(self,layer_id)
  MHATokenToKVPool._get_value_buffer(self,layer_id)
  MHATokenToKVPool.get_contiguous_buf_infos(self)
  MHATokenToKVPool.get_cpu_copy(self,indices)
  MHATokenToKVPool.get_key_buffer(self,layer_id)
  MHATokenToKVPool.get_kv_buffer(self,layer_id)
  MHATokenToKVPool.get_kv_size_bytes(self)
  MHATokenToKVPool.get_value_buffer(self,layer_id)
  MHATokenToKVPool.load_cpu_copy(self,kv_cache_cpu,indices)
  MHATokenToKVPool.maybe_get_custom_mem_pool(self)
  MHATokenToKVPool.move_kv_cache(self,tgt_loc,src_loc)
  MHATokenToKVPool.set_kv_buffer(self,layer,loc,cache_k,cache_v,k_scale,v_scale,layer_id_override)
  MLATokenToKVPool.__init__(self,size,page_size,dtype,kv_lora_rank,qk_rope_head_dim,layer_num,device,enable_memory_saver,start_layer,end_layer)
  MLATokenToKVPool.get_contiguous_buf_infos(self)
  MLATokenToKVPool.get_cpu_copy(self,indices)
  MLATokenToKVPool.get_key_buffer(self,layer_id)
  MLATokenToKVPool.get_kv_buffer(self,layer_id)
  MLATokenToKVPool.get_kv_size_bytes(self)
  MLATokenToKVPool.get_value_buffer(self,layer_id)
  MLATokenToKVPool.load_cpu_copy(self,kv_cache_cpu,indices)
  MLATokenToKVPool.maybe_get_custom_mem_pool(self)
  MLATokenToKVPool.set_kv_buffer(self,layer,loc,cache_k,cache_v)
  MLATokenToKVPool.set_mla_kv_buffer(self,layer,loc,cache_k_nope,cache_k_rope)
  ReqToTokenPool.__init__(self,size,max_context_len,device,enable_memory_saver)
  ReqToTokenPool.alloc(self,need_size)
  ReqToTokenPool.available_size(self)
  ReqToTokenPool.clear(self)
  ReqToTokenPool.free(self,free_index)
  ReqToTokenPool.write(self,indices,values)
  SWAKVPool.__init__(self,size,size_swa,dtype,head_num,head_dim,swa_attention_layer_ids,full_attention_layer_ids,enable_kvcache_transpose,device)
  SWAKVPool.get_contiguous_buf_infos(self)
  SWAKVPool.get_key_buffer(self,layer_id)
  SWAKVPool.get_kv_buffer(self,layer_id)
  SWAKVPool.get_kv_size_bytes(self)
  SWAKVPool.get_value_buffer(self,layer_id)
  SWAKVPool.set_kv_buffer(self,layer,loc,cache_k,cache_v,k_scale,v_scale)
  SWAKVPool.translate_loc_from_full_to_swa(self,kv_indices)
  copy_all_layer_kv_cache(data_ptrs,strides,tgt_loc_ptr,src_loc_ptr,num_locs,num_locs_upper)
  set_mla_kv_buffer_kernel(kv_buffer_ptr,cache_k_nope_ptr,cache_k_rope_ptr,loc_ptr,buffer_stride,nope_stride,rope_stride,nope_dim,rope_dim,BLOCK)
  set_mla_kv_buffer_triton(kv_buffer,loc,cache_k_nope,cache_k_rope)
mem_cache/memory_pool_host.py:
  HostKVCache.__init__(self,device_pool,host_to_device_ratio,host_size,page_size,layout,pin_memory,device)
  HostKVCache.alloc(self,need_size)
  HostKVCache.available_size(self)
  HostKVCache.backup_from_device_all_layer(self,device_pool,host_indices,device_indices,io_backend)
  HostKVCache.clear(self)
  HostKVCache.complete_io(self,indices)
  HostKVCache.free(self,indices)
  HostKVCache.get_dummy_flat_data_page(self)
  HostKVCache.get_flat_data_page(self,index)
  HostKVCache.get_size_per_token(self)
  HostKVCache.get_state(self,indices)
  HostKVCache.init_kv_buffer(self)
  HostKVCache.is_backup(self,indices)
  HostKVCache.is_protected(self,indices)
  HostKVCache.is_reserved(self,indices)
  HostKVCache.is_synced(self,indices)
  HostKVCache.load_to_device_per_layer(self,device_pool,host_indices,device_indices,layer_id,io_backend)
  HostKVCache.protect_load(self,indices)
  HostKVCache.protect_write(self,indices)
  HostKVCache.set_from_flat_data_page(self,index,data_page)
  HostKVCache.update_backup(self,indices)
  HostKVCache.update_prefetch(self,indices)
  HostKVCache.update_synced(self,indices)
  MHATokenToKVPoolHost.__init__(self,device_pool,host_to_device_ratio,host_size,page_size,layout,pin_memory,device)
  MHATokenToKVPoolHost.backup_from_device_all_layer(self,device_pool,host_indices,device_indices,io_backend)
  MHATokenToKVPoolHost.get_buffer_meta(self,keys,indices,local_rank)
  MHATokenToKVPoolHost.get_buffer_with_hash(self,keys,indices)
  MHATokenToKVPoolHost.get_dummy_flat_data_page(self)
  MHATokenToKVPoolHost.get_flat_data_page(self,index)
  MHATokenToKVPoolHost.get_ksize_per_token(self)
  MHATokenToKVPoolHost.get_size_per_token(self)
  MHATokenToKVPoolHost.init_kv_buffer(self)
  MHATokenToKVPoolHost.k_buffer(self)
  MHATokenToKVPoolHost.load_to_device_per_layer(self,device_pool,host_indices,device_indices,layer_id,io_backend)
  MHATokenToKVPoolHost.set_from_flat_data_page(self,index,data_page)
  MHATokenToKVPoolHost.v_buffer(self)
  MLATokenToKVPoolHost.__init__(self,device_pool,host_to_device_ratio,host_size,page_size,layout,pin_memory,device)
  MLATokenToKVPoolHost.backup_from_device_all_layer(self,device_pool,host_indices,device_indices,io_backend)
  MLATokenToKVPoolHost.get_buffer_meta(self,keys,indices,local_rank)
  MLATokenToKVPoolHost.get_buffer_with_hash(self,keys,indices)
  MLATokenToKVPoolHost.get_dummy_flat_data_page(self)
  MLATokenToKVPoolHost.get_flat_data_page(self,index)
  MLATokenToKVPoolHost.get_ksize_per_token(self)
  MLATokenToKVPoolHost.get_size_per_token(self)
  MLATokenToKVPoolHost.init_kv_buffer(self)
  MLATokenToKVPoolHost.load_to_device_per_layer(self,device_pool,host_indices,device_indices,layer_id,io_backend)
  MLATokenToKVPoolHost.set_from_flat_data_page(self,index,data_page)
  _decorator(func)
  synchronized(debug_only)
  wrapper(self,*args,**kwargs)
mem_cache/multimodal_cache.py:
  MultiModalCache.__init__(self,max_size)
  MultiModalCache.__len__(self)
  MultiModalCache._allocate(self,embedding_size)
  MultiModalCache._get_tensor_size(self,embedding)
  MultiModalCache.clear(self)
  MultiModalCache.get(self,mm_hash)
  MultiModalCache.has(self,mm_hash)
  MultiModalCache.put(self,mm_hash,embedding)
mem_cache/radix_cache.py:
  RadixCache.__init__(self,req_to_token_pool,token_to_kv_pool_allocator,page_size,disable,enable_kv_cache_events)
  RadixCache._collect_leaves(self)
  RadixCache._delete_leaf(self,node)
  RadixCache._dfs_helper(node)
  RadixCache._insert_helper(self,node,key,value)
  RadixCache._match_prefix_helper(self,node,key)
  RadixCache._print_helper(self,node,indent)
  RadixCache._record_all_cleared_event(self)
  RadixCache._record_remove_event(self,node)
  RadixCache._record_store_event(self,node)
  RadixCache._split_node(self,key,child,split_len)
  RadixCache._total_size_helper(self)
  RadixCache.all_values_flatten(self)
  RadixCache.cache_finished_req(self,req)
  RadixCache.cache_unfinished_req(self,req,chunked)
  RadixCache.dec_lock_ref(self,node)
  RadixCache.evict(self,num_tokens)
  RadixCache.evictable_size(self)
  RadixCache.inc_lock_ref(self,node)
  RadixCache.insert(self,key,value,chunked)
  RadixCache.match_prefix(self,key,**kwargs)
  RadixCache.pretty_print(self)
  RadixCache.protected_size(self)
  RadixCache.reset(self)
  RadixCache.take_events(self)
  RadixCache.total_size(self)
  TreeNode.__init__(self,id)
  TreeNode.__lt__(self,other)
  TreeNode.backuped(self)
  TreeNode.evicted(self)
  TreeNode.get_last_hash_value(self)
  TreeNode.protect_host(self)
  TreeNode.release_host(self)
  _key_match_page_size1(key0,key1)
  _key_match_paged(key0,key1,page_size)
mem_cache/radix_cache_cpp.py:
  RadixCacheCpp.__init__(self,disable,use_hicache,req_to_token_pool,token_to_kv_pool,tp_cache_group,page_size,hicache_ratio,hicache_size,hicache_write_policy,enable_kv_cache_events,hicache_oracle,enable_write_cancel)
  RadixCacheCpp._insert(self,key,value)
  RadixCacheCpp._merge_tensor(self,l)
  RadixCacheCpp.cache_finished_req(self,req)
  RadixCacheCpp.cache_unfinished_req(self,req,chunked)
  RadixCacheCpp.dec_lock_ref(self,node)
  RadixCacheCpp.evict(self,num_tokens)
  RadixCacheCpp.evictable_size(self)
  RadixCacheCpp.inc_lock_ref(self,node)
  RadixCacheCpp.match_prefix(self,key,**kwargs)
  RadixCacheCpp.pretty_print(self)
  RadixCacheCpp.protected_size(self)
  RadixCacheCpp.reset(self)
  RadixCacheCpp.total_size(self)
mem_cache/storage/hf3fs/client_hf3fs.py:
  Hf3fsClient.__init__(self,path,size,bytes_per_page,entries)
  Hf3fsClient.batch_read(self,offsets,tensors)
  Hf3fsClient.batch_write(self,offsets,tensors)
  Hf3fsClient.check(self,offsets,tensors)
  Hf3fsClient.close(self)
  Hf3fsClient.flush(self)
  Hf3fsClient.get_size(self)
  _decorator(func)
  _decorator(func)
  rsynchronized()
  wrapper(self,*args,**kwargs)
  wrapper(self,*args,**kwargs)
  wsynchronized()
mem_cache/storage/hf3fs/mini_3fs_metadata_server.py:
  GlobalMetadataState.__init__(self,persistence_path,save_interval)
  GlobalMetadataState.load_from_disk(self)
  GlobalMetadataState.save_to_disk(self)
  GlobalMetadataState.schedule_save(self)
  GlobalMetadataState.shutdown(self)
  Hf3fsGlobalMetadataClient.__init__(self,base_url,max_retries)
  Hf3fsGlobalMetadataClient._post(self,endpoint,json_data)
  Hf3fsGlobalMetadataClient.clear(self,rank)
  Hf3fsGlobalMetadataClient.confirm_write(self,rank,written_keys_to_confirm,pages_to_release)
  Hf3fsGlobalMetadataClient.delete_keys(self,rank,keys)
  Hf3fsGlobalMetadataClient.exists(self,rank,keys)
  Hf3fsGlobalMetadataClient.get_page_indices(self,rank,keys)
  Hf3fsGlobalMetadataClient.initialize(self,rank,num_pages)
  Hf3fsGlobalMetadataClient.reserve_and_allocate_page_indices(self,rank,keys)
  Hf3fsLocalMetadataClient.__init__(self)
  Hf3fsLocalMetadataClient.clear(self,rank)
  Hf3fsLocalMetadataClient.confirm_write(self,rank,written_keys_to_confirm,pages_to_release)
  Hf3fsLocalMetadataClient.delete_keys(self,rank,keys)
  Hf3fsLocalMetadataClient.exists(self,rank,keys)
  Hf3fsLocalMetadataClient.get_page_indices(self,rank,keys)
  Hf3fsLocalMetadataClient.initialize(self,rank,num_pages)
  Hf3fsLocalMetadataClient.reserve_and_allocate_page_indices(self,rank,keys)
  Hf3fsMetadataServer.__init__(self,persistence_path,save_interval)
  Hf3fsMetadataServer._setup_routes(self)
  Hf3fsMetadataServer.get_rank_metadata(self,rank)
  Hf3fsMetadataServer.run(self,host,port)
  RankMetadata.__init__(self,num_pages)
  RankMetadata.clear_all(self)
  RankMetadata.confirm_write(self,written_keys_to_confirm,pages_to_release)
  RankMetadata.delete_keys(self,keys)
  RankMetadata.exists_keys(self,keys)
  RankMetadata.get_page_indices(self,keys)
  RankMetadata.reserve_and_allocate_page_indices(self,keys)
  async Hf3fsMetadataServer.clear(self,rank)
  async Hf3fsMetadataServer.confirm_write(self,rank,request)
  async Hf3fsMetadataServer.delete_keys(self,rank,request)
  async Hf3fsMetadataServer.exists(self,rank,request)
  async Hf3fsMetadataServer.get_page_indices(self,rank,request)
  async Hf3fsMetadataServer.initialize(self,rank,request)
  async Hf3fsMetadataServer.reserve_and_allocate_page_indices(self,rank,request)
  run_metadata_server(host,port,persistence_path,save_interval)
mem_cache/storage/hf3fs/storage_hf3fs.py:
  AtomicCounter.__init__(self,n)
  AtomicCounter.next(self)
  Hf3fsMetadataInterface.clear(self,rank)
  Hf3fsMetadataInterface.confirm_write(self,rank,written_keys_to_confirm,pages_to_release)
  Hf3fsMetadataInterface.delete_keys(self,rank,keys)
  Hf3fsMetadataInterface.exists(self,rank,keys)
  Hf3fsMetadataInterface.get_page_indices(self,rank,keys)
  Hf3fsMetadataInterface.initialize(self,rank,num_pages)
  Hf3fsMetadataInterface.reserve_and_allocate_page_indices(self,rank,keys)
  HiCacheHF3FS.__init__(self,rank,file_path,file_size,numjobs,bytes_per_page,entries,dtype,metadata_client,is_mla_model)
  HiCacheHF3FS.batch_exists(self,keys)
  HiCacheHF3FS.batch_get(self,keys,target_locations,target_sizes)
  HiCacheHF3FS.batch_set(self,keys,values,target_locations,target_sizes)
  HiCacheHF3FS.clear(self)
  HiCacheHF3FS.close(self)
  HiCacheHF3FS.delete(self,key)
  HiCacheHF3FS.exists(self,key)
  HiCacheHF3FS.from_env_config(bytes_per_page,dtype,storage_config)
  HiCacheHF3FS.get(self,key,target_location,target_sizes)
  HiCacheHF3FS.set(self,key,value,target_location,target_sizes)
  _decorator(func)
  synchronized()
  wrapper(self,*args,**kwargs)
mem_cache/storage/hf3fs/test_hf3fs_utils.py:
  test_rw_shm()
mem_cache/storage/mooncake_store/mooncake_store.py:
  MooncakeStore.__init__(self,storage_config)
  MooncakeStore._batch_exist(self,key_strs)
  MooncakeStore._get_batch_zero_copy_impl(self,key_strs,buffer_ptrs,buffer_sizes)
  MooncakeStore._put_batch_zero_copy_impl(self,key_strs,buffer_ptrs,buffer_sizes)
  MooncakeStore.batch_exists(self,keys)
  MooncakeStore.batch_get(self,keys,target_location,target_sizes)
  MooncakeStore.batch_set(self,keys,values,target_location,target_sizes)
  MooncakeStore.clear(self)
  MooncakeStore.close(self)
  MooncakeStore.delete(self,key)
  MooncakeStore.exists(self,key)
  MooncakeStore.get(self,key,target_location,target_sizes)
  MooncakeStore.register_buffer(self,buffer)
  MooncakeStore.set(self,key,value,target_location,target_sizes)
  MooncakeStore.warmup(self)
  MooncakeStoreConfig.__post_init__(self)
  MooncakeStoreConfig.from_file()
  MooncakeStoreConfig.load_from_env()
mem_cache/storage/mooncake_store/unit_test.py:
  test_exists()
  test_init_and_warmup()
  test_register_buffer()
  test_set_and_get()
mem_cache/storage/nixl/hicache_nixl.py:
  HiCacheNixl.__init__(self,file_path,plugin)
  HiCacheNixl._execute_transfer(self,buffers,keys,direction)
  HiCacheNixl.batch_get(self,keys,target_locations,target_sizes)
  HiCacheNixl.batch_set(self,keys,values,target_locations,target_sizes)
  HiCacheNixl.exists(self,key)
  HiCacheNixl.get(self,key,target_location,target_sizes)
  HiCacheNixl.register_buffers(self,buffers)
  HiCacheNixl.register_files(self,file_paths,open_file)
  HiCacheNixl.register_objects(self,keys,sizes)
  HiCacheNixl.set(self,key,value,target_location,target_sizes)
mem_cache/storage/nixl/nixl_utils.py:
  NixlBackendSelection.__init__(self,plugin)
  NixlBackendSelection.create_backend(self,agent)
  NixlBackendSelection.set_bucket(self,bucket_name)
  NixlFileManager.__init__(self,base_dir)
  NixlFileManager.close_file(self,fd)
  NixlFileManager.create_file(self,file_path)
  NixlFileManager.files_to_nixl_tuples(self,file_paths)
  NixlFileManager.get_file_path(self,key)
  NixlFileManager.open_file(self,file_path)
  NixlRegistration.__init__(self,agent)
  NixlRegistration._register_memory(self,items,mem_type)
  NixlRegistration.create_query_tuples(self,key,mem_type,file_manager)
mem_cache/storage/nixl/test_hicache_nixl_storage.py:
  TestNixlUnified.delete_test_file(self,file_path)
  TestNixlUnified.setUp(self)
  TestNixlUnified.tearDown(self)
  TestNixlUnified.test_basic_file_operations(self)
  TestNixlUnified.test_batch_set_get(self)
  TestNixlUnified.test_create_nixl_tuples(self)
  TestNixlUnified.test_data_integrity(self)
  TestNixlUnified.test_error_handling(self)
  TestNixlUnified.test_mixed_operations(self)
  TestNixlUnified.test_register_buffers(self)
  TestNixlUnified.test_register_files_with_tuples(self)
  TestNixlUnified.test_single_set_get(self)
  TestNixlUnified.verify_tensor_lists_equal(self,expected,actual)
  TestNixlUnified.verify_tensors_equal(self,expected,actual)
mem_cache/swa_radix_cache.py:
  LRUList.__init__(self,swa)
  LRUList._add_node(self,node)
  LRUList._add_node_after(self,old_node,new_node)
  LRUList._get_lru(self)
  LRUList._remove_node(self,node)
  LRUList.get_leaf_lru_no_lock(self)
  LRUList.get_lru_no_lock(self)
  LRUList.get_prev_leaf_no_lock(self,node,check_id)
  LRUList.get_prev_no_lock(self,node,check_id)
  LRUList.in_list(self,node)
  LRUList.insert_mru(self,node)
  LRUList.remove_node(self,node)
  LRUList.reset_node_and_parents_mru(self,node,root_node)
  LRUList.reset_node_mru(self,node)
  LRUList.sanity_check(self,tree_cache)
  LRUList.sanity_check_evictable_size(self)
  SWARadixCache.__init__(self,req_to_token_pool,token_to_kv_pool_allocator,sliding_window_size,page_size,disable)
  SWARadixCache._collect_all_nodes(self)
  SWARadixCache._collect_leaves(self)
  SWARadixCache._collect_nontombstone_nodes(self)
  SWARadixCache._delete_leaf(self,node)
  SWARadixCache._delete_tombstone_leaf(self,node)
  SWARadixCache._dfs_helper(node)
  SWARadixCache._insert_helper(self,node,key,value,update_kv_after_len)
  SWARadixCache._iteratively_delete_tombstone_leaf(self,node)
  SWARadixCache._match_prefix_helper(self,key)
  SWARadixCache._print_helper(self,node,indent)
  SWARadixCache._split_node(self,key,child,split_len)
  SWARadixCache._tombstone_internal_node(self,node)
  SWARadixCache._total_size_helper(self)
  SWARadixCache.all_values_flatten(self)
  SWARadixCache.cache_finished_req(self,req)
  SWARadixCache.cache_unfinished_req(self,req,chunked)
  SWARadixCache.dec_lock_ref(self,node,swa_uuid_for_lock)
  SWARadixCache.evict(self,full_num_tokens,swa_num_tokens)
  SWARadixCache.evictable_size(self)
  SWARadixCache.full_evictable_size(self)
  SWARadixCache.full_lru_list_evictable_size(self)
  SWARadixCache.full_protected_size(self)
  SWARadixCache.inc_lock_ref(self,node)
  SWARadixCache.insert(self,key,value,prev_prefix_len)
  SWARadixCache.match_prefix(self,key,**kwargs)
  SWARadixCache.pretty_print(self)
  SWARadixCache.protected_size(self)
  SWARadixCache.reset(self)
  SWARadixCache.sanity_check(self)
  SWARadixCache.swa_evictable_size(self)
  SWARadixCache.swa_lru_list_evictable_size(self)
  SWARadixCache.swa_protected_size(self)
  SWARadixCache.total_size(self)
  TreeNode.__init__(self,id)
  TreeNode.__lt__(self,other)
  TreeNode.backuped(self)
  TreeNode.evicted(self)
  _key_match_page_size1(key0,key1)
  _key_match_paged(key0,key1,page_size)
  gen_swa_uuid()
metrics/collector.py:
  SchedulerMetricsCollector.__init__(self,labels)
  SchedulerMetricsCollector._log_gauge(self,gauge,data)
  SchedulerMetricsCollector.increment_bootstrap_failed_reqs(self)
  SchedulerMetricsCollector.increment_transfer_failed_reqs(self)
  SchedulerMetricsCollector.log_stats(self,stats)
  TimeStats.__str__(self)
  TimeStats.format_duration(self,duration)
  TimeStats.get_type(self)
  TokenizerMetricsCollector.__init__(self,labels,bucket_time_to_first_token,bucket_inter_token_latency,bucket_e2e_request_latency,collect_tokens_histogram)
  TokenizerMetricsCollector._log_histogram(self,histogram,data)
  TokenizerMetricsCollector.observe_inter_token_latency(self,internval,num_new_tokens)
  TokenizerMetricsCollector.observe_one_aborted_request(self)
  TokenizerMetricsCollector.observe_one_finished_request(self,prompt_tokens,generation_tokens,cached_tokens,e2e_latency,has_grammar)
  TokenizerMetricsCollector.observe_time_to_first_token(self,value)
metrics/func_timer.py:
  async async_wrapper(*args,**kwargs)
  enable_func_timer()
  exponential_buckets(start,width,length)
  measure(func)
  sync_wrapper(*args,**kwargs)
  time_func_latency(func,name)
model_executor/cuda_graph_runner.py:
  CudaGraphRunner.__init__(self,model_runner)
  CudaGraphRunner._cache_loc_dtype(self)
  CudaGraphRunner._capture_graph(self,graph,pool,stream,run_once_fn)
  CudaGraphRunner._create_device_graph(self)
  CudaGraphRunner.can_run(self,forward_batch)
  CudaGraphRunner.capture(self)
  CudaGraphRunner.capture_one_batch_size(self,bs,forward)
  CudaGraphRunner.get_spec_info(self,num_tokens)
  CudaGraphRunner.recapture_if_needed(self,forward_batch)
  CudaGraphRunner.replay(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors)
  CudaGraphRunner.replay_prepare(self,forward_batch,pp_proxy_tensors)
  CudaGraphRunner.run_once()
  _to_torch(model,reverse,num_tokens)
  freeze_gc(enable_cudagraph_gc)
  get_batch_sizes_to_capture(model_runner)
  get_global_graph_memory_pool()
  get_is_capture_mode()
  model_capture_mode()
  patch_model(model,enable_compile,num_tokens,tp_group)
  set_global_graph_memory_pool(val)
  set_torch_compile_config()
model_executor/forward_batch_info.py:
  CaptureHiddenMode.__lt__(self,other)
  CaptureHiddenMode.is_full(self)
  CaptureHiddenMode.is_last(self)
  CaptureHiddenMode.need_capture(self)
  ForwardBatch._compute_mrope_positions(self,model_runner,batch)
  ForwardBatch._pad_tensor_to_size(self,tensor,size,value)
  ForwardBatch.can_run_tbo(self)
  ForwardBatch.contains_audio_inputs(self)
  ForwardBatch.contains_image_inputs(self)
  ForwardBatch.contains_mm_inputs(self)
  ForwardBatch.contains_video_inputs(self)
  ForwardBatch.get_max_chunk_capacity(self)
  ForwardBatch.get_prefix_chunk_seq_lens(self,prefix_lens,num_prefix_chunks,prefix_chunk_len)
  ForwardBatch.init_new(cls,batch,model_runner)
  ForwardBatch.merge_mm_inputs(self)
  ForwardBatch.post_forward_mlp_sync_batch(self,logits_output)
  ForwardBatch.prepare_chunked_kv_indices(self,device)
  ForwardBatch.prepare_chunked_prefix_cache_info(self,device)
  ForwardBatch.prepare_mlp_sync_batch(self,model_runner)
  ForwardBatch.set_attn_attend_prefix_cache(self,attn_attend_prefix_cache)
  ForwardBatch.set_prefix_chunk_idx(self,idx)
  ForwardMode.is_cuda_graph(self)
  ForwardMode.is_decode(self)
  ForwardMode.is_decode_or_idle(self)
  ForwardMode.is_draft_extend(self)
  ForwardMode.is_dummy_first(self)
  ForwardMode.is_extend(self)
  ForwardMode.is_extend_or_draft_extend_or_mixed(self)
  ForwardMode.is_idle(self)
  ForwardMode.is_mixed(self)
  ForwardMode.is_prefill(self)
  ForwardMode.is_split_prefill(self)
  ForwardMode.is_target_verify(self)
  PPProxyTensors.__eq__(self,other)
  PPProxyTensors.__getitem__(self,key)
  PPProxyTensors.__init__(self,tensors)
  PPProxyTensors.__len__(self)
  PPProxyTensors.__repr__(self)
  PPProxyTensors.__setitem__(self,key,value)
  clamp_position(seq_lens)
  compute_position(attn_backend,extend_prefix_lens,extend_seq_lens,extend_seq_lens_sum)
  compute_position_kernel(positions,extend_start_loc,extend_prefix_lens,extend_seq_lens,has_prefix)
  compute_position_torch(extend_prefix_lens,extend_seq_lens)
  compute_position_triton(extend_prefix_lens,extend_seq_lens,extend_seq_lens_sum)
  create_chunked_prefix_cache_kv_indices(req_to_token_ptr,req_pool_indices_ptr,chunk_start_idx_ptr,chunk_seq_lens_ptr,chunk_cu_seq_lens_ptr,chunk_kv_indices_ptr,req_to_token_ptr_stride)
  enable_num_token_non_padded(server_args)
model_executor/model_runner.py:
  LocalSerializedTensor.get(self,rank)
  ModelRunner.__init__(self,model_config,mem_fraction_static,gpu_id,tp_rank,tp_size,moe_ep_rank,moe_ep_size,pp_rank,pp_size,nccl_port,server_args,dp_rank,is_draft_worker,req_to_token_pool,token_to_kv_pool_allocator)
  ModelRunner._forward_raw(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors,reinit_attn_backend,split_forward_count)
  ModelRunner._get_attention_backend(self)
  ModelRunner._get_attention_backend_from_str(self,backend_str)
  ModelRunner._preprocess_logits(self,logits_output,sampling_info)
  ModelRunner._update_weights_from_flattened_bucket(self,flattened_tensor_bucket_dict)
  ModelRunner.apply_torch_tp(self)
  ModelRunner.forward(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors,reinit_attn_backend,split_forward_count)
  ModelRunner.forward_decode(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors)
  ModelRunner.forward_extend(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors)
  ModelRunner.forward_idle(self,forward_batch,pp_proxy_tensors)
  ModelRunner.forward_split_prefill(self,forward_batch,reinit_attn_backend,forward_count)
  ModelRunner.get_weight_iter(config)
  ModelRunner.get_weights_by_name(self,name,truncate_size)
  ModelRunner.init_attention_backend(self)
  ModelRunner.init_cublas(self)
  ModelRunner.init_device_graphs(self)
  ModelRunner.init_double_sparsity_channel_config(self,selected_channel)
  ModelRunner.init_lora_manager(self)
  ModelRunner.init_memory_pool(self,total_gpu_memory,max_num_reqs,max_total_tokens)
  ModelRunner.init_threads_binding(self)
  ModelRunner.init_torch_distributed(self)
  ModelRunner.init_weights_update_group(self,master_address,master_port,rank_offset,world_size,group_name,backend)
  ModelRunner.initialize(self,min_per_gpu_memory)
  ModelRunner.load_lora_adapter(self,lora_ref)
  ModelRunner.load_model(self)
  ModelRunner.model_is_mrope(self)
  ModelRunner.model_load_weights(model,iter)
  ModelRunner.model_specific_adjustment(self)
  ModelRunner.profile_max_num_token(self,total_gpu_memory)
  ModelRunner.sample(self,logits_output,forward_batch)
  ModelRunner.save_remote_model(self,url)
  ModelRunner.save_sharded_model(self,path,pattern,max_size)
  ModelRunner.set_num_token_hybrid(self)
  ModelRunner.unload_lora_adapter(self,lora_ref)
  ModelRunner.update_expert_location(self,new_expert_location_metadata,update_layer_ids)
  ModelRunner.update_weights_from_disk(self,model_path,load_format)
  ModelRunner.update_weights_from_distributed(self,names,dtypes,shapes,group_name)
  ModelRunner.update_weights_from_tensor(self,named_tensors,load_format)
  RankZeroFilter.__init__(self,is_rank_zero)
  RankZeroFilter.filter(self,record)
  _model_load_weights_direct(model,named_tensors)
  _unwrap_tensor(tensor,tp_rank,device)
model_executor/npu_graph_runner.py:
  NPUGraphRunner.__init__(self,model_runner)
  NPUGraphRunner._cache_loc_dtype(self)
  NPUGraphRunner._capture_graph(self,graph,pool,stream,run_once_fn)
  NPUGraphRunner._create_device_graph(self)
  NPUGraphRunner._update_inputs(self,seq_lens)
  NPUGraphRunner.replay(self,forward_batch,skip_attn_backend_init,pp_proxy_tensors)
model_loader/__init__.py:
  get_model(model_config,load_config,device_config)
model_loader/loader.py:
  BaseModelLoader.__init__(self,load_config)
  BaseModelLoader.download_model(self,model_config)
  BaseModelLoader.load_model(self,model_config,device_config)
  BitsAndBytesModelLoader.__init__(self,load_config)
  BitsAndBytesModelLoader._get_config_file(self,qlora_adapter)
  BitsAndBytesModelLoader._get_quantized_weights_iterator(self,model_name_or_path,revision,pre_quant,load_8bit)
  BitsAndBytesModelLoader._get_weight_files(self,model_name_or_path,allowed_patterns,revision)
  BitsAndBytesModelLoader._hf_weight_iter(self,hf_weights_files,use_safetensors)
  BitsAndBytesModelLoader._is_4bit_weight_name(self,weight_name)
  BitsAndBytesModelLoader._is_8bit_weight_name(self,weight_name)
  BitsAndBytesModelLoader._load_weights(self,model_config,model)
  BitsAndBytesModelLoader._parse_quant_state(param_name,temp_state_dict)
  BitsAndBytesModelLoader._prepare_weights(self,model_name_or_path,revision)
  BitsAndBytesModelLoader._quantized_4bit_generator(self,hf_weights_files,use_safetensors,quant_state_dict)
  BitsAndBytesModelLoader._quantized_8bit_generator(self,hf_weights_files,use_safetensors,quant_state_dict)
  BitsAndBytesModelLoader._unquantized_generator(self,hf_weights_files,use_safetensors,quant_state_dict)
  BitsAndBytesModelLoader.download_model(self,model_config)
  BitsAndBytesModelLoader.load_model(self,model_config,device_config)
  DefaultModelLoader.Source.init_new(cls,model_config,model)
  DefaultModelLoader.__init__(self,load_config)
  DefaultModelLoader._get_all_weights(self,model_config,model)
  DefaultModelLoader._get_weights_iterator(self,source)
  DefaultModelLoader._maybe_download_from_modelscope(self,model,revision)
  DefaultModelLoader._prepare_weights(self,model_name_or_path,revision,fall_back_to_pt)
  DefaultModelLoader.download_model(self,model_config)
  DefaultModelLoader.load_model(self,model_config,device_config)
  DefaultModelLoader.load_weights_and_postprocess(model,weights,target_device)
  DummyModelLoader.__init__(self,load_config)
  DummyModelLoader.download_model(self,model_config)
  DummyModelLoader.load_model(self,model_config,device_config)
  GGUFModelLoader.__init__(self,load_config)
  GGUFModelLoader._get_gguf_weights_map(self,model_config)
  GGUFModelLoader._get_weights_iterator(self,model_name_or_path,gguf_to_hf_name_map)
  GGUFModelLoader._prepare_weights(self,model_name_or_path)
  GGUFModelLoader.download_model(self,model_config)
  GGUFModelLoader.load_model(self,model_config,device_config)
  LayeredModelLoader.__init__(self,load_config)
  LayeredModelLoader.fill_module(module,fqn,weights)
  LayeredModelLoader.load_model(self,model_config,device_config)
  RemoteModelLoader.__init__(self,load_config)
  RemoteModelLoader._get_weights_iterator_fs(self,client)
  RemoteModelLoader._get_weights_iterator_kv(self,client)
  RemoteModelLoader._load_model_from_remote_fs(self,model,client,model_config,device_config)
  RemoteModelLoader._load_model_from_remote_kv(self,model,model_config,client)
  RemoteModelLoader.download_model(self,model_config)
  RemoteModelLoader.load_model(self,model_config,device_config)
  RemoteModelLoader.save_model(model,model_path,url)
  ShardedStateLoader.__init__(self,load_config)
  ShardedStateLoader._filter_subtensors(tensors)
  ShardedStateLoader._prepare_weights(self,model_name_or_path,revision)
  ShardedStateLoader.download_model(self,model_config)
  ShardedStateLoader.get_end_ptr(tensor)
  ShardedStateLoader.load_model(self,model_config,device_config)
  ShardedStateLoader.save_model(model,path,pattern,max_size)
  _get_quantization_config(model_config,load_config,packed_modules_mapping)
  _initialize_model(model_config,load_config)
  device_loading_context(module,target_device)
  get_model_loader(load_config)
  load_model_with_cpu_quantization(self,model_config,device_config)
model_loader/utils.py:
  get_architecture_class_name(model_config)
  get_model_architecture(model_config)
  post_load_weights(model,model_config)
  resolve_transformers_arch(model_config,architectures)
  set_default_torch_dtype(dtype)
model_loader/weight_utils.py:
  DisabledTqdm.__init__(self,*args,**kwargs)
  KVCacheQuantSchema.check_current_rank(self,info)
  KVCacheQuantSchema.check_is_fp8(self)
  KVCacheQuantSchema.check_tp_ranks(self,info)
  QuantParamSchema.check_model_type(self,info)
  _load_file(bin_file)
  _load_file(st_file)
  _shared_pointers(tensors)
  composed_loader(param,loaded_weight)
  composed_weight_loader(loader,fn)
  convert_bin_to_safetensor_file(pt_filename,sf_filename)
  convert_pyslice_to_tensor(x)
  decrypt(fn,key)
  default_weight_loader(param,loaded_weight)
  download_safetensors_index_file_from_hf(model_name_or_path,index_file,cache_dir,revision)
  download_weights_from_hf(model_name_or_path,cache_dir,allow_patterns,revision,ignore_patterns)
  enable_hf_transfer()
  filter_duplicate_safetensors_files(hf_weights_files,hf_folder,index_file)
  filter_files_not_needed_for_inference(hf_weights_files)
  get_actual_shard_size(shard_size,weight_start,weight_end)
  get_gguf_extra_tensor_names(gguf_file,gguf_to_hf_name_map)
  get_lock(model_name_or_path,cache_dir)
  get_quant_config(model_config,load_config,packed_modules_mapping)
  gguf_quant_weights_iterator(gguf_file,gguf_to_hf_name_map)
  initialize_dummy_weights(model,low,high,seed)
  kv_cache_scales_loader(filename,tp_rank,tp_size,num_hidden_layers,model_type)
  loader(param,loaded_weight)
  maybe_remap_kv_scale_name(name,params_dict)
  multi_thread_pt_weights_iterator(hf_weights_files,max_workers)
  multi_thread_safetensors_weights_iterator(hf_weights_files,is_all_weights_sharded,decryption_key,max_workers,disable_mmap)
  narrow_padded_param_and_loaded_weight(param_data,loaded_weight,param_data_start,weight_start,dim,shard_size,narrow_weight)
  np_cache_weights_iterator(model_name_or_path,cache_dir,hf_folder,hf_weights_files)
  pt_weights_iterator(hf_weights_files)
  reset_param_data_if_needed(param_data,dim,start,length)
  row_parallel_weight_loader(param,loaded_weight)
  runai_safetensors_weights_iterator(hf_weights_files)
  safetensors_encrypted_weights_iterator(hf_weights_files,is_all_weights_sharded,decryption_key)
  safetensors_weights_iterator(hf_weights_files,is_all_weights_sharded,decryption_key,disable_mmap)
  set_runai_streamer_env(load_config)
  sharded_weight_loader(shard_axis)
model_parallel.py:
  ColwiseParallelSharded._partition_linear_fn(self,name,module,device_mesh)
  RowwiseParallelMaybeWait._partition_linear_fn(self,name,module,device_mesh)
  RowwiseParallelMaybeWait._prepare_output_fn(output_layouts,use_local_output,mod,outputs,device_mesh)
  _shard_tensor(full_tensor,device_mesh,placements)
  tensor_parallel(module,device_mesh)
  tplize(mod)
models/arcee.py:
  ArceeAttention.__init__(self,config,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,rope_is_neox_style,max_position_embeddings,quant_config,prefix,bias)
  ArceeAttention.forward(self,positions,hidden_states,forward_batch)
  ArceeDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  ArceeDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  ArceeForCausalLM.__init__(self,config,quant_config,prefix)
  ArceeForCausalLM._init_model(self,config,quant_config,prefix)
  ArceeForCausalLM.end_layer(self)
  ArceeForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding,pp_proxy_tensors)
  ArceeForCausalLM.get_input_embeddings(self)
  ArceeForCausalLM.load_kv_cache_scales(self,quantization_param_path)
  ArceeForCausalLM.load_weights(self,weights)
  ArceeForCausalLM.start_layer(self)
  ArceeMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix,reduce_results)
  ArceeMLP.forward(self,x,forward_batch)
  ArceeModel.__init__(self,config,quant_config,prefix)
  ArceeModel.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  ArceeModel.load_kv_cache_scales(self,quantization_param_path)
models/baichuan.py:
  BaiChuanAttention.__init__(self,hidden_size,num_heads,position_embedding,rope_theta,max_position_embeddings,quant_config,layer_id,prefix)
  BaiChuanAttention.forward(self,positions,hidden_states,forward_batch)
  BaiChuanBaseForCausalLM.__init__(self,config,position_embedding,quant_config,prefix)
  BaiChuanBaseForCausalLM.forward(self,input_ids,positions,forward_batch)
  BaiChuanBaseForCausalLM.load_weights(self,weights)
  BaiChuanDecoderLayer.__init__(self,config,position_embedding,layer_id,quant_config,prefix)
  BaiChuanDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  BaiChuanMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  BaiChuanMLP.forward(self,x)
  BaiChuanModel.__init__(self,config,position_embedding,quant_config,prefix)
  BaiChuanModel.forward(self,input_ids,positions,forward_batch)
  BaichuanForCausalLM.__init__(self,config,quant_config,prefix)
  _get_alibi_slopes(total_num_heads)
models/bailing_moe.py:
  BailingAttention.__init__(self,config,layer_id,quant_config,prefix)
  BailingAttention.forward(self,hidden_states,position_ids,forward_batch)
  BailingMLP.__init__(self,intermediate_size,config,quant_config,reduce_results,prefix)
  BailingMLP.forward(self,x)
  BailingMoE.__init__(self,config,layer_id,quant_config,prefix)
  BailingMoE.forward(self,hidden_states)
  BailingMoeBlock.__init__(self,config,layer_id,quant_config,prefix)
  BailingMoeBlock.forward(self,hidden_states,position_ids,residual,forward_batch)
  BailingMoeForCausalLM.__init__(self,config,quant_config)
  BailingMoeForCausalLM.forward(self,input_ids,positions,forward_batch,inputs_embeds)
  BailingMoeForCausalLM.load_weights(self,weights)
  BailingMoeModel.__init__(self,config,quant_config,prefix)
  BailingMoeModel.forward(self,input_ids,position_ids,forward_batch,input_embeds)
models/bert.py:
  BertAttention.__init__(self,hidden_size,num_attention_heads,layer_norm_eps,layer_id,quant_config,prefix)
  BertAttention.forward(self,hidden_states,forward_batch)
  BertEmbedding.__init__(self,config)
  BertEmbedding.forward(self,input_ids,positions,forward_batch)
  BertEncoder.__init__(self,config,quant_config,prefix)
  BertEncoder.forward(self,hidden_states,forward_batch)
  BertForSequenceClassification.__init__(self,config,quant_config,prefix)
  BertForSequenceClassification.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  BertForSequenceClassification.load_weights(self,weights)
  BertForSequenceClassification.weight_filter()
  BertIntermediate.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  BertIntermediate.forward(self,hidden_states)
  BertLayer.__init__(self,config,layer_id,quant_config,prefix)
  BertLayer.forward(self,hidden_states,forward_batch)
  BertModel.__init__(self,config,quant_config,use_bert_pooler,prefix)
  BertModel.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  BertModel.load_weights(self,weights)
  BertOutput.__init__(self,hidden_size,intermediate_size,layer_norm_eps,quant_config,prefix)
  BertOutput.forward(self,hidden_states,input_tensor)
  BertPooler.__init__(self,config)
  BertPooler.forward(self,hidden_states,forward_batch)
  BertSelfAttention.__init__(self,hidden_size,num_attention_heads,layer_id,quant_config,prefix)
  BertSelfAttention.forward(self,hidden_states,forward_batch)
  BertSelfOutput.__init__(self,hidden_size,layer_norm_eps,quant_config,prefix)
  BertSelfOutput.forward(self,hidden_states,input_tensor)
models/chatglm.py:
  ChatGLMForCausalLM.__init__(self,config,quant_config,prefix)
  ChatGLMForCausalLM.forward(self,input_ids,positions,forward_batch)
  ChatGLMForCausalLM.load_weights(self,weights)
  ChatGLMM.__init__(self,config,quant_config,prefix)
  ChatGLMM.forward(self,input_ids,position_ids,forward_batch)
  GLMAttention.__init__(self,config,layer_id,quant_config,prefix)
  GLMAttention.forward(self,hidden_states,position_ids,forward_batch)
  GLMBlock.__init__(self,config,layer_id,quant_config,prefix)
  GLMBlock.forward(self,hidden_states,position_ids,forward_batch)
  GLMMLP.__init__(self,config,quant_config,prefix)
  GLMMLP.forward(self,hidden_states)
  GLMTransformer.__init__(self,config,quant_config,prefix)
  GLMTransformer.forward(self,hidden_states,position_ids,forward_batch)
models/clip.py:
  CLIPEncoder.__init__(self,config,quant_config,prefix)
  CLIPEncoder.forward(self,inputs_embeds,attention_mask,causal_attention_mask,return_all_hidden_states)
  CLIPEncoderLayer.__init__(self,config,act_layer,norm_layer,attn_implementation,quant_config,prefix)
  CLIPEncoderLayer.forward(self,hidden_states,attention_mask,causal_attention_mask)
  CLIPMLP.__init__(self,config,act_layer,quant_config,prefix)
  CLIPMLP.forward(self,x)
  CLIPModel.__init__(self,config,quant_config,prefix)
  CLIPModel.forward(self,input_ids,positions,forward_batch,get_embedding)
  CLIPModel.load_weights(self,weights)
  CLIPModel.pad_input_ids(self,input_ids,image_inputs)
  CLIPTextEmbeddings.__init__(self,config)
  CLIPTextEmbeddings.forward(self,input_ids,position_ids,inputs_embeds)
  CLIPTextModel.__init__(self,config,quant_config,prefix)
  CLIPTextModel.forward(self,input_ids,position_ids)
  CLIPTextTransformer.__init__(self,config,quant_config,prefix)
  CLIPTextTransformer.device(self)
  CLIPTextTransformer.forward(self,input_ids,attention_mask,position_ids)
  CLIPVisionEmbeddings.__init__(self,config)
  CLIPVisionEmbeddings.forward(self,pixel_values)
  CLIPVisionModel.__init__(self,config,quant_config,prefix)
  CLIPVisionModel.device(self)
  CLIPVisionModel.forward(self,pixel_values)
  CLIPVisionTransformer.__init__(self,config,quant_config,prefix)
  CLIPVisionTransformer.device(self)
  CLIPVisionTransformer.forward(self,pixel_values)
  monkey_patch_weight_loader()
  prepare_weights(self,model_name_or_path,revision,fall_back_to_pt)
models/commandr.py:
  CohereAttention.__init__(self,config,layer_id,quant_config,prefix)
  CohereAttention._apply_qk_norm(self,q,k)
  CohereAttention.forward(self,positions,hidden_states,forward_batch)
  CohereDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  CohereDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  CohereForCausalLM.__init__(self,config,quant_config,prefix)
  CohereForCausalLM.forward(self,input_ids,positions,forward_batch)
  CohereForCausalLM.load_weights(self,weights)
  CohereMLP.__init__(self,config,quant_config,prefix)
  CohereMLP.forward(self,x)
  CohereModel.__init__(self,config,quant_config,prefix)
  CohereModel.forward(self,input_ids,positions,forward_batch)
  LayerNorm.__init__(self,param_shape,eps)
  LayerNorm.forward(self,hidden_states,residuals)
  LayerNorm.weight_loader(self,param,loaded_weight)
  layer_norm_func(hidden_states,weight,variance_epsilon)
models/dbrx.py:
  DbrxAttention.__init__(self,config,layer_id,quant_config,prefix)
  DbrxAttention.forward(self,position_ids,hidden_states,forward_batch)
  DbrxBlock.__init__(self,config,layer_id,quant_config,prefix)
  DbrxBlock.forward(self,position_ids,hidden_states,forward_batch)
  DbrxExperts.__init__(self,config,quant_config,params_dtype,prefix)
  DbrxExperts.forward(self,hidden_states)
  DbrxExperts.weight_loader(self,param,loaded_weight,weight_name)
  DbrxForCausalLM.__init__(self,config,quant_config,prefix)
  DbrxForCausalLM.forward(self,input_ids,positions,forward_batch)
  DbrxForCausalLM.load_weights(self,weights)
  DbrxFusedNormAttention.__init__(self,config,layer_id,quant_config,prefix)
  DbrxFusedNormAttention.forward(self,position_ids,hidden_states,forward_batch)
  DbrxModel.__init__(self,config,quant_config,prefix)
  DbrxModel.forward(self,input_ids,position_ids,forward_batch,input_embeds)
  DbrxRouter.__init__(self,config,params_dtype,prefix)
  DbrxRouter.forward(self,hidden_states)
models/deepseek.py:
  DeepseekAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,max_position_embeddings,quant_config,prefix)
  DeepseekAttention.forward(self,positions,hidden_states,forward_batch)
  DeepseekDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  DeepseekDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  DeepseekForCausalLM.__init__(self,config,quant_config,prefix)
  DeepseekForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  DeepseekForCausalLM.get_input_embeddings(self)
  DeepseekForCausalLM.load_weights(self,weights)
  DeepseekMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,reduce_results,prefix)
  DeepseekMLP.forward(self,x)
  DeepseekMoE.__init__(self,config,quant_config,prefix)
  DeepseekMoE.forward(self,hidden_states)
  DeepseekMoE.pack_params(self)
  DeepseekModel.__init__(self,config,quant_config,prefix)
  DeepseekModel.forward(self,input_ids,positions,forward_batch,input_embeds)
models/deepseek_janus_pro.py:
  AttentionPoolLatent.__init__(self,in_features,out_features,embed_dim,num_heads,feat_size,mlp_ratio,qkv_bias,qk_norm,latent_len,latent_dim,pos_embed,pool_type,norm_layer,drop)
  AttentionPoolLatent.forward(self,x)
  AttentionPoolLatent.init_weights(self)
  AttnBlock.__init__(self,in_channels,norm_type)
  AttnBlock.forward(self,x)
  CLIPVisionTower.__init__(self,model_name,image_size,select_feature,select_layer,select_layers,ckpt_path,pixel_mean,pixel_std,**kwargs)
  CLIPVisionTower.build_vision_tower(self,vision_tower_params)
  CLIPVisionTower.device(self)
  CLIPVisionTower.dtype(self)
  CLIPVisionTower.feature_select(self,image_forward_outs)
  CLIPVisionTower.forward(self,images)
  Decoder.__init__(self,z_channels,ch,ch_mult,num_res_blocks,norm_type,dropout,resamp_with_conv,out_channels)
  Decoder.forward(self,z)
  Decoder.last_layer(self)
  Downsample.__init__(self,in_channels,with_conv)
  Downsample.forward(self,x)
  DropPath.__init__(self,drop_prob,scale_by_keep)
  DropPath.extra_repr(self)
  DropPath.forward(self,x)
  Encoder.__init__(self,in_channels,ch,ch_mult,num_res_blocks,norm_type,dropout,resamp_with_conv,z_channels)
  Encoder.forward(self,x)
  LayerScale.__init__(self,dim,init_values,inplace)
  LayerScale.forward(self,x)
  Mlp.__init__(self,in_features,hidden_features,out_features,act_layer,norm_layer,bias,drop,use_conv)
  Mlp.forward(self,x)
  MlpProjector.__init__(self,cfg)
  MlpProjector.forward(self,x_or_tuple)
  MultiModalityCausalLM.__init__(self,config,quant_config)
  MultiModalityCausalLM.forward(self,input_ids,positions,forward_batch,get_embedding)
  MultiModalityCausalLM.get_image_feature(self,items)
  MultiModalityCausalLM.get_input_embeddings(self)
  MultiModalityCausalLM.load_weights(self,weights)
  MultiModalityCausalLM.pad_input_ids(self,input_ids,image_inputs)
  MultiModalityCausalLM.prepare_gen_img_embeds(self,image_ids)
  Normalize(in_channels,norm_type)
  Normalize.__init__(self,mean,std,inplace)
  Normalize.__repr__(self)
  Normalize.forward(self,tensor)
  PatchDropout.__init__(self,prob,num_prefix_tokens,ordered,return_indices)
  PatchDropout.forward(self,x)
  PatchEmbed.__init__(self,img_size,patch_size,in_chans,embed_dim,norm_layer,flatten,output_fmt,bias,strict_img_size,dynamic_img_pad)
  PatchEmbed._init_img_size(self,img_size)
  PatchEmbed.dynamic_feat_size(self,img_size)
  PatchEmbed.feat_ratio(self,as_scalar)
  PatchEmbed.forward(self,x)
  PatchEmbed.set_input_size(self,img_size,patch_size)
  ResnetBlock.__init__(self,in_channels,out_channels,conv_shortcut,dropout,norm_type)
  ResnetBlock.forward(self,x)
  Upsample.__init__(self,in_channels,with_conv)
  Upsample.forward(self,x)
  VQModel.__init__(self,config)
  VQModel.decode(self,quant)
  VQModel.decode_code(self,code_b,shape,channel_first)
  VQModel.encode(self,x)
  VQModel.forward(self,input)
  VQ_16(**kwargs)
  VectorQuantizer.__init__(self,n_e,e_dim,beta,entropy_loss_ratio,l2_norm,show_usage)
  VectorQuantizer.forward(self,z)
  VectorQuantizer.get_codebook_entry(self,indices,shape,channel_first)
  VisionTransformer.__init__(self,img_size,patch_size,in_chans,num_classes,global_pool,embed_dim,depth,num_heads,mlp_ratio,qkv_bias,qk_norm,init_values,class_token,no_embed_class,reg_tokens,pre_norm,fc_norm,dynamic_img_size,dynamic_img_pad,drop_rate,pos_drop_rate,patch_drop_rate,proj_drop_rate,attn_drop_rate,drop_path_rate,weight_init,embed_layer,_norm_layer,_act_layer,block_fn,mlp_layer,ignore_head)
  VisionTransformer._intermediate_layers(self,x,n)
  VisionTransformer._pos_embed(self,x)
  VisionTransformer.forward(self,x)
  VisionTransformer.forward_features(self,x)
  VisionTransformer.forward_head(self,x,pre_logits)
  VisionTransformer.get_classifier(self)
  VisionTransformer.group_matcher(self,coarse)
  VisionTransformer.init_weights(self,mode)
  VisionTransformer.no_weight_decay(self)
  VisionTransformer.reset_classifier(self,num_classes,global_pool)
  VisionTransformerBlock.__init__(self,dim,num_heads,mlp_ratio,qkv_bias,qk_norm,proj_drop,attn_drop,init_values,drop_path,act_layer,norm_layer,mlp_layer)
  VisionTransformerBlock.forward(self,x)
  _ntuple(n)
  _trunc_normal_(tensor,mean,std,a,b)
  compute_entropy_loss(affinity,loss_type,temperature)
  create_siglip_vit(model_name,image_size,select_layer,ckpt_path,**kwargs)
  drop_path(x,drop_prob,training,scale_by_keep)
  get_resize_mat(_old_size,_new_size)
  init_weights(self)
  init_weights_vit_timm(module,name)
  model_name_to_cls(cls_name)
  named_apply(fn,module,name,depth_first,include_root)
  nchw_to(x,fmt)
  nonlinearity(x)
  norm_cdf(x)
  parse(x)
  resample_abs_pos_embed(posemb,new_size,old_size,num_prefix_tokens,interpolation,antialias,verbose)
  resample_kernel(kernel)
  resample_patch_embed(patch_embed,new_size,interpolation,antialias,verbose)
  resize(x_np,_new_size)
  trunc_normal_tf_(tensor,mean,std,a,b)
  use_fused_attn(experimental)
  vision_head.__init__(self,params)
  vision_head.forward(self,x)
models/deepseek_nextn.py:
  DeepseekModelNextN.__init__(self,config,quant_config,prefix)
  DeepseekModelNextN.forward(self,input_ids,positions,forward_batch,input_embeds)
  DeepseekV3ForCausalLMNextN.__init__(self,config,quant_config,prefix)
  DeepseekV3ForCausalLMNextN.forward(self,input_ids,positions,forward_batch)
  DeepseekV3ForCausalLMNextN.load_weights(self,weights)
models/deepseek_v2.py:
  DeepseekV2AttentionMLA.__init__(self,config,hidden_size,num_heads,qk_nope_head_dim,qk_rope_head_dim,v_head_dim,q_lora_rank,kv_lora_rank,rope_theta,rope_scaling,max_position_embeddings,quant_config,reduce_results,layer_id,prefix,alt_stream)
  DeepseekV2AttentionMLA._chunked_prefix_attn_mha(self,q,accum_output,accum_lse,forward_batch)
  DeepseekV2AttentionMLA._dispatch_mla_subtype()
  DeepseekV2AttentionMLA._fuse_rope_for_trtllm_mla(self,forward_batch)
  DeepseekV2AttentionMLA.dispatch_attn_forward_method(self,forward_batch)
  DeepseekV2AttentionMLA.forward(self,positions,hidden_states,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_core(self,q_pe,k_pe,q_nope_out,k_nope,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_core(self,q_input,key_cache_buf,val_cache_buf,attn_output,kv_indptr,kv_indices,k_pe_output,cos_sin_cache,positions,attn_logits,num_kv_split,sm_scale,enable_rope_fusion,k_input,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_core(self,q_input,k_input,v_input,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_prepare(self,positions,hidden_states,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_prepare(self,positions,hidden_states,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_prepare(self,positions,hidden_states,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_core(self,intermediate_state)
  DeepseekV2AttentionMLA.forward_normal_chunked_kv_core(self,q,k,v,forward_batch)
  DeepseekV2AttentionMLA.forward_normal_chunked_kv_prepare(self,positions,hidden_states,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_normal_core(self,q,k,v,forward_batch)
  DeepseekV2AttentionMLA.forward_normal_prepare(self,positions,hidden_states,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.forward_prepare(self,positions,hidden_states,forward_batch,zero_allocator)
  DeepseekV2AttentionMLA.op_core(self,state)
  DeepseekV2AttentionMLA.op_prepare(self,state)
  DeepseekV2DecoderLayer.__init__(self,config,layer_id,quant_config,is_nextn,prefix,alt_stream)
  DeepseekV2DecoderLayer._is_layer_sparse(self,layer_id,is_nextn)
  DeepseekV2DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual,zero_allocator)
  DeepseekV2DecoderLayer.op_comm_postprocess_layer(self,state)
  DeepseekV2DecoderLayer.op_comm_prepare_attn(self,state,positions,hidden_states,forward_batch,residual,zero_allocator,tbo_subbatch_index)
  DeepseekV2DecoderLayer.op_comm_prepare_mlp(self,state)
  DeepseekV2DecoderLayer.op_mlp(self,state)
  DeepseekV2ForCausalLM.__init__(self,config,quant_config,prefix)
  DeepseekV2ForCausalLM._weight_requant_ue8m0(self,is_nextn)
  DeepseekV2ForCausalLM.determine_num_fused_shared_experts(self,architecture)
  DeepseekV2ForCausalLM.end_layer(self)
  DeepseekV2ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  DeepseekV2ForCausalLM.get_embed_and_head(self)
  DeepseekV2ForCausalLM.get_input_embeddings(self)
  DeepseekV2ForCausalLM.get_model_config_for_expert_location(cls,config)
  DeepseekV2ForCausalLM.load_weights(self,weights,is_nextn)
  DeepseekV2ForCausalLM.post_load_weights(self,is_nextn,weight_names)
  DeepseekV2ForCausalLM.routed_experts_weights_of_layer(self)
  DeepseekV2ForCausalLM.set_embed_and_head(self,embed,head)
  DeepseekV2ForCausalLM.start_layer(self)
  DeepseekV2MLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,reduce_results,prefix,tp_rank,tp_size)
  DeepseekV2MLP.forward(self,x,forward_batch,should_allreduce_fusion,use_reduce_scatter)
  DeepseekV2MoE.__init__(self,config,layer_id,quant_config,prefix,alt_stream,is_nextn)
  DeepseekV2MoE._forward_shared_experts(self,hidden_states)
  DeepseekV2MoE.forward(self,hidden_states,forward_batch,should_allreduce_fusion,use_reduce_scatter)
  DeepseekV2MoE.forward_cpu(self,hidden_states,should_allreduce_fusion)
  DeepseekV2MoE.forward_deepep(self,hidden_states,forward_batch)
  DeepseekV2MoE.forward_normal(self,hidden_states,should_allreduce_fusion,use_reduce_scatter)
  DeepseekV2MoE.forward_normal_dual_stream(self,hidden_states,should_allreduce_fusion,use_reduce_scatter)
  DeepseekV2MoE.get_moe_weights(self)
  DeepseekV2MoE.op_combine_a(self,state)
  DeepseekV2MoE.op_combine_b(self,state)
  DeepseekV2MoE.op_dispatch_a(self,state)
  DeepseekV2MoE.op_dispatch_b(self,state)
  DeepseekV2MoE.op_experts(self,state)
  DeepseekV2MoE.op_gate(self,state)
  DeepseekV2MoE.op_output(self,state)
  DeepseekV2MoE.op_select_experts(self,state)
  DeepseekV2MoE.op_shared_experts(self,state)
  DeepseekV2Model.__init__(self,config,quant_config,prefix)
  DeepseekV2Model.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  DeepseekV2Model.get_input_embeddings(self)
  MoEGate.__init__(self,config,prefix,is_nextn)
  MoEGate.forward(self,hidden_states)
  yarn_get_mscale(scale,mscale)
models/deepseek_vl2.py:
  DeepseekVL2ForCausalLM.__init__(self,config,quant_config)
  DeepseekVL2ForCausalLM._init_vision_module(self,vision_config,quant_config)
  DeepseekVL2ForCausalLM.forward(self,input_ids,positions,forward_batch,**kwargs)
  DeepseekVL2ForCausalLM.get_image_feature(self,items)
  DeepseekVL2ForCausalLM.load_weights(self,weights)
  DeepseekVL2ForCausalLM.pad_input_ids(self,input_ids,mm_inputs)
  DeepseekVL2MlpProjector.__init__(self,config,quant_config)
  DeepseekVL2MlpProjector.forward(self,x)
models/ernie4.py:
  Ernie4DecoderLayer.__init__(self,config,layer_id,quant_config,prefix,is_mtp)
  Ernie4DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Ernie4Model.__init__(self,config,quant_config,prefix)
  Ernie4Model.forward(self,input_ids,positions,forward_batch,input_embeds)
  Ernie4Moe.__init__(self,config,layer_id,quant_config,prefix)
  Ernie4Moe.forward(self,hidden_states)
  Ernie4Moe.forward_normal(self,hidden_states)
  Ernie4_5_ForCausalLM.__init__(self,config,quant_config,prefix)
  Ernie4_5_ForCausalLM.forward(self,input_ids,positions,forward_batch)
  Ernie4_5_ForCausalLM.get_embed_and_head(self)
  Ernie4_5_ForCausalLM.load_weights(self,weights)
  Ernie4_5_MoeForCausalLM.load_weights(self,weights)
  MoEGate.__init__(self,config,prefix)
  MoEGate.forward(self,hidden_states)
models/ernie4_eagle.py:
  Ernie4ModelMTP.__init__(self,config,layer_id,prefix,quant_config)
  Ernie4ModelMTP.forward(self,input_ids,positions,forward_batch,input_embeds)
  Ernie4_5_MoeForCausalLMMTP.__init__(self,config,quant_config,prefix,mtp_layer_id)
  Ernie4_5_MoeForCausalLMMTP.forward(self,input_ids,positions,forward_batch)
  Ernie4_5_MoeForCausalLMMTP.get_embed_and_head(self)
  Ernie4_5_MoeForCausalLMMTP.load_weights(self,weights)
  Ernie4_5_MoeForCausalLMMTP.set_embed_and_head(self,embed,head)
models/exaone.py:
  ExaoneAttention.__init__(self,config,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,rope_is_neox_style,max_position_embeddings,quant_config,prefix)
  ExaoneAttention.forward(self,positions,hidden_states,forward_batch)
  ExaoneDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  ExaoneDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  ExaoneForCausalLM.__init__(self,config,quant_config,prefix)
  ExaoneForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  ExaoneForCausalLM.load_weights(self,weights)
  ExaoneGatedMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  ExaoneGatedMLP.forward(self,x)
  ExaoneModel.__init__(self,config,quant_config,prefix)
  ExaoneModel.forward(self,input_ids,positions,forward_batch,input_embeds)
models/gemma.py:
  GemmaAttention.__init__(self,hidden_size,num_heads,num_kv_heads,head_dim,layer_id,max_position_embeddings,rope_theta,quant_config,prefix)
  GemmaAttention.forward(self,positions,hidden_states,forward_batch)
  GemmaDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  GemmaDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  GemmaForCausalLM.__init__(self,config,quant_config,prefix)
  GemmaForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  GemmaForCausalLM.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval,input_embeds)
  GemmaForCausalLM.load_weights(self,weights)
  GemmaMLP.__init__(self,hidden_size,intermediate_size,quant_config,prefix)
  GemmaMLP.forward(self,x)
  GemmaModel.__init__(self,config,quant_config,prefix)
  GemmaModel.forward(self,input_ids,positions,forward_batch,input_embeds)
models/gemma2.py:
  Gemma2Attention.__init__(self,layer_id,config,hidden_size,num_heads,num_kv_heads,head_dim,max_position_embeddings,rope_theta,quant_config,prefix)
  Gemma2Attention.forward(self,positions,hidden_states,forward_batch)
  Gemma2DecoderLayer.__init__(self,layer_id,config,quant_config,prefix)
  Gemma2DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Gemma2ForCausalLM.__init__(self,config,quant_config,prefix)
  Gemma2ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  Gemma2ForCausalLM.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval,input_embeds)
  Gemma2ForCausalLM.get_attention_sliding_window_size(self)
  Gemma2ForCausalLM.load_weights(self,weights)
  Gemma2MLP.__init__(self,hidden_size,intermediate_size,hidden_act,hidden_activation,quant_config,prefix)
  Gemma2MLP.forward(self,x)
  Gemma2Model.__init__(self,config,quant_config,prefix)
  Gemma2Model.forward(self,input_ids,positions,forward_batch,input_embeds)
  get_attention_sliding_window_size(config)
models/gemma2_reward.py:
  Gemma2ForSequenceClassification.__init__(self,config,quant_config,prefix)
  Gemma2ForSequenceClassification.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  Gemma2ForSequenceClassification.load_weights(self,weights)
models/gemma3_causal.py:
  Gemma3Attention.__init__(self,layer_id,config,max_position_embeddings,quant_config,prefix)
  Gemma3Attention.forward(self,hidden_states,position_embeddings,forward_batch,**kwargs)
  Gemma3Attention.naive_attn_with_masks(self,q,k,v,out,**kwargs)
  Gemma3DecoderLayer.__init__(self,layer_id,config,quant_config,prefix)
  Gemma3DecoderLayer.forward(self,positions,hidden_states,position_embeddings_global,position_embeddings_local,forward_batch,**kwargs)
  Gemma3ForCausalLM.__init__(self,config,quant_config,prefix)
  Gemma3ForCausalLM.dtype(self)
  Gemma3ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,**kwargs)
  Gemma3ForCausalLM.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval,input_embeds)
  Gemma3ForCausalLM.get_attention_sliding_window_size(self)
  Gemma3ForCausalLM.get_input_embeddings(self)
  Gemma3ForCausalLM.load_weights(self,weights)
  Gemma3MLP.__init__(self,hidden_size,intermediate_size,hidden_activation,quant_config,prefix)
  Gemma3MLP.forward(self,x)
  Gemma3RotaryEmbedding.__init__(self,config,device)
  Gemma3RotaryEmbedding._dynamic_frequency_update(self,position_ids,device)
  Gemma3RotaryEmbedding.forward(self,x,position_ids)
  Gemma3TextModel.__init__(self,config,quant_config,prefix)
  Gemma3TextModel.forward(self,input_ids,positions,forward_batch,input_embeds,**kwargs)
  Gemma3TextScaledWordEmbedding.__init__(self,num_embeddings,embedding_dim,padding_idx,embed_scale)
  Gemma3TextScaledWordEmbedding.forward(self,input_ids)
  extract_layer_index(prefix)
  get_attention_sliding_window_size(config)
models/gemma3_mm.py:
  Gemma3ForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Gemma3ForConditionalGeneration.forward(self,input_ids,positions,forward_batch,input_embeds,**kwargs)
  Gemma3ForConditionalGeneration.get_attention_sliding_window_size(self)
  Gemma3ForConditionalGeneration.get_image_feature(self,items)
  Gemma3ForConditionalGeneration.get_input_embeddings(self)
  Gemma3ForConditionalGeneration.load_weights(self,weights)
  Gemma3ForConditionalGeneration.pad_input_ids(self,input_ids,image_inputs)
  Gemma3ForConditionalGeneration.prepare_attn_masks(self,input_ids,positions,mask_dtype,**kwargs)
  Gemma3ForConditionalGeneration.tie_weights(self)
  Gemma3MultiModalProjector.__init__(self,config)
  Gemma3MultiModalProjector.forward(self,vision_outputs)
models/gemma3n_audio.py:
  Gemma3nAudioAttention.__init__(self,config,quant_config,prefix)
  Gemma3nAudioAttention._convert_to_block(self,x)
  Gemma3nAudioAttention._extract_block_context(self,x)
  Gemma3nAudioAttention._pad_dim1(self,x,dim10_val,dim11_val)
  Gemma3nAudioAttention.forward(self,x,mask)
  Gemma3nAudioConformerAttention.__init__(self,config,quant_config,prefix)
  Gemma3nAudioConformerAttention.forward(self,audio_encodings,audio_mel_mask)
  Gemma3nAudioConformerBlock.__init__(self,config,quant_config,prefix)
  Gemma3nAudioConformerBlock.forward(self,audio_encodings,audio_mel_mask)
  Gemma3nAudioConformerFeedForward.__init__(self,config,quant_config,prefix)
  Gemma3nAudioConformerFeedForward.forward(self,audio_encodings)
  Gemma3nAudioConformerLightConv1d.__init__(self,config,quant_config,prefix)
  Gemma3nAudioConformerLightConv1d.forward(self,audio_encodings)
  Gemma3nAudioEncoder.__init__(self,config,quant_config,prefix)
  Gemma3nAudioEncoder.forward(self,audio_mel,audio_mel_mask)
  Gemma3nAudioRelativePositionEmbedding.__init__(self,config,quant_config,prefix)
  Gemma3nAudioRelativePositionEmbedding._get_timing_signal_1d_pos(self,position,dtype)
  Gemma3nAudioRelativePositionEmbedding._relative_shift(self,term_bd_before_shift,batch_size,num_heads,num_query_blocks,query_block_size,key_context_size,max_span_plus_1)
  Gemma3nAudioRelativePositionEmbedding.forward(self,queries,keys)
  Gemma3nAudioSSCPConvBlock.__init__(self,config,idx,input_freq_dim,manual_padding,quant_config,prefix)
  Gemma3nAudioSSCPConvBlock.forward(self,audio_encodings)
  Gemma3nAudioSubSampleConvProjection.__init__(self,config,quant_config,prefix)
  Gemma3nAudioSubSampleConvProjection.forward(self,audio_encodings)
  Gemma3nCumulativeGroupNorm.__init__(self,num_channels,feature_dims,eps)
  Gemma3nCumulativeGroupNorm.forward(self,x,mask)
models/gemma3n_causal.py:
  Gemma3nAltUp.__init__(self,config,quant_config,prefix)
  Gemma3nAltUp.compute_router_modalities(self,x)
  Gemma3nAltUp.correct(self,predictions,activated)
  Gemma3nAltUp.forward(self,hidden_states,activated)
  Gemma3nAltUp.predict(self,hidden_states)
  Gemma3nAltUp.scale_corrected_output(self,corrected)
  Gemma3nAttention.__init__(self,layer_id,config,max_position_embeddings,quant_config,prefix)
  Gemma3nAttention.forward(self,hidden_states,positions,forward_batch,**kwargs)
  Gemma3nDecoderLayer.__init__(self,layer_id,config,quant_config,prefix)
  Gemma3nDecoderLayer.forward(self,positions,hidden_states,per_layer_input,forward_batch,**kwargs)
  Gemma3nForCausalLM.__init__(self,config,quant_config,prefix)
  Gemma3nForCausalLM.dtype(self)
  Gemma3nForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,per_layer_inputs,**kwargs)
  Gemma3nForCausalLM.get_attention_sliding_window_size(self)
  Gemma3nForCausalLM.get_input_embeddings(self)
  Gemma3nForCausalLM.load_weights(self,weights)
  Gemma3nLaurelBlock.__init__(self,config,quant_config,prefix)
  Gemma3nLaurelBlock.forward(self,x)
  Gemma3nRMSNorm.__init__(self,dim,eps,with_scale)
  Gemma3nRMSNorm.forward(self,x)
  Gemma3nTextMLP.__init__(self,hidden_size,intermediate_size,hidden_activation,activation_sparsity,quant_config,prefix)
  Gemma3nTextMLP._gaussian_topk(self,inputs)
  Gemma3nTextMLP.forward(self,x)
  Gemma3nTextModel.__init__(self,config,quant_config,prefix)
  Gemma3nTextModel.dtype(self)
  Gemma3nTextModel.forward(self,input_ids,positions,forward_batch,input_embeds,per_layer_inputs,**kwargs)
  Gemma3nTextModel.get_input_embeddings(self)
  Gemma3nTextModel.get_per_layer_inputs(self,input_ids)
  Gemma3nTextModel.project_per_layer_inputs(self,inputs_embeds,per_layer_inputs)
  get_attention_sliding_window_size(config)
models/gemma3n_mm.py:
  Gemma3nForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Gemma3nForConditionalGeneration.forward(self,input_ids,positions,forward_batch,input_embeds,**kwargs)
  Gemma3nForConditionalGeneration.get_attention_sliding_window_size(self)
  Gemma3nForConditionalGeneration.get_audio_feature(self,items)
  Gemma3nForConditionalGeneration.get_hidden_dim(self,module_name)
  Gemma3nForConditionalGeneration.get_image_feature(self,items)
  Gemma3nForConditionalGeneration.get_input_embeddings(self)
  Gemma3nForConditionalGeneration.get_per_layer_inputs(self,input_ids)
  Gemma3nForConditionalGeneration.load_weights(self,weights)
  Gemma3nForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
  Gemma3nForConditionalGeneration.project_per_layer_inputs(self,inputs_embeds,per_layer_inputs)
  Gemma3nForConditionalGeneration.should_apply_lora(self,module_name)
  Gemma3nForConditionalGeneration.tie_weights(self)
  Gemma3nMultimodalEmbedder.__init__(self,multimodal_config,text_config,quant_config,prefix)
  Gemma3nMultimodalEmbedder.forward(self,input_ids,inputs_embeds)
models/glm4.py:
  Glm4Attention.__init__(self,config,layer_id,quant_config,prefix)
  Glm4Attention.forward(self,positions,hidden_states,forward_batch)
  Glm4DecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  Glm4DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Glm4ForCausalLM.__init__(self,config,quant_config,prefix)
  Glm4ForCausalLM.forward(self,input_ids,positions,forward_batch)
  Glm4ForCausalLM.load_weights(self,weights)
  Glm4Model.__init__(self,config,quant_config,prefix)
  Glm4Model.dtype(self)
  Glm4Model.forward(self,input_ids,positions,forward_batch,input_embeds)
  Glm4Model.get_input_embeddings(self)
models/glm4_moe.py:
  Glm4MoeAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,partial_rotary_factor,rope_scaling,max_position_embeddings,head_dim,rms_norm_eps,attention_bias,quant_config,use_qk_norm,prefix,alt_stream)
  Glm4MoeAttention._apply_qk_norm(self,q,k)
  Glm4MoeAttention.forward(self,positions,hidden_states,forward_batch)
  Glm4MoeAttention.forward_core(self,intermediate_state)
  Glm4MoeAttention.forward_prepare(self,positions,hidden_states,forward_batch)
  Glm4MoeAttention.op_core(self,state)
  Glm4MoeAttention.op_prepare(self,state)
  Glm4MoeDecoderLayer.__init__(self,config,layer_id,quant_config,is_nextn,prefix,alt_stream)
  Glm4MoeDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual,zero_allocator)
  Glm4MoeForCausalLM.__init__(self,config,quant_config,prefix)
  Glm4MoeForCausalLM.determine_num_fused_shared_experts(self,architecture)
  Glm4MoeForCausalLM.get_input_embeddings(self)
  Glm4MoeForCausalLM.load_weights(self,weights,is_nextn)
  Glm4MoeGate.__init__(self,config,prefix,is_nextn)
  Glm4MoeGate.forward(self,hidden_states)
  Glm4MoeMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,reduce_results,prefix,tp_rank,tp_size)
  Glm4MoeMLP.forward(self,x,forward_batch,should_allreduce_fusion)
  Glm4MoeModel.__init__(self,config,quant_config,prefix)
  Glm4MoeSparseMoeBlock.__init__(self,config,layer_id,quant_config,prefix,alt_stream,is_nextn)
  Glm4MoeSparseMoeBlock.forward_normal(self,hidden_states,should_allreduce_fusion,use_reduce_scatter)
  Glm4MoeSparseMoeBlock.forward_normal_dual_stream(self,hidden_states,should_allreduce_fusion,use_reduce_scatter)
models/glm4_moe_nextn.py:
  Glm4MoeForCausalLMNextN.__init__(self,config,quant_config,prefix)
  Glm4MoeForCausalLMNextN.forward(self,input_ids,positions,forward_batch)
  Glm4MoeForCausalLMNextN.load_weights(self,weights)
  Glm4MoeModelNextN.__init__(self,config,quant_config,prefix)
  Glm4MoeModelNextN.forward(self,input_ids,positions,forward_batch,input_embeds)
models/glm4v.py:
  Glm4vForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Glm4vForConditionalGeneration._pad_vit_attn_dummy_heads(self,name,loaded_weight)
  Glm4vForConditionalGeneration._update_hf_config(self)
  Glm4vForConditionalGeneration.get_image_feature(self,items)
  Glm4vForConditionalGeneration.get_video_feature(self,items)
  Glm4vForConditionalGeneration.load_weights(self,weights)
  Glm4vPatchMerger.__init__(self,d_model,context_dim,quant_config,bias,prefix)
  Glm4vPatchMerger.forward(self,x)
  Glm4vRMSNorm.forward(self,x)
  Glm4vVisionBlock.__init__(self,config,norm_layer,quant_config,prefix)
  Glm4vVisionEmbeddings.__init__(self,config)
  Glm4vVisionEmbeddings.forward(self,embeddings,lengths,image_shapes,h_coords,w_coords)
  Glm4vVisionMLP.__init__(self,in_features,hidden_features,bias,quant_config,prefix)
  Glm4vVisionMLP.forward(self,x)
  Glm4vVisionModel.__init__(self,vision_config,norm_eps,quant_config,prefix)
  Glm4vVisionModel.device(self)
  Glm4vVisionModel.dtype(self)
  Glm4vVisionModel.forward(self,x,grid_thw)
  Glm4vVisionModel.rot_pos_emb(self,grid_thw)
  Glm4vVisionPatchEmbed.__init__(self,patch_size,temporal_patch_size,in_channels,hidden_size)
  Glm4vVisionPatchEmbed.forward(self,x)
  Glm4vVisionRotaryEmbedding.__init__(self,dim,theta)
  Glm4vVisionRotaryEmbedding.forward(self,seqlen)
  Glm4vVisionRotaryEmbedding.update_freqs_cache(self,seqlen)
models/glm4v_moe.py:
  Glm4vMoeForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Glm4vMoeForConditionalGeneration.determine_num_fused_shared_experts(self,architecture)
  Glm4vMoeForConditionalGeneration.load_weights(self,weights,is_nextn)
models/gpt2.py:
  GPT2Attention.__init__(self,layer_id,config,quant_config,prefix)
  GPT2Attention.forward(self,hidden_states,forward_batch)
  GPT2Block.__init__(self,layer_id,config,act_layer,quant_config,prefix)
  GPT2Block.forward(self,hidden_states,forward_batch)
  GPT2LMHeadModel.__init__(self,config,quant_config,prefix)
  GPT2LMHeadModel.forward(self,input_ids,positions,forward_batch)
  GPT2LMHeadModel.load_weights(self,weights)
  GPT2MLP.__init__(self,intermediate_size,config,act_layer,quant_config,prefix)
  GPT2MLP.forward(self,hidden_states)
  GPT2Model.__init__(self,config,quant_config,prefix)
  GPT2Model.forward(self,input_ids,position_ids,forward_batch)
models/gpt_bigcode.py:
  GPTBigCodeAttention.__init__(self,layer_id,config,quant_config,prefix)
  GPTBigCodeAttention.forward(self,hidden_states,forward_batch)
  GPTBigCodeBlock.__init__(self,layer_id,config,quant_config,prefix)
  GPTBigCodeBlock.forward(self,hidden_states,forward_batch)
  GPTBigCodeForCausalLM.__init__(self,config,quant_config,prefix)
  GPTBigCodeForCausalLM.forward(self,input_ids,positions,forward_batch)
  GPTBigCodeForCausalLM.load_weights(self,weights)
  GPTBigCodeModel.__init__(self,config,quant_config,prefix)
  GPTBigCodeModel.forward(self,input_ids,position_ids,forward_batch)
  GPTBigMLP.__init__(self,intermediate_size,config,quant_config,prefix)
  GPTBigMLP.forward(self,hidden_states)
models/gpt_oss.py:
  GptOssAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,max_position_embeddings,head_dim,rms_norm_eps,attention_bias,quant_config,prefix,sliding_window_size,layer_type,params_dtype)
  GptOssAttention.forward(self,positions,hidden_states,forward_batch)
  GptOssAttention.forward_core(self,intermediate_state)
  GptOssAttention.forward_prepare(self,positions,hidden_states,forward_batch)
  GptOssConfig.__init__(self,**kwargs)
  GptOssDecoderLayer.__init__(self,config,layer_id,quant_config,prefix,sliding_window_size)
  GptOssDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  GptOssForCausalLM.__init__(self,config,quant_config,prefix)
  GptOssForCausalLM._get_default_weight_mapping(self)
  GptOssForCausalLM._load_mxfp4_experts_weights(self,weights)
  GptOssForCausalLM._load_normal_weights(self,weights,is_nextn,weight_name_mapping,other_loaded_param_names)
  GptOssForCausalLM._load_weights_mxfp4(self,weights,is_nextn,weight_name_mapping)
  GptOssForCausalLM.end_layer(self)
  GptOssForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  GptOssForCausalLM.get_attention_sliding_window_size(self)
  GptOssForCausalLM.get_embed_and_head(self)
  GptOssForCausalLM.get_model_config_for_expert_location(cls,config)
  GptOssForCausalLM.load_weights(self,weights,is_nextn,weight_name_mapping)
  GptOssForCausalLM.routed_experts_weights_of_layer(self)
  GptOssForCausalLM.set_eagle3_layers_to_capture(self,layer_ids)
  GptOssForCausalLM.set_embed_and_head(self,embed,head)
  GptOssForCausalLM.start_layer(self)
  GptOssModel.__init__(self,config,quant_config,prefix,decoder_layer_type)
  GptOssModel.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  GptOssSparseMoeBlock.__init__(self,layer_id,config,quant_config,prefix)
  GptOssSparseMoeBlock.forward(self,hidden_states,forward_batch,should_allreduce_fusion)
  GptOssSparseMoeBlock.forward_normal(self,hidden_states,should_allreduce_fusion)
  GptOssSparseMoeBlock.get_moe_weights(self)
  _WeightCreator.__init__(self,fn)
  _WeightCreator.maybe_materialize(obj)
  _canonicalize_weights(config,weights_in)
  _create_fused_set_kv_buffer_arg(value,layer,forward_batch)
  _dequant_mlp_weight(debug_name,w_blocks,w_scales)
  _enable_fused_set_kv_buffer()
  get_attention_sliding_window_size(config)
models/granite.py:
  GraniteAttention.__init__(self,config,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,rope_is_neox_style,max_position_embeddings,quant_config,prefix)
  GraniteAttention.forward(self,positions,hidden_states,forward_batch)
  GraniteDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  GraniteDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  GraniteForCausalLM.__init__(self,config,quant_config,prefix)
  GraniteForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  GraniteForCausalLM.get_module_name_from_weight_name(self,name)
  GraniteForCausalLM.get_num_params(self)
  GraniteForCausalLM.get_weights_by_name(self,name,truncate_size,tp_size)
  GraniteForCausalLM.load_weights(self,weights)
  GraniteMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  GraniteMLP.forward(self,x)
  GraniteModel.__init__(self,config,quant_config,prefix)
  GraniteModel.forward(self,input_ids,positions,forward_batch,input_embeds)
models/granitemoe.py:
  GraniteMoeAttention.__init__(self,hidden_size,num_heads,num_kv_heads,max_position,layer_id,rope_theta,quant_config,attention_multiplier,prefix)
  GraniteMoeAttention.forward(self,positions,hidden_states,forward_batch)
  GraniteMoeDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  GraniteMoeDecoderLayer.forward(self,positions,hidden_states,forward_batch)
  GraniteMoeForCausalLM.__init__(self,config,quant_config,prefix)
  GraniteMoeForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  GraniteMoeForCausalLM.load_weights(self,weights)
  GraniteMoeMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,params_dtype,quant_config,tp_size,prefix)
  GraniteMoeMoE.forward(self,hidden_states)
  GraniteMoeModel.__init__(self,config,quant_config,prefix)
  GraniteMoeModel.forward(self,input_ids,positions,forward_batch,inputs_embeds)
  GraniteMoeModel.get_input_embeddings(self,input_ids)
models/grok.py:
  Grok1Attention.__init__(self,config,hidden_size,num_heads,num_kv_heads,layer_id,max_position,rope_theta,quant_config,reduce_results,alt_stream,load_presharded_attn,prefix)
  Grok1Attention.forward(self,positions,hidden_states,forward_batch)
  Grok1DecoderLayer.__init__(self,config,layer_id,quant_config,load_presharded_moe,load_presharded_attn,load_presharded_mlp,alt_stream,skip_moe,prefix)
  Grok1DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual,deferred_norm)
  Grok1DecoderLayer.moe_with_rmoe(self,x)
  Grok1ForCausalLM.__init__(self,config,quant_config,prefix)
  Grok1ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  Grok1ForCausalLM.get_num_params_analytical(self)
  Grok1ForCausalLM.get_num_params_torch(self)
  Grok1ForCausalLM.load_weight_wrapper(name,loaded_weight,*args,**kwargs)
  Grok1ForCausalLM.load_weights(self,weights,ignore_parent_name,check_hit_names,model_config)
  Grok1MLP.__init__(self,hidden_size,intermediate_size,layer_id,quant_config,prefix,reduce_results,use_presharded_weights,split_gate_up)
  Grok1MLP.forward(self,x)
  Grok1MoE.__init__(self,config,layer_id,num_experts,top_k,hidden_size,intermediate_size,params_dtype,quant_config,tp_size,reduce_results,use_presharded_weights,inplace,no_combine,prefix)
  Grok1MoE.forward(self,hidden_states)
  Grok1Model.__init__(self,config,quant_config,load_presharded_moe,load_presharded_embedding,load_presharded_attn,load_presharded_mlp,replicate_embedding,prefix)
  Grok1Model.forward(self,input_ids,positions,forward_batch,input_embeds)
  ScalingRotaryEmbedding.__init__(self,head_size,rotary_dim,max_position_embeddings,base,is_neox_style,scaling_factor,dtype,extra_method,extrapolation_factor,attn_factor,beta_fast,beta_slow)
  ScalingRotaryEmbedding._compute_cos_sin_cache(self)
  ScalingRotaryEmbedding._compute_inv_freq(self,scaling_factor)
  _prepare_presharded_weights(self,model_name_or_path,revision,fall_back_to_pt)
  _yarn_linear_ramp_mask(low,high,dim,dtype)
  get_rope_scaling(config)
models/hunyuan.py:
  HunYuanAttention.__init__(self,config,hidden_size,num_heads,num_kv_heads,rope_theta,rope_scaling,max_position_embeddings,quant_config,bias,prefix,attention_type,layer_id)
  HunYuanAttention.forward(self,positions,hidden_states,forward_batch,kv_states)
  HunYuanDecoderLayer.__init__(self,config,quant_config,prefix,layer_id)
  HunYuanDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual,kv_states)
  HunYuanMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,bias,prefix,reduce_results)
  HunYuanMLP.forward(self,x)
  HunYuanMoEV1ForCausalLM.__init__(self,config,quant_config)
  HunYuanMoEV1ForCausalLM._split_qkv_weight(self,qkv)
  HunYuanMoEV1ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  HunYuanMoEV1ForCausalLM.load_kv_cache_scales(self,quantization_param_path)
  HunYuanMoEV1ForCausalLM.load_weights(self,weights)
  HunYuanModel.__init__(self,config,quant_config,prefix)
  HunYuanModel.forward(self,input_ids,positions,forward_batch,input_embeds)
  HunYuanModel.get_input_embeddings(self,input_ids)
  HunYuanSparseMoeBlock.__init__(self,config,quant_config,layer_id)
  HunYuanSparseMoeBlock.forward(self,hidden_states)
  _get_cla_factor(config)
  _is_moe(config)
  check_head_dim(config)
  get_head_dim(config)
models/idefics2.py:
  Idefics2Encoder.__init__(self,config,quant_config,prefix)
  Idefics2Encoder.forward(self,inputs_embeds,cu_seqlens)
  Idefics2EncoderLayer.__init__(self,config,quant_config,prefix)
  Idefics2EncoderLayer.forward(self,hidden_states,cu_seqlens)
  Idefics2VisionEmbeddings.__init__(self,config)
  Idefics2VisionEmbeddings.forward(self,pixel_values,patch_attention_mask,tgt_sizes)
  Idefics2VisionEmbeddings.get_position_ids(self,pixel_values,patch_attention_mask,tgt_sizes)
  Idefics2VisionMLP.__init__(self,config,quant_config,prefix)
  Idefics2VisionMLP.forward(self,hidden_states)
  Idefics2VisionTransformer.__init__(self,config,quant_config,require_post_norm,prefix)
  Idefics2VisionTransformer.compute_cu_seqlens(self,tgt_sizes,input_embeds)
  Idefics2VisionTransformer.forward(self,pixel_values,patch_attention_mask,tgt_sizes)
  Idefics2VisionTransformer.get_input_embeddings(self)
models/internlm2.py:
  InternLM2Attention.__init__(self,hidden_size,num_heads,num_kv_heads,rope_theta,rope_scaling,max_position_embeddings,layer_id,quant_config,prefix)
  InternLM2Attention.forward(self,positions,hidden_states,forward_batch)
  InternLM2ForCausalLM.__init__(self,config,quant_config,prefix)
  InternLM2ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  InternLM2ForCausalLM.get_input_embeddings(self)
  InternLM2ForCausalLM.load_weights(self,weights)
  InternLM2MLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  InternLM2MLP.forward(self,x)
  InternLM2Model.__init__(self,config,quant_config,prefix)
  InternLM2Model.forward(self,input_ids,positions,forward_batch,input_embeds)
  InternLMDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  InternLMDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
models/internlm2_reward.py:
  InternLM2ForRewardModel.__init__(self,config,quant_config,prefix)
  InternLM2ForRewardModel.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  InternLM2ForRewardModel.load_weights(self,weights)
models/interns1.py:
  InternS1ForConditionalGeneration.__init__(self,config,quant_config,use_flash_attn)
  InternS1ForConditionalGeneration._mapping_interns1_name(self,name)
  InternS1ForConditionalGeneration.extract_feature(self,pixel_values)
  InternS1ForConditionalGeneration.forward(self,input_ids,positions,forward_batch,input_embeds)
  InternS1ForConditionalGeneration.get_image_feature(self,items)
  InternS1ForConditionalGeneration.load_weights(self,weights)
  InternS1ForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
  InternS1ForConditionalGeneration.pixel_shuffle(self,x,scale_factor)
models/internvl.py:
  InternAttention.__init__(self,config,quant_config)
  InternAttention.forward(self,hidden_states,cu_seqlens)
  InternMLP.__init__(self,config)
  InternMLP.forward(self,hidden_states)
  InternRMSNorm.__init__(self,hidden_size,eps)
  InternRMSNorm.forward(self,hidden_states)
  InternVLChatModel.__init__(self,config,quant_config,use_flash_attn)
  InternVLChatModel.extract_feature(self,pixel_values)
  InternVLChatModel.forward(self,input_ids,positions,forward_batch,input_embeds)
  InternVLChatModel.get_image_feature(self,items)
  InternVLChatModel.load_weights(self,weights)
  InternVLChatModel.pad_input_ids(self,input_ids,mm_inputs)
  InternVLChatModel.pixel_shuffle(self,x,scale_factor)
  InternVisionEmbeddings.__init__(self,config)
  InternVisionEmbeddings._get_pos_embed(self,pos_embed,H,W)
  InternVisionEmbeddings.forward(self,pixel_values)
  InternVisionEncoder.__init__(self,config,quant_config)
  InternVisionEncoder.forward(self,inputs_embeds,output_hidden_states,return_dict)
  InternVisionEncoderLayer.__init__(self,config,drop_path_rate,quant_config)
  InternVisionEncoderLayer.forward(self,hidden_states,cu_seqlens)
  InternVisionModel.__init__(self,config,quant_config)
  InternVisionModel.forward(self,pixel_values,output_hidden_states,return_dict,pixel_embeds)
  InternVisionModel.get_input_embeddings(self)
  InternVisionModel.resize_pos_embeddings(self,old_size,new_size,patch_size)
models/kimi_vl.py:
  KimiVLForConditionalGeneration.__init__(self,config,quant_config,prefix,**kwargs)
  KimiVLForConditionalGeneration.forward(self,input_ids,positions,forward_batch,get_embedding)
  KimiVLForConditionalGeneration.get_image_feature(self,items)
  KimiVLForConditionalGeneration.load_weights(self,weights)
  KimiVLForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
  KimiVLMultiModalProjector.__init__(self,config)
  KimiVLMultiModalProjector.forward(self,image_features)
  get_spec_layer_idx_from_weight_name(config,weight_name)
models/kimi_vl_moonvit.py:
  Learnable2DInterpPosEmb.__init__(self,height,width,dim,interpolation_mode)
  Learnable2DInterpPosEmb.forward(self,x,grid_hws)
  Learnable2DInterpPosEmb.reset_parameters(self)
  MLP2.__init__(self,dims,activation,bias)
  MLP2.forward(self,x)
  MoonVisionPatchEmbed.__init__(self,out_dim,in_dim,patch_size,pos_emb_height,pos_emb_width)
  MoonVisionPatchEmbed.forward(self,x,grid_hw)
  MoonVitEncoder.__init__(self,hidden_dim,num_layers,block_cfg)
  MoonVitEncoder.forward(self,hidden_states,grid_hw)
  MoonVitEncoderLayer.__init__(self,num_heads,hidden_dim,mlp_dim,attn_implementation,activation,attn_bias)
  MoonVitEncoderLayer.attention_qkvpacked(self,x,cu_seqlens,rope_freqs_cis)
  MoonVitEncoderLayer.forward(self,hidden_states,cu_seqlens,rope_freqs_cis)
  MoonVitPretrainedModel.__init__(self,config,*inputs,**kwargs)
  MoonVitPretrainedModel.forward(self,pixel_values,grid_hw)
  MoonVitVLProjector.__init__(self,in_channels,merge_kernel_size,hidden_act,ln_eps,out_dim)
  MoonVitVLProjector.forward(self,hidden_states)
  Rope2DPosEmb.__init__(self,dim,max_height,max_width,theta_base,device)
  Rope2DPosEmb.extra_repr(self)
  Rope2DPosEmb.get_freqs_cis_by_idx(self,pos_idx,pos_idx_mask)
  Rope2DPosEmb.get_freqs_cis_by_seqlens(self,grid_hws)
  Rope2DPosEmb.precomputed_freqs_cis(self)
  _apply_rope_input_validation(x,freqs_cis)
  apply_rope(xq,xk,freqs_cis)
  multihead_attention(q,k,v,q_cu_seqlens,k_cu_seqlens)
  patch_merger(x,grid_hw,merge_kernel_size)
  sdpa_attention(q,k,v,q_cu_seqlens,k_cu_seqlens)
models/llama.py:
  LlamaAttention.__init__(self,config,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,rope_is_neox_style,max_position_embeddings,quant_config,prefix,bias)
  LlamaAttention.forward(self,positions,hidden_states,forward_batch)
  LlamaDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  LlamaDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  LlamaForCausalLM.__init__(self,config,quant_config,prefix)
  LlamaForCausalLM._init_model(self,config,quant_config,prefix)
  LlamaForCausalLM.end_layer(self)
  LlamaForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding,pp_proxy_tensors)
  LlamaForCausalLM.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval,input_embeds)
  LlamaForCausalLM.get_embed(self)
  LlamaForCausalLM.get_embed_and_head(self)
  LlamaForCausalLM.get_input_embeddings(self)
  LlamaForCausalLM.get_module_name_from_weight_name(self,name)
  LlamaForCausalLM.get_num_params(self)
  LlamaForCausalLM.get_weights_by_name(self,name,truncate_size,tp_size)
  LlamaForCausalLM.load_kv_cache_scales(self,quantization_param_path)
  LlamaForCausalLM.load_weights(self,weights)
  LlamaForCausalLM.set_eagle3_layers_to_capture(self,layer_ids)
  LlamaForCausalLM.set_embed(self,embed)
  LlamaForCausalLM.set_embed_and_head(self,embed,head)
  LlamaForCausalLM.start_layer(self)
  LlamaMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix,reduce_results)
  LlamaMLP.forward(self,x,forward_batch,use_reduce_scatter)
  LlamaModel.__init__(self,config,quant_config,prefix)
  LlamaModel.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  LlamaModel.load_kv_cache_scales(self,quantization_param_path)
models/llama4.py:
  Llama4Attention.__init__(self,config,layer_id,hidden_size,num_heads,num_kv_heads,rope_theta,rope_scaling,max_position_embeddings,quant_config,bias,bias_o_proj,prefix)
  Llama4Attention._get_attn_scale(self,positions)
  Llama4Attention._mul_attn_scale(self,positions,q)
  Llama4Attention.forward(self,positions,hidden_states,forward_batch)
  Llama4DecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  Llama4DecoderLayer._is_moe_layer(self,layer_id)
  Llama4DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Llama4ForCausalLM.__init__(self,config,quant_config,prefix)
  Llama4ForCausalLM._init_model(self,config,quant_config,prefix)
  Llama4ForCausalLM.get_input_embeddings(self)
  Llama4MoE.__init__(self,config,layer_id,quant_config,prefix)
  Llama4MoE._forward_core(self,hidden_states,forward_mode)
  Llama4MoE._forward_core_normal(self,hidden_states)
  Llama4MoE._forward_core_shared_routed_overlap(self,hidden_states)
  Llama4MoE.custom_routing_function(hidden_states,gating_output,topk,renormalize)
  Llama4MoE.forward(self,hidden_states,forward_batch,use_reduce_scatter)
  Llama4Model.__init__(self,config,quant_config,prefix)
  Llama4Model.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  _get_or_create_alt_stream(device_module)
models/llama_classification.py:
  LlamaForClassification.__init__(self,config,quant_config,prefix)
  LlamaForClassification.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  LlamaForClassification.load_weights(self,weights)
models/llama_eagle.py:
  LlamaDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  LlamaForCausalLMEagle.__init__(self,config,quant_config,prefix)
  LlamaForCausalLMEagle.load_weights(self,weights)
  LlamaModel.__init__(self,config,quant_config,prefix)
  LlamaModel.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
models/llama_eagle3.py:
  LlamaDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  LlamaDecoderLayer.forward(self,positions,embeds,hidden_states,forward_batch,residual)
  LlamaForCausalLMEagle3.__init__(self,config,quant_config,prefix)
  LlamaForCausalLMEagle3.get_hot_token_id(self)
  LlamaForCausalLMEagle3.load_weights(self,weights)
  LlamaModel.__init__(self,config,quant_config,prefix)
  LlamaModel.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
models/llama_embedding.py:
  LlamaEmbeddingModel.__init__(self,config,quant_config,prefix)
  LlamaEmbeddingModel.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  LlamaEmbeddingModel.load_weights(self,weights)
models/llama_reward.py:
  LlamaForSequenceClassification.__init__(self,config,quant_config,prefix)
  LlamaForSequenceClassification.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  LlamaForSequenceClassification.load_weights(self,weights)
  LlamaForSequenceClassificationWithNormal_Weights.Weights.__init__(self,hidden_size,num_label)
  LlamaForSequenceClassificationWithNormal_Weights.Weights.forward(self,x)
  LlamaForSequenceClassificationWithNormal_Weights.__init__(self,config,quant_config,prefix)
  LlamaForSequenceClassificationWithNormal_Weights.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  LlamaForSequenceClassificationWithNormal_Weights.load_weights(self,weights)
models/llava.py:
  LlavaBaseForCausalLM.encode_images(self,pixel_values)
  LlavaBaseForCausalLM.forward(self,input_ids,positions,forward_batch)
  LlavaBaseForCausalLM.load_weights(self,weights)
  LlavaBaseForCausalLM.num_patches_per_side(self)
  LlavaBaseForCausalLM.pad_input_ids(self,input_ids,image_inputs)
  LlavaForConditionalGeneration.__init__(self,config,quant_config,prefix)
  LlavaForConditionalGeneration._config_cls_name_to_arch_name_mapping(self,auto_model_type)
  LlavaForConditionalGeneration._get_sgl_model_cls(self,config,auto_model_type)
  LlavaForConditionalGeneration.dtype(self)
  LlavaForConditionalGeneration.forward(self,input_ids,positions,forward_batch,get_embedding)
  LlavaForConditionalGeneration.get_image_feature(self,items)
  LlavaForConditionalGeneration.load_weights(self,weights)
  LlavaForConditionalGeneration.pad_input_ids(self,input_ids,image_inputs)
  LlavaLlamaForCausalLM.__init__(self,config,quant_config,prefix)
  LlavaMistralForCausalLM.__init__(self,config,quant_config,prefix)
  LlavaQwenForCausalLM.__init__(self,config,quant_config,prefix)
models/llavavid.py:
  LlavaVidForCausalLM.__init__(self,config,quant_config,prefix)
  LlavaVidForCausalLM.encode_images(self,pixel_values)
  LlavaVidForCausalLM.forward(self,input_ids,positions,forward_batch)
  LlavaVidForCausalLM.load_weights(self,weights)
  LlavaVidForCausalLM.num_patches_per_side(self)
  LlavaVidForCausalLM.pad_input_ids(self,input_ids,image_inputs)
models/longcat_flash.py:
  LongcatFlashDecoderLayer.__init__(self,config,layer_id,quant_config,prefix,alt_stream)
  LongcatFlashDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual,zero_allocator)
  LongcatFlashDecoderLayer.forward_mlp(self,hidden_states,positions,residual,forward_batch,zero_allocator)
  LongcatFlashForCausalLM.__init__(self,config,quant_config,prefix)
  LongcatFlashForCausalLM._weight_requant_ue8m0(self)
  LongcatFlashForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  LongcatFlashForCausalLM.get_embed_and_head(self)
  LongcatFlashForCausalLM.get_input_embeddings(self)
  LongcatFlashForCausalLM.get_model_config_for_expert_location(cls,config)
  LongcatFlashForCausalLM.load_weights(self,weights)
  LongcatFlashForCausalLM.post_load_weights(self,weight_names)
  LongcatFlashForCausalLM.set_embed_and_head(self,embed,head)
  LongcatFlashMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,reduce_results,prefix)
  LongcatFlashMLP.forward(self,x)
  LongcatFlashMoE.__init__(self,config,layer_id,quant_config,prefix)
  LongcatFlashMoE.forward(self,hidden_states)
  LongcatFlashMoE.get_moe_weights(self)
  LongcatFlashModel.__init__(self,config,quant_config,prefix)
  LongcatFlashModel.forward(self,input_ids,positions,forward_batch,input_embeds)
  LongcatFlashModel.get_input_embeddings(self)
  LongcatFlashRouter.__init__(self,config,zero_expert_num,rounter_params_dtype,prefix)
  LongcatFlashRouter.forward(self,hidden_states)
models/longcat_flash_nextn.py:
  LongcatFlashDenseDecoderLayer.__init__(self,config,layer_id,quant_config,prefix,alt_stream)
  LongcatFlashDenseDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual,zero_allocator)
  LongcatFlashForCausalLMNextN.__init__(self,config,quant_config)
  LongcatFlashForCausalLMNextN._weight_requant_ue8m0(self)
  LongcatFlashForCausalLMNextN.forward(self,input_ids,positions,forward_batch)
  LongcatFlashForCausalLMNextN.load_weights(self,weights)
  LongcatFlashForCausalLMNextN.post_load_weights(self)
  LongcatFlashModelNextN.__init__(self,config,quant_config,prefix)
  LongcatFlashModelNextN.forward(self,input_ids,positions,forward_batch,input_embeds)
  LongcatFlashModelNextN.get_input_embeddings(self)
models/mimo.py:
  MiMoForCausalLM.__init__(self,config,quant_config,prefix)
  MiMoForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  MiMoForCausalLM.get_embed_and_head(self)
  MiMoForCausalLM.get_input_embeddings(self,input_ids)
  MiMoForCausalLM.load_kv_cache_scales(self,quantization_param_path)
  MiMoForCausalLM.load_weights(self,weights)
  MiMoForCausalLM.set_embed_and_head(self,embed,head)
  MiMoModel.__init__(self,config,quant_config,prefix)
models/mimo_mtp.py:
  MiMoMTP.__init__(self,config,quant_config,prefix)
  MiMoMTP.forward(self,input_ids,positions,forward_batch)
  MiMoMTP.get_embed_and_head(self)
  MiMoMTP.load_weights(self,weights)
  MiMoMTP.map_model_name_to_mtp_param_name(self,name)
  MiMoMTP.set_embed_and_head(self,embed,head)
  MiMoMultiTokenPredictorLayer.__init__(self,config,prefix,quant_config)
  MiMoMultiTokenPredictorLayer.forward(self,input_ids,positions,forward_batch,input_embeds)
models/minicpm.py:
  MiniCPMAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,max_position_embeddings,quant_config,prefix)
  MiniCPMAttention.forward(self,positions,hidden_states,forward_batch)
  MiniCPMDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  MiniCPMDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  MiniCPMForCausalLM.__init__(self,config,quant_config,prefix)
  MiniCPMForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  MiniCPMForCausalLM.load_weights(self,weights)
  MiniCPMMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  MiniCPMMLP.forward(self,x)
  MiniCPMModel.__init__(self,config,quant_config,prefix)
  MiniCPMModel.forward(self,input_ids,positions,forward_batch,input_embeds)
models/minicpm3.py:
  MiniCPM3AttentionMLA.__init__(self,config,hidden_size,num_heads,qk_nope_head_dim,qk_rope_head_dim,v_head_dim,q_lora_rank,kv_lora_rank,rope_theta,rope_scaling,max_position_embeddings,quant_config,layer_id,prefix)
  MiniCPM3AttentionMLA.forward(self,positions,hidden_states,forward_batch)
  MiniCPM3DecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  MiniCPM3DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  MiniCPM3ForCausalLM.__init__(self,config,quant_config,prefix)
  MiniCPM3ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  MiniCPM3ForCausalLM.load_weights(self,weights)
  MiniCPM3MLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  MiniCPM3MLP.forward(self,x)
  MiniCPM3Model.__init__(self,config,quant_config,prefix)
  MiniCPM3Model.forward(self,input_ids,positions,forward_batch,input_embeds)
  input_to_float8(x,dtype)
models/minicpmo.py:
  ConditionalChatTTS.__init__(self,config)
  ConditionalChatTTS.decode_to_mel_specs(self,result_list)
  ConditionalChatTTS.generate(self,input_ids,past_key_values,temperature,eos_token,streaming_tts_text_mask,force_no_stop,min_new_token,max_new_token,logits_warpers,logits_processors,show_tqdm)
  ConditionalChatTTS.merge_inputs_embeds(self,input_ids,lm_spk_emb_last_hidden_states)
  ConditionalChatTTS.prefill_audio_ids(self,input_ids,past_key_values,streaming_tts_text_mask,add_audio_bos)
  ConditionalChatTTS.prefill_text(self,input_ids,position_ids,past_key_values,lm_spk_emb_last_hidden_states)
  ConvNeXtBlock.__init__(self,dim,intermediate_dim,kernel,dilation,layer_scale_init_value)
  ConvNeXtBlock.forward(self,x,cond)
  CustomRepetitionPenaltyLogitsProcessorRepeat.__call__(self,input_ids,scores)
  CustomRepetitionPenaltyLogitsProcessorRepeat.__init__(self,penalty,max_input_ids,past_window)
  DVAE.__init__(self)
  DVAE.forward(self,inp,mode)
  DVAEDecoder.__init__(self,idim,odim,n_layer,bn_dim,hidden,kernel,dilation,up)
  DVAEDecoder.forward(self,x,conditioning)
  GFSQ.__call__(self,x)
  GFSQ.__init__(self,dim,levels,G,R,eps,transpose)
  GFSQ._embed(self,x)
  GFSQ.forward(self,x)
  MiniCPMO.__init__(self,config,quant_config)
  MiniCPMO._get_feat_extract_output_lengths(self,input_lengths)
  MiniCPMO.forward(self,input_ids,positions,forward_batch,**kwargs)
  MiniCPMO.get_audio_embedding(self,items,chunk_length)
  MiniCPMO.get_audio_embedding_streaming(self,items)
  MiniCPMO.get_audio_feature(self,items)
  MiniCPMO.get_image_feature(self,items)
  MiniCPMO.get_omni_embedding(self,items,chunk_length,stream_input)
  MiniCPMO.init_audio_module(self)
  MiniCPMO.init_llm(self,config,quant_config,prefix)
  MiniCPMO.init_resampler(self,embed_dim,vision_dim,quant_config,prefix)
  MiniCPMO.init_tts_module(self)
  MiniCPMO.init_vision_module(self,config,quant_config,prefix)
  MiniCPMO.load_weights(self,weights)
  MiniCPMO.pad_input_ids(self,input_ids,mm_input)
  MiniCPMO.subsequent_chunk_mask(self,size,chunk_size,num_left_chunks,device,num_lookhead)
  MiniCPMWhisperEncoder.__init__(self,config)
  MiniCPMWhisperEncoder.forward(self,input_features,attention_mask,head_mask,output_attentions,output_hidden_states,return_dict,past_key_values,use_cache)
  MiniCPMWhisperEncoderLayer.__init__(self,config,layer_idx)
  MiniCPMWhisperEncoderLayer.forward(self,hidden_states,attention_mask,layer_head_mask,output_attentions,past_key_values,use_cache)
  MultiModalProjector.__init__(self,in_dim,out_dim)
  MultiModalProjector.forward(self,audio_features)
  apply_spk_emb(input_ids,spk_emb,input_embeds,spk_emb_token_id,num_spk_embs)
  make_streaming_chunk_mask_generation(inputs_embeds,past_seen_tokens,streaming_tts_text_mask,streaming_reserved_length,streaming_audio_chunk_size,streaming_text_chunk_size,num_spk_emb,use_spk_emb)
models/minicpmv.py:
  BaseResampler.__init__(self,num_queries,embed_dim,num_heads,kv_dim,norm_layer,do_post_projection,quant_config,prefix)
  BaseResampler._init_weights(self,m)
  BaseResampler._repeat(self,query,N)
  MiniCPMBaseModel.__init__(self,config,quant_config,prefix)
  MiniCPMBaseModel._get_image_bounds(self,input_ids,pad_values,im_start_id,im_end_id,slice_start_id,slice_end_id)
  MiniCPMBaseModel._parse_and_validate_inputs(self,input_ids,**kwargs)
  MiniCPMBaseModel.forward(self,input_ids,positions,forward_batch,**kwargs)
  MiniCPMBaseModel.get_embedding(self,input_ids,image_inputs)
  MiniCPMBaseModel.get_image_feature(self,items)
  MiniCPMBaseModel.get_input_embeddings(self)
  MiniCPMBaseModel.get_vision_embedding(self,pixel_values,patch_attn_mask,tgt_sizes)
  MiniCPMBaseModel.init_llm(self,config,quant_config,prefix)
  MiniCPMBaseModel.init_resampler(self,embed_dim,vision_dim,quant_config,prefix)
  MiniCPMBaseModel.init_vision_module(self,config,quant_config,prefix)
  MiniCPMV.__call__(self,*args,**kwargs)
  MiniCPMV.__getattr__(self,name)
  MiniCPMV.__init__(self,config,quant_config,prefix)
  MiniCPMV.load_weights(self,weights)
  MiniCPMV2_6.__init__(self,config,quant_config,prefix)
  MiniCPMV2_6.get_image_feature(self,items)
  MiniCPMV2_6.get_vision_embedding(self,pixel_values,patch_attn_mask,tgt_sizes)
  MiniCPMV2_6.init_llm(self,config,quant_config,prefix)
  MiniCPMV2_6.init_resampler(self,embed_dim,vision_dim,quant_config,prefix)
  MiniCPMV2_6.init_vision_module(self,config,quant_config,prefix)
  MiniCPMV2_6.pad_input_ids(self,input_ids,image_inputs)
  Resampler2_5.__init__(self,num_queries,embed_dim,num_heads,kv_dim,norm_layer,max_size,quant_config,prefix)
  Resampler2_5._adjust_pos_cache(self,tgt_sizes,device)
  Resampler2_5._set_2d_pos_cache(self,max_size,device)
  Resampler2_5.forward(self,x,tgt_sizes)
  get_1d_sincos_pos_embed_from_grid(embed_dim,pos,version)
  get_2d_sincos_pos_embed(embed_dim,grid_size,cls_token,version)
  get_2d_sincos_pos_embed_from_grid(embed_dim,grid,version)
  get_version_by_config(config)
models/mistral.py:
  Mistral3ForConditionalGeneration.__call__(self,*args,**kwargs)
  Mistral3ForConditionalGeneration.__getattr__(self,name)
  Mistral3ForConditionalGeneration.__hasattr__(self,name)
  Mistral3ForConditionalGeneration.__init__(self,**kwargs)
  Mistral3ForConditionalGeneration.get_image_feature(self,items)
models/mixtral.py:
  MixtralAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,max_position,rope_theta,quant_config,prefix)
  MixtralAttention.forward(self,positions,hidden_states,forward_batch)
  MixtralDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  MixtralDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  MixtralForCausalLM.__init__(self,config,quant_config,prefix)
  MixtralForCausalLM.end_layer(self)
  MixtralForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  MixtralForCausalLM.load_weights(self,weights)
  MixtralForCausalLM.start_layer(self)
  MixtralMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,params_dtype,quant_config,tp_size,prefix)
  MixtralMoE.forward(self,hidden_states)
  MixtralModel.__init__(self,config,quant_config,prefix)
  MixtralModel.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
models/mixtral_quant.py:
  MixtralAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,max_position,rope_theta,quant_config,prefix)
  MixtralAttention.forward(self,positions,hidden_states,forward_batch)
  MixtralDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  MixtralDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  MixtralMLP.__init__(self,num_experts,hidden_size,intermediate_size,quant_config,prefix)
  MixtralMLP.forward(self,hidden_states)
  MixtralMoE.__init__(self,config,quant_config,prefix)
  MixtralMoE.forward(self,hidden_states)
  MixtralModel.__init__(self,config,quant_config,prefix)
  MixtralModel.forward(self,input_ids,positions,forward_batch,input_embeds)
  QuantMixtralForCausalLM.__init__(self,config,quant_config,prefix)
  QuantMixtralForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  QuantMixtralForCausalLM.load_weights(self,weights)
models/mllama.py:
  ColumnParallelConv2dPatch.__init__(self,in_channels,out_channels,kernel_size,stride,bias)
  ColumnParallelConv2dPatch.forward(self,x)
  MllamaCrossAttentionDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  MllamaCrossAttentionDecoderLayer.forward(self,hidden_states,cross_attention_states,cross_attention_mask,full_text_row_masked_out_mask,forward_batch)
  MllamaForCausalLM.__init__(self,config,quant_config,prefix)
  MllamaForCausalLM.forward(self,input_ids,positions,cross_attention_states,cross_attention_mask,full_text_row_masked_out_mask,forward_batch,skip_cross_attention)
  MllamaForConditionalGeneration.__init__(self,config,quant_config,prefix)
  MllamaForConditionalGeneration._batch_image_inputs(self,forward_batch)
  MllamaForConditionalGeneration.flat_encoder_result(self,cross_attention_states,encoder_lens_need)
  MllamaForConditionalGeneration.forward(self,input_ids,positions,forward_batch)
  MllamaForConditionalGeneration.get_full_text_row_masked_out_mask(self,forward_batch)
  MllamaForConditionalGeneration.load_weights(self,weights)
  MllamaForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
  MllamaPrecomputedAspectRatioEmbedding.__init__(self,config,is_gated)
  MllamaPrecomputedAspectRatioEmbedding.forward(self,hidden_state,aspect_ratio_ids)
  MllamaPrecomputedPositionEmbedding.__init__(self,config)
  MllamaPrecomputedPositionEmbedding.forward(self,hidden_state,aspect_ratio_ids)
  MllamaTextCrossAttention.__init__(self,config,layer_id,quant_config,prefix)
  MllamaTextCrossAttention.forward(self,hidden_states,attention_mask,cross_attention_states,forward_batch)
  MllamaTextModel.__init__(self,config,quant_config,prefix)
  MllamaTextModel.forward(self,input_ids,positions,cross_attention_states,cross_attention_mask,full_text_row_masked_out_mask,forward_batch,skip_cross_attention)
  MllamaTextRMSNorm.__init__(self,hidden_size,eps)
  MllamaTextRMSNorm.extra_repr(self)
  MllamaTextRMSNorm.forward(self,hidden_states)
  MllamaVisionEncoder.__init__(self,config,quant_config,num_layers,is_gated,output_hidden_states,prefix)
  MllamaVisionEncoder.forward(self,hidden_states,attention_mask)
  MllamaVisionEncoderLayer.__init__(self,config,quant_config,is_gated,prefix)
  MllamaVisionEncoderLayer.forward(self,hidden_state,attention_mask)
  MllamaVisionMLP.__init__(self,config,quant_config,prefix)
  MllamaVisionMLP.forward(self,hidden_states)
  MllamaVisionModel.__init__(self,config,quant_config,prefix)
  MllamaVisionModel.apply_class_embedding(self,hidden_state)
  MllamaVisionModel.forward(self,pixel_values,aspect_ratio_ids,aspect_ratio_mask)
models/mllama4.py:
  Llama4ForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Llama4ForConditionalGeneration._check_vision_weights_in_index(self,index_file)
  Llama4ForConditionalGeneration._handle_default_weight(self,name,loaded_weight,params_dict)
  Llama4ForConditionalGeneration._handle_expert_scale_params(self,name,loaded_weight,params_dict,num_experts,loaded_params)
  Llama4ForConditionalGeneration._handle_expert_weight_params(self,name,loaded_weight,params_dict,num_experts,loaded_params)
  Llama4ForConditionalGeneration._handle_expert_weights(self,name,loaded_weight,expert_params_mapping,params_dict,num_experts,loaded_params)
  Llama4ForConditionalGeneration._handle_other_expert_params(self,name,loaded_weight,expert_params_mapping,params_dict,loaded_params)
  Llama4ForConditionalGeneration._handle_scale_remapping(self,name,params_dict)
  Llama4ForConditionalGeneration._handle_stacked_params(self,name,loaded_weight,stacked_params_mapping,params_dict,loaded_params)
  Llama4ForConditionalGeneration._has_vision_weights(self,config)
  Llama4ForConditionalGeneration._should_skip_weight(self,name)
  Llama4ForConditionalGeneration._transform_expert_name(self,name,is_weight)
  Llama4ForConditionalGeneration._transform_weight_name(self,name)
  Llama4ForConditionalGeneration.forward(self,input_ids,positions,forward_batch,**kwargs)
  Llama4ForConditionalGeneration.get_embed(self)
  Llama4ForConditionalGeneration.get_embed_and_head(self)
  Llama4ForConditionalGeneration.get_image_feature(self,items)
  Llama4ForConditionalGeneration.load_weights(self,weights)
  Llama4ForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
  Llama4ForConditionalGeneration.permute(w,n_heads)
  Llama4ForConditionalGeneration.permute_qk_weight_for_rotary(self,name,loaded_weight)
  Llama4ForConditionalGeneration.set_eagle3_layers_to_capture(self,layer_ids)
  Llama4ForConditionalGeneration.set_embed(self,embed)
  Llama4ForConditionalGeneration.set_embed_and_head(self,embed,head)
  Llama4UnfoldConvolution.__init__(self,config,quant_config,prefix,use_data_parallel)
  Llama4UnfoldConvolution.forward(self,hidden_states)
  Llama4VisionEncoder.__init__(self,config,quant_config,prefix,use_data_parallel)
  Llama4VisionEncoder.forward(self,hidden_states,freqs_ci)
  Llama4VisionEncoderLayer.__init__(self,config,quant_config,prefix,use_data_parallel)
  Llama4VisionEncoderLayer.forward(self,hidden_state,freqs_ci)
  Llama4VisionMLP.__init__(self,input_size,intermediate_size,output_size,bias,output_activation,quant_config,prefix,use_data_parallel)
  Llama4VisionMLP.forward(self,hidden_states)
  Llama4VisionModel.__init__(self,config,quant_config,prefix)
  Llama4VisionModel.forward(self,pixel_values)
  Llama4VisionPixelShuffleMLP.__init__(self,config,quant_config,prefix,use_data_parallel)
  Llama4VisionPixelShuffleMLP.forward(self,encoded_patches)
  Llama4VisionRotaryEmbedding.__init__(self,config)
  Llama4VisionRotaryEmbedding.forward(self,hidden_states)
  apply_position_embedding(q,k,freqs_ci,shape)
  pixel_shuffle(input_tensor,shuffle_ratio)
models/nemotron_nas.py:
  DeciLMDecoderLayer.__init__(self,config,layer_idx,quant_config,prefix)
  DeciLMDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  DeciLMForCausalLM.__init__(self,config,quant_config,prefix)
  DeciLMForCausalLM._init_model(self,config,quant_config,prefix)
  DeciLMForCausalLM.forward(self,input_ids,positions,forward_batch,inputs_embeds,get_embedding,pp_proxy_tensors)
  DeciLMForCausalLM.get_input_embeddings(self,input_ids)
  DeciLMForCausalLM.load_weights(self,weights)
  DeciModel.__init__(self,config,quant_config,prefix,layer_type)
  DeciModel.forward(self,input_ids,positions,forward_batch,inputs_embeds,pp_proxy_tensors)
  DeciModel.get_input_embeddings(self,input_ids)
  DeciModel.get_layer(idx,prefix)
  _ffn_mult_to_intermediate_size(ffn_mult,n_embd)
  _find_multiple(n,k)
models/olmo.py:
  OlmoAttention.__init__(self,config,layer_id,quant_config,prefix)
  OlmoAttention.forward(self,positions,hidden_states,forward_batch)
  OlmoDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  OlmoDecoderLayer.forward(self,positions,hidden_states,forward_batch)
  OlmoForCausalLM.__init__(self,config,quant_config,prefix)
  OlmoForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  OlmoForCausalLM.load_weights(self,weights)
  OlmoMLP.__init__(self,config,quant_config,prefix)
  OlmoMLP.forward(self,x)
  OlmoModel.__init__(self,config,quant_config,prefix)
  OlmoModel.forward(self,input_ids,positions,forward_batch,input_embeds)
models/olmo2.py:
  Olmo2Attention.__init__(self,config,layer_id,quant_config,prefix)
  Olmo2Attention._apply_qk_norm(self,q,k)
  Olmo2Attention.forward(self,positions,hidden_states,forward_batch)
  Olmo2DecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  Olmo2DecoderLayer.forward(self,positions,hidden_states,forward_batch)
  Olmo2ForCausalLM.__init__(self,config,quant_config,prefix)
  Olmo2ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  Olmo2ForCausalLM.load_weights(self,weights)
  Olmo2MLP.__init__(self,config,quant_config,prefix)
  Olmo2MLP.forward(self,x)
  Olmo2Model.__init__(self,config,quant_config,prefix)
  Olmo2Model.forward(self,input_ids,positions,forward_batch,input_embeds)
models/olmoe.py:
  OlmoeAttention.__init__(self,layer_id,hidden_size,num_heads,num_kv_heads,rope_theta,rope_scaling,max_position_embeddings,quant_config,prefix)
  OlmoeAttention.forward(self,positions,hidden_states,forward_batch)
  OlmoeDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  OlmoeDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  OlmoeForCausalLM.__init__(self,config,quant_config,prefix)
  OlmoeForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  OlmoeForCausalLM.load_weights(self,weights)
  OlmoeMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,params_dtype,quant_config,tp_size,layer_id,prefix)
  OlmoeMoE.forward(self,hidden_states)
  OlmoeModel.__init__(self,config,quant_config,prefix)
  OlmoeModel.forward(self,input_ids,positions,forward_batch,input_embeds)
models/persimmon.py:
  PersimmonAttention.__init__(self,config,quant_config,prefix,layer_id)
  PersimmonAttention._merge_heads(self,x)
  PersimmonAttention._split_heads(self,x)
  PersimmonAttention.forward(self,position_ids,forward_batch,hidden_states)
  PersimmonDecoderLayer.__init__(self,config,quant_config,prefix,idx)
  PersimmonDecoderLayer.forward(self,position_ids,forward_batch,hidden_states)
  PersimmonForCausalLM.__init__(self,config,quant_config,prefix)
  PersimmonForCausalLM.forward(self,input_ids,positions,forward_batch,inputs_embeds)
  PersimmonForCausalLM.get_input_embeddings(self,input_ids)
  PersimmonForCausalLM.load_weights(self,weights)
  PersimmonMLP.__init__(self,config,quant_config)
  PersimmonMLP.forward(self,hidden_states)
  PersimmonModel.__init__(self,config,quant_config,prefix)
  PersimmonModel.forward(self,input_ids,forward_batch,positions,inputs_embeds)
  PersimmonModel.get_input_embeddings(self,input_ids)
models/phi.py:
  PhiAttention.__init__(self,config,quant_config,prefix,layer_id)
  PhiAttention.forward(self,position_ids,forward_batch,hidden_states)
  PhiForCausalLM.__init__(self,config,quant_config,prefix)
  PhiForCausalLM.forward(self,input_ids,positions,forward_batch,inputs_embeds)
  PhiForCausalLM.get_input_embeddings(self,input_ids)
  PhiForCausalLM.load_weights(self,weights)
  PhiLayer.__init__(self,config,quant_config,prefix,idx)
  PhiLayer.forward(self,position_ids,forward_batch,hidden_states)
  PhiMLP.__init__(self,config,quant_config)
  PhiMLP.forward(self,hidden_states)
  PhiModel.__init__(self,config,quant_config,prefix)
  PhiModel.forward(self,input_ids,forward_batch,positions,inputs_embeds)
  PhiModel.get_input_embeddings(self,input_ids)
models/phi3_small.py:
  Phi3SmallDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  Phi3SmallDecoderLayer.forward(self,positions,hidden_states,forward_batch)
  Phi3SmallForCausalLM.__init__(self,config,quant_config,prefix)
  Phi3SmallForCausalLM.compute_logits(self,input_ids,hidden_states,sampling_metadata)
  Phi3SmallForCausalLM.forward(self,input_ids,positions,forward_batch,inputs_embeds,get_embedding)
  Phi3SmallForCausalLM.get_decoder(self)
  Phi3SmallForCausalLM.get_input_embeddings(self,input_ids)
  Phi3SmallForCausalLM.get_output_embeddings(self)
  Phi3SmallForCausalLM.load_weights(self,weights)
  Phi3SmallForCausalLM.set_decoder(self,decoder)
  Phi3SmallForCausalLM.set_input_embeddings(self,value)
  Phi3SmallForCausalLM.set_output_embeddings(self,value)
  Phi3SmallMLP.__init__(self,config,quant_config,prefix)
  Phi3SmallMLP.forward(self,x)
  Phi3SmallModel.__init__(self,config,quant_config,prefix)
  Phi3SmallModel.forward(self,input_ids,positions,forward_batch,inputs_embeds)
  Phi3SmallModel.get_input_embeddings(self,input_ids)
  Phi3SmallSelfAttention.__init__(self,config,layer_id,quant_config,prefix)
  Phi3SmallSelfAttention.forward(self,positions,hidden_states,forward_batch)
  gegelu(input,limit)
  quick_gelu(x)
models/phi4mm.py:
  Phi4MMForCausalLM.__init__(self,config,quant_config,prefix)
  Phi4MMForCausalLM._should_skip(name)
  Phi4MMForCausalLM.forward(self,input_ids,positions,forward_batch,**kwargs)
  Phi4MMForCausalLM.get_audio_feature(self,items)
  Phi4MMForCausalLM.get_image_feature(self,items)
  Phi4MMForCausalLM.load_weights(self,weights)
  Phi4MMForCausalLM.pad_input_ids(self,input_ids,mm_inputs)
  Phi4MMForCausalLM.should_apply_lora(self,module_name)
  Phi4MMImageEncoder.__init__(self,config,quant_config,prefix,model_dir)
  Phi4MMImageEncoder.forward(self,pixel_values,image_sizes,image_attention_mask)
  Phi4MMImageEncoder.get_img_features(self,img_embeds,attention_mask)
models/phi4mm_audio.py:
  AudioEmbedding.__init__(self,config,**kwargs)
  AudioEmbedding.forward(self,audio_features,audio_attention_mask,audio_projection_mode)
  AudioEmbedding.get_audio_features(self,input_embeds,audio_attention_mask,audio_projection_mode)
  AudioEmbedding.set_audio_embed_sizes(self,audio_embed_sizes)
  AudioEmbedding.set_audio_embeds(self,input_embeds)
  ConformerEncoder.__init__(self,input_size,chunk_size,left_chunk,num_lang,attention_dim,attention_heads,linear_units,num_blocks,dropout_rate,input_layer,causal,batch_norm,cnn_out,cnn_layer_norm,ext_pw_out_channel,ext_pw_kernel_size,depthwise_seperable_out_channel,depthwise_multiplier,chunk_se,kernel_size,activation,conv_activation,conv_glu_type,bias_in_glu,linear_glu_in_convm,attention_glu_type,export,extra_layer_output_idx,extra_multi_layer_output_idxs,activation_checkpointing,relative_attention_bias_args,time_reduction,use_pt_scaled_dot_product_attention,nemo_conv_settings,conv2d_extra_padding,replication_pad_for_subsample_embedding,attention_group_size,encoder_embedding_config)
  ConformerEncoder.calculate_hs_mask(self,xs_pad,device,mask)
  ConformerEncoder.forward(self,xs_pad,masks)
  ConformerEncoder.init_relative_attention_bias(self,input_tensor)
  ConformerEncoderLayer.__init__(self,d_model,ext_pw_out_channel,depthwise_seperable_out_channel,depthwise_multiplier,n_head,d_ffn,ext_pw_kernel_size,kernel_size,dropout_rate,causal,batch_norm,activation,chunk_se,chunk_size,conv_activation,conv_glu_type,bias_in_glu,linear_glu_in_convm,attention_inner_dim,attention_glu_type,activation_checkpointing,export,use_pt_scaled_dot_product_attention,attn_group_sizes)
  ConformerEncoderLayer.forward(self,x,pos_k,pos_v,mask,relative_attention_bias)
  TransformerEncoderBase.__init__(self,input_size,chunk_size,left_chunk,attention_dim,attention_heads,input_layer,cnn_out,cnn_layer_norm,time_reduction,dropout_rate,padding_idx,relative_attention_bias_args,positional_dropout_rate,nemo_conv_settings,conv2d_extra_padding,attention_group_size,encoder_embedding_config)
  TransformerEncoderBase._chunk_size_selection(self,chunk_size,left_chunk)
  TransformerEncoderBase._forward_embeddings_core(self,input_tensor,masks)
  TransformerEncoderBase._get_embed_class(self,embed)
  TransformerEncoderBase._position_embedding(self,input_tensor)
  TransformerEncoderBase._streaming_mask(self,seq_len,batch_size,chunk_size,left_chunk)
  TransformerEncoderBase.compute_lens_change(self,feature_lens)
  TransformerEncoderBase.forward(self)
  TransformerEncoderBase.forward_embeddings(self,xs_pad,masks,chunk_size_nc,left_chunk_nc)
  TransformerEncoderBase.get_offset(self)
  WindowQformer.__init__(self,window_size,num_queries,num_blocks,attention_dim,attention_heads,linear_units,dropout_rate,normalize_before)
  WindowQformer.forward(self,audio_embed,mask,embed_len)
models/phi4mm_utils.py:
  AbsolutePositionalEncoding.__init__(self,d_model,dropout_rate,max_len)
  AbsolutePositionalEncoding.extend_pe(self,x)
  AbsolutePositionalEncoding.forward(self,x)
  AttBlock.memory_dims(self,max_len)
  AttModule.__init__(self)
  AttModule.forward(self,x,memory,pos_emb,att_mask)
  AttModule.set_export(self,mode)
  BlockBase.__init__(self,input_size,output_size)
  CausalConv1D.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,groups,bias,padding_mode,device,dtype)
  CausalConv1D.forward(self,x,cache)
  CausalConv1D.update_cache(self,x,cache)
  CausalConv2D.__init__(self,in_channels,out_channels,kernel_size,stride,padding,dilation,groups,bias,padding_mode,device,dtype)
  CausalConv2D.forward(self,x)
  ConvModule.__init__(self,input_dim,ext_pw_out_channel,depthwise_seperable_out_channel,ext_pw_kernel_size,kernel_size,depthwise_multiplier,dropout_rate,causal,batch_norm,chunk_se,chunk_size,activation,glu_type,bias_in_glu,linear_glu_in_convm,export)
  ConvModule._add_ext_pw_layer(self)
  ConvModule.forward(self,x)
  DepthWiseSeperableConv1d.__init__(self,input_dim,depthwise_seperable_out_channel,kernel_size,depthwise_multiplier,padding)
  DepthWiseSeperableConv1d.forward(self,x)
  FeedForward.__init__(self,d_model,d_inner,dropout_rate,activation,bias_in_glu)
  FeedForward.forward(self,x)
  GLU.__init__(self,dim,act_name)
  GLU.forward(self,x)
  GLULinear.__init__(self,input_dim,output_dim,glu_type,bias_in_glu)
  GLULinear.forward(self,x)
  GLUPointWiseConv.__init__(self,input_dim,output_dim,kernel_size,glu_type,bias_in_glu,causal)
  GLUPointWiseConv.forward(self,x)
  MeanVarianceNormLayer.__init__(self,input_size)
  MeanVarianceNormLayer.forward(self,input_)
  MultiHeadedAttention.__init__(self,n_head,n_feat,dropout_rate,attention_inner_dim,glu_type,bias_in_glu,use_pt_scaled_dot_product_attention,n_value,group_size)
  MultiHeadedAttention.forward(self,query,key,value,pos_k,pos_v,mask,relative_attention_bias)
  MultiSequential.forward(self,*args)
  NemoConvSubsampling.__init__(self,feat_in,feat_out,subsampling_factor,subsampling,conv_channels,subsampling_conv_chunking_factor,activation,is_causal)
  NemoConvSubsampling.change_subsampling_conv_chunking_factor(self,subsampling_conv_chunking_factor)
  NemoConvSubsampling.channel_chunked_conv(self,conv,chunk_size,x)
  NemoConvSubsampling.conv_split_by_batch(self,x)
  NemoConvSubsampling.conv_split_by_channel(self,x)
  NemoConvSubsampling.forward(self,x,mask)
  NemoConvSubsampling.get_sampling_frames(self)
  NemoConvSubsampling.get_streaming_cache_size(self)
  NemoConvSubsampling.reset_parameters(self)
  Swish.__init__(self)
  Swish.forward(self,x)
  T5RelativeAttentionLogitBias.__init__(self,num_heads,num_buckets,max_distance,symmetric)
  T5RelativeAttentionLogitBias._bucket_relative_position(self,relative_position)
  T5RelativeAttentionLogitBias.forward(self,x)
  _pre_hook(state_dict,prefix,local_metadata,strict,missing_keys,unexpected_keys,error_msgs)
  adaptive_enc_mask(x_len,chunk_start_idx,left_window,right_window)
  calc_length(lengths,all_paddings,kernel_size,stride,ceil_mode,repeat_num)
  get_activation(name)
  get_offset(input_layer,time_reduction)
  masked_softmax(scores,mask)
  unfold_tensor(xs_pad,max_seq_len)
models/phimoe.py:
  PhiMoE.__init__(self,num_experts,top_k,hidden_size,intermediate_size,layer_id,quant_config,prefix)
  PhiMoE.forward(self,hidden_states,forward_batch)
  PhiMoEAttention.__init__(self,hidden_size,num_heads,num_kv_heads,head_dim,max_position,rope_theta,layer_id,attention_bias,quant_config,rope_scaling,prefix)
  PhiMoEAttention.forward(self,positions,hidden_states,forward_batch)
  PhiMoEConfig.__init__(self,vocab_size,hidden_size,intermediate_size,num_hidden_layers,num_attention_heads,num_key_value_heads,head_dim,hidden_act,max_position_embeddings,initializer_range,rms_norm_eps,use_cache,pad_token_id,bos_token_id,eos_token_id,tie_word_embeddings,rope_theta,sliding_window,attention_dropout,num_experts_per_tok,num_local_experts,output_router_logits,router_aux_loss_coef,router_jitter_noise,attention_bias,lm_head_bias,**kwargs)
  PhiMoEDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  PhiMoEDecoderLayer.forward(self,positions,hidden_states,residual,forward_batch)
  PhiMoEForCausalLM.__init__(self,config,quant_config,prefix)
  PhiMoEForCausalLM.forward(self,input_ids,positions,forward_batch,inputs_embeds,get_embedding)
  PhiMoEForCausalLM.load_weights(self,weights)
  PhiMoEModel.__init__(self,config,quant_config,prefix)
  PhiMoEModel.forward(self,input_ids,positions,forward_batch,input_embeds)
  phimoe_routing_function(hidden_states,gating_output,topk,renormalize)
  sparsemixer(scores,jitter_eps)
models/pixtral.py:
  PixtralHFMLP.__init__(self,config,quant_config,prefix)
  PixtralHFMLP.forward(self,x)
  PixtralHFTransformer.__init__(self,config,quant_config,num_hidden_layers_override,prefix)
  PixtralHFTransformer.forward(self,x,attention_mask,position_embeddings,return_all_hidden_states)
  PixtralHFTransformerBlock.__init__(self,config,layer_id,quant_config,prefix)
  PixtralHFTransformerBlock.forward(self,hidden_states,attention_mask,position_embeddings)
  PixtralHFVisionModel.__init__(self,config,quant_config,num_hidden_layers_override,prefix)
  PixtralHFVisionModel.device(self)
  PixtralHFVisionModel.dtype(self)
  PixtralHFVisionModel.forward(self,pixel_values,image_sizes,output_hidden_states,feature_sample_layers)
  PixtralHFVisionModel.load_weights(self,weights)
  PixtralHFVisionModel.pad_input_ids(self,input_ids,mm_inputs)
  resolve_visual_encoder_outputs(outputs,feature_sample_layers,post_norm,num_hidden_layers)
models/qwen.py:
  QWenAttention.__init__(self,hidden_size,num_heads,max_position_embeddings,layer_id,rope_theta,rope_scaling,quant_config,prefix)
  QWenAttention.forward(self,positions,hidden_states,forward_batch)
  QWenBlock.__init__(self,config,layer_id,quant_config,prefix)
  QWenBlock.forward(self,positions,hidden_states,forward_batch)
  QWenLMHeadModel.__init__(self,config,quant_config,prefix)
  QWenLMHeadModel.forward(self,input_ids,positions,forward_batch)
  QWenLMHeadModel.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval)
  QWenLMHeadModel.load_weights(self,weights)
  QWenMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  QWenMLP.forward(self,x)
  QWenModel.__init__(self,config,quant_config,prefix)
  QWenModel.forward(self,input_ids,positions,forward_batch)
models/qwen2.py:
  Qwen2Attention.__init__(self,hidden_size,num_heads,num_kv_heads,head_dim,layer_id,rope_theta,rope_scaling,max_position_embeddings,quant_config,dual_chunk_attention_config,prefix)
  Qwen2Attention.forward(self,positions,hidden_states,forward_batch)
  Qwen2DecoderLayer.__init__(self,config,layer_id,quant_config,prefix,alt_stream)
  Qwen2DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Qwen2ForCausalLM.__init__(self,config,quant_config,prefix)
  Qwen2ForCausalLM.end_layer(self)
  Qwen2ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding,pp_proxy_tensors)
  Qwen2ForCausalLM.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval,input_embeds)
  Qwen2ForCausalLM.get_embed_and_head(self)
  Qwen2ForCausalLM.get_input_embedding(self,input_ids)
  Qwen2ForCausalLM.get_input_embeddings(self)
  Qwen2ForCausalLM.load_kv_cache_scales(self,quantization_param_path)
  Qwen2ForCausalLM.load_weights(self,weights)
  Qwen2ForCausalLM.set_eagle3_layers_to_capture(self,layer_ids)
  Qwen2ForCausalLM.set_embed_and_head(self,embed,head)
  Qwen2ForCausalLM.start_layer(self)
  Qwen2MLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  Qwen2MLP.forward(self,x)
  Qwen2Model.__init__(self,config,quant_config,prefix,decoder_layer_type,alt_stream)
  Qwen2Model.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  Qwen2Model.get_input_embedding(self,input_ids)
  Qwen2Model.get_input_embeddings(self)
  Qwen2Model.load_kv_cache_scales(self,quantization_param_path)
models/qwen2_5_vl.py:
  Qwen2_5_VLForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Qwen2_5_VLForConditionalGeneration.forward(self,input_ids,positions,forward_batch,get_embedding)
  Qwen2_5_VLForConditionalGeneration.get_image_feature(self,items)
  Qwen2_5_VLForConditionalGeneration.get_input_embeddings(self)
  Qwen2_5_VLForConditionalGeneration.get_video_feature(self,items)
  Qwen2_5_VLForConditionalGeneration.load_weights(self,weights)
  Qwen2_5_VLForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
  Qwen2_5_VLMLP.__init__(self,in_features,hidden_features,bias,hidden_act,quant_config,prefix)
  Qwen2_5_VLMLP.forward(self,x)
  Qwen2_5_VisionBlock.__init__(self,dim,intermediate_dim,num_heads,hidden_act,norm_layer,attn_implementation,quant_config,prefix,num_dummy_heads)
  Qwen2_5_VisionBlock.forward(self,x,cu_seqlens,position_embeddings)
  Qwen2_5_VisionPatchMerger.__init__(self,dim,context_dim,spatial_merge_size,quant_config,prefix)
  Qwen2_5_VisionPatchMerger.forward(self,x)
  Qwen2_5_VisionTransformer.__init__(self,vision_config,norm_eps,quant_config,prefix)
  Qwen2_5_VisionTransformer.device(self)
  Qwen2_5_VisionTransformer.dtype(self)
  Qwen2_5_VisionTransformer.forward(self,x,grid_thw)
  Qwen2_5_VisionTransformer.get_window_index(self,grid_thw)
  Qwen2_5_VisionTransformer.rot_pos_emb(self,grid_thw)
models/qwen2_audio.py:
  Qwen2AudioForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Qwen2AudioForConditionalGeneration.forward(self,input_ids,positions,forward_batch,**kwargs)
  Qwen2AudioForConditionalGeneration.get_audio_feature(self,items)
  Qwen2AudioForConditionalGeneration.load_weights(self,weights)
  Qwen2AudioForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
models/qwen2_eagle.py:
  Qwen2DecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  Qwen2ForCausalLMEagle.__init__(self,config,quant_config,prefix)
  Qwen2ForCausalLMEagle.load_weights(self,weights)
  Qwen2Model.__init__(self,config,quant_config,prefix)
  Qwen2Model.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
models/qwen2_moe.py:
  Qwen2MoeAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,max_position_embeddings,qkv_bias,quant_config,dual_chunk_attention_config,prefix)
  Qwen2MoeAttention.forward(self,positions,hidden_states,forward_batch)
  Qwen2MoeDecoderLayer.__init__(self,config,layer_id,quant_config,prefix,alt_stream)
  Qwen2MoeDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Qwen2MoeForCausalLM.__init__(self,config,quant_config,prefix)
  Qwen2MoeForCausalLM.end_layer(self)
  Qwen2MoeForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  Qwen2MoeForCausalLM.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval,input_embeds)
  Qwen2MoeForCausalLM.get_model_config_for_expert_location(cls,config)
  Qwen2MoeForCausalLM.load_weights(self,weights)
  Qwen2MoeForCausalLM.set_eagle3_layers_to_capture(self,layer_ids)
  Qwen2MoeForCausalLM.start_layer(self)
  Qwen2MoeMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,reduce_results,prefix)
  Qwen2MoeMLP.forward(self,x,use_reduce_scatter)
  Qwen2MoeModel.__init__(self,config,quant_config,prefix,decoder_layer_type,alt_stream)
  Qwen2MoeModel.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  Qwen2MoeSparseMoeBlock.__init__(self,layer_id,config,quant_config,prefix)
  Qwen2MoeSparseMoeBlock.forward(self,hidden_states,forward_batch,use_reduce_scatter)
models/qwen2_rm.py:
  Qwen2ForRewardModel.__init__(self,config,quant_config,prefix)
  Qwen2ForRewardModel.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  Qwen2ForRewardModel.load_weights(self,weights)
models/qwen2_vl.py:
  Qwen2VLForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Qwen2VLForConditionalGeneration._process_video_input(self,video_input)
  Qwen2VLForConditionalGeneration.forward(self,input_ids,positions,forward_batch,get_embedding)
  Qwen2VLForConditionalGeneration.get_image_feature(self,items)
  Qwen2VLForConditionalGeneration.get_input_embeddings(self)
  Qwen2VLForConditionalGeneration.get_video_feature(self,items)
  Qwen2VLForConditionalGeneration.load_weights(self,weights)
  Qwen2VLForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
  Qwen2VisionBlock.__init__(self,dim,num_heads,mlp_ratio,act_layer,norm_layer,attn_implementation,quant_config,prefix)
  Qwen2VisionBlock.forward(self,x,cu_seqlens,position_embeddings)
  Qwen2VisionMLP.__init__(self,in_features,hidden_features,act_layer,quant_config,prefix)
  Qwen2VisionMLP.forward(self,x)
  Qwen2VisionPatchEmbed.__init__(self,patch_size,temporal_patch_size,in_chans,embed_dim)
  Qwen2VisionPatchEmbed.forward(self,x)
  Qwen2VisionPatchMerger.__init__(self,d_model,context_dim,norm_layer,spatial_merge_size,quant_config,prefix)
  Qwen2VisionPatchMerger.forward(self,x)
  Qwen2VisionRotaryEmbedding.__init__(self,dim,theta)
  Qwen2VisionRotaryEmbedding.forward(self,seqlen)
  Qwen2VisionRotaryEmbedding.update_freqs_cache(self,seqlen)
  Qwen2VisionTransformer.__init__(self,vision_config,norm_eps,quant_config,prefix)
  Qwen2VisionTransformer.device(self)
  Qwen2VisionTransformer.dtype(self)
  Qwen2VisionTransformer.forward(self,x,grid_thw)
  Qwen2VisionTransformer.rot_pos_emb(self,grid_thw)
models/qwen3.py:
  Qwen3Attention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,head_dim,max_position_embeddings,quant_config,rms_norm_eps,attention_bias,prefix,alt_stream)
  Qwen3Attention._apply_qk_norm(self,q,k)
  Qwen3Attention.forward(self,positions,hidden_states,forward_batch)
  Qwen3DecoderLayer.__init__(self,config,layer_id,quant_config,prefix,alt_stream)
  Qwen3DecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Qwen3ForCausalLM.__init__(self,config,quant_config,prefix)
  Qwen3ForCausalLM.end_layer(self)
  Qwen3ForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding,pp_proxy_tensors)
  Qwen3ForCausalLM.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval,input_embeds)
  Qwen3ForCausalLM.get_embed_and_head(self)
  Qwen3ForCausalLM.get_input_embeddings(self)
  Qwen3ForCausalLM.load_kv_cache_scales(self,quantization_param_path)
  Qwen3ForCausalLM.load_weights(self,weights)
  Qwen3ForCausalLM.set_eagle3_layers_to_capture(self,layer_ids)
  Qwen3ForCausalLM.set_embed_and_head(self,embed,head)
  Qwen3ForCausalLM.start_layer(self)
  Qwen3Model.__init__(self,config,quant_config,prefix)
models/qwen3_classification.py:
  Qwen3ForSequenceClassification.__init__(self,config,quant_config,prefix)
  Qwen3ForSequenceClassification.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  Qwen3ForSequenceClassification.load_weights(self,weights)
models/qwen3_moe.py:
  Qwen3MoeAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,max_position_embeddings,head_dim,rms_norm_eps,attention_bias,quant_config,prefix,dual_chunk_attention_config,alt_stream)
  Qwen3MoeAttention._apply_qk_norm(self,q,k)
  Qwen3MoeAttention.forward(self,positions,hidden_states,forward_batch)
  Qwen3MoeAttention.forward_core(self,intermediate_state)
  Qwen3MoeAttention.forward_prepare(self,positions,hidden_states,forward_batch)
  Qwen3MoeAttention.op_core(self,state)
  Qwen3MoeAttention.op_prepare(self,state)
  Qwen3MoeDecoderLayer.__init__(self,config,layer_id,quant_config,prefix,alt_stream)
  Qwen3MoeDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Qwen3MoeDecoderLayer.op_comm_postprocess_layer(self,state)
  Qwen3MoeDecoderLayer.op_comm_prepare_attn(self,state,positions,hidden_states,forward_batch,residual,tbo_subbatch_index)
  Qwen3MoeDecoderLayer.op_comm_prepare_mlp(self,state)
  Qwen3MoeDecoderLayer.op_mlp(self,state)
  Qwen3MoeForCausalLM.__init__(self,config,quant_config,prefix)
  Qwen3MoeForCausalLM.end_layer(self)
  Qwen3MoeForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,pp_proxy_tensors)
  Qwen3MoeForCausalLM.forward_split_prefill(self,input_ids,positions,forward_batch,split_interval,input_embeds)
  Qwen3MoeForCausalLM.get_embed_and_head(self)
  Qwen3MoeForCausalLM.get_input_embeddings(self)
  Qwen3MoeForCausalLM.get_model_config_for_expert_location(cls,config)
  Qwen3MoeForCausalLM.load_weights(self,weights)
  Qwen3MoeForCausalLM.set_eagle3_layers_to_capture(self,layer_ids)
  Qwen3MoeForCausalLM.start_layer(self)
  Qwen3MoeModel.__init__(self,config,quant_config,prefix)
  Qwen3MoeSparseMoeBlock.__init__(self,layer_id,config,quant_config,prefix)
  Qwen3MoeSparseMoeBlock.forward(self,hidden_states,forward_batch,use_reduce_scatter)
  Qwen3MoeSparseMoeBlock.forward_deepep(self,hidden_states,forward_batch)
  Qwen3MoeSparseMoeBlock.forward_normal(self,hidden_states,use_reduce_scatter)
  Qwen3MoeSparseMoeBlock.get_moe_weights(self)
  Qwen3MoeSparseMoeBlock.op_combine_a(self,state)
  Qwen3MoeSparseMoeBlock.op_combine_b(self,state)
  Qwen3MoeSparseMoeBlock.op_dispatch_a(self,state)
  Qwen3MoeSparseMoeBlock.op_dispatch_b(self,state)
  Qwen3MoeSparseMoeBlock.op_experts(self,state)
  Qwen3MoeSparseMoeBlock.op_gate(self,state)
  Qwen3MoeSparseMoeBlock.op_output(self,state)
  Qwen3MoeSparseMoeBlock.op_select_experts(self,state)
models/registry.py:
  _ModelRegistry._normalize_archs(self,architectures)
  _ModelRegistry._raise_for_unsupported(self,architectures)
  _ModelRegistry._try_load_model_cls(self,model_arch)
  _ModelRegistry.get_supported_archs(self)
  _ModelRegistry.resolve_model_cls(self,architectures)
  import_model_classes()
models/roberta.py:
  RobertaClassificationHead.__init__(self,config)
  RobertaClassificationHead.forward(self,features,**kwargs)
  RobertaEmbedding.__init__(self,config)
  RobertaEmbedding.forward(self,input_ids,seq_lens,position_ids,forward_batch)
  XLMRobertaBaseModel.__init__(self,config,quant_config,prefix,add_pooling_layer)
  XLMRobertaBaseModel.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  XLMRobertaBaseModel.load_weights(self,weights)
  XLMRobertaForSequenceClassification.__init__(self,config,quant_config,prefix)
  XLMRobertaForSequenceClassification.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  XLMRobertaForSequenceClassification.load_weights(self,weights)
  XLMRobertaForSequenceClassification.weight_filter()
  XLMRobertaModel.__init__(self,config,quant_config,prefix)
  XLMRobertaModel.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  XLMRobertaModel.load_weights(self,weights)
  create_position_ids_from_input_ids(input_ids,padding_idx,past_key_values_length)
models/siglip.py:
  SiglipEncoder.__init__(self,config,quant_config,prefix)
  SiglipEncoder.forward(self,inputs_embeds,attention_mask,causal_attention_mask,return_all_hidden_states)
  SiglipEncoderLayer.__init__(self,config,act_layer,norm_layer,attn_implementation,quant_config,prefix)
  SiglipEncoderLayer.forward(self,hidden_states,attention_mask,causal_attention_mask)
  SiglipMLP.__init__(self,config,act_layer,quant_config,prefix)
  SiglipMLP.forward(self,x)
  SiglipVisionEmbeddings.__init__(self,config)
  SiglipVisionEmbeddings.forward(self,pixel_values)
  SiglipVisionModel.__init__(self,config,quant_config,prefix)
  SiglipVisionModel.device(self)
  SiglipVisionModel.forward(self,pixel_values)
  SiglipVisionTransformer.__init__(self,config,quant_config,prefix)
  SiglipVisionTransformer.device(self)
  SiglipVisionTransformer.forward(self,pixel_values)
models/stablelm.py:
  StableLMEpochModel.__init__(self,config,quant_config,prefix)
  StableLMEpochModel.forward(self,input_ids,positions,forward_batch,input_embeds)
  StableLmForCausalLM.__init__(self,config,quant_config,prefix)
  StableLmForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  StableLmForCausalLM.load_weights(self,weights)
  StablelmAttention.__init__(self,config,layer_id,quant_config,prefix)
  StablelmAttention.forward(self,positions,hidden_states,forward_batch)
  StablelmDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  StablelmDecoderLayer.forward(self,positions,hidden_states,forward_batch)
  StablelmMLP.__init__(self,config,quant_config,prefix)
  StablelmMLP.forward(self,x)
models/step3_vl.py:
  Step3TextAttention.__init__(self,hidden_size,num_heads,num_kv_heads,head_dim,share_q_dim,layer_id,rope_theta,rope_scaling,max_position_embeddings,quant_config,rms_norm_eps,prefix)
  Step3TextAttention.forward(self,positions,hidden_states,forward_batch)
  Step3TextDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  Step3TextDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  Step3TextDecoderLayer.moe_mlp_forward(self,hidden_states)
  Step3TextMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  Step3TextMLP.forward(self,x)
  Step3TextMoEMLP.__init__(self,layer_id,config,quant_config,prefix)
  Step3TextMoEMLP.forward(self,hidden_states)
  Step3TextModel.__init__(self,config,quant_config,prefix)
  Step3TextModel.forward(self,input_ids,positions,forward_batch,input_embeds)
  Step3TextModel.get_input_embeddings(self)
  Step3VLForConditionalGeneration.__init__(self,config,quant_config,prefix)
  Step3VLForConditionalGeneration._flatten_embeddings(self,embeddings)
  Step3VLForConditionalGeneration._get_vision_model_output(self,input_tensor)
  Step3VLForConditionalGeneration._process_image_features(self,image_features)
  Step3VLForConditionalGeneration.forward(self,input_ids,positions,forward_batch,input_embeds)
  Step3VLForConditionalGeneration.get_image_feature(self,items)
  Step3VLForConditionalGeneration.get_model_config_for_expert_location(cls,config)
  Step3VLForConditionalGeneration.load_weights(self,weights)
  Step3VLForConditionalGeneration.match_expert_and_shard_ids(name_path,weight_path)
  Step3VLForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
  Step3VisionAttention.__init__(self,dim,num_heads,qkv_backend,quant_config,prefix)
  Step3VisionAttention.forward(self,hidden_states)
  Step3VisionEmbeddings.__init__(self,config)
  Step3VisionEmbeddings.forward(self,pixel_values)
  Step3VisionEncoder.__init__(self,config)
  Step3VisionEncoder.forward(self,inputs_embeds)
  Step3VisionEncoderLayer.__init__(self,config,attn_implementation)
  Step3VisionEncoderLayer.forward(self,hidden_states)
  Step3VisionMLP.__init__(self,dim,intermediate_size,bias,hidden_act,quant_config,prefix)
  Step3VisionMLP.forward(self,hidden_states)
  Step3VisionTransformer.__init__(self,config)
  Step3VisionTransformer.dtype(self)
  Step3VisionTransformer.forward(self,pixel_values)
  get_abs_pos(abs_pos,tgt_size)
models/torch_native_llama.py:
  LlamaAttention.__init__(self,config,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,rope_is_neox_style,max_position_embeddings,quant_config,prefix)
  LlamaAttention.forward(self,positions,hidden_states,forward_batch)
  LlamaDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  LlamaDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  LlamaMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  LlamaMLP.forward(self,x)
  LlamaModel.__init__(self,config,quant_config)
  LlamaModel.forward(self,input_ids,positions,forward_batch,input_embeds)
  TorchNativeLlamaForCausalLM.__init__(self,config,quant_config)
  TorchNativeLlamaForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  TorchNativeLlamaForCausalLM.get_module_name_from_weight_name(self,name)
  TorchNativeLlamaForCausalLM.get_num_params(self)
  TorchNativeLlamaForCausalLM.load_weights(self,weights)
  TorchNativeLlamaForCausalLM.load_weights_to_module(self,fqn,weights)
  gate_up_proj_weight_loader(self,param,loaded_weight,loaded_shard_id)
  qkv_proj_weight_loader(self,param,loaded_weight,loaded_shard_id)
models/transformers.py:
  HFColumnParallelLinear.forward(self,input)
  HFCompatibleLinear.forward(self,input)
  HFCompatibleLinear.parent_cls(self)
  HFRowParallelLinear.forward(self,input)
  TransformersForCausalLM.__init__(self,config,quant_config,prefix)
  TransformersForCausalLM._tensor_parallel(module,prefix)
  TransformersForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds,get_embedding)
  TransformersForCausalLM.load_weights(self,weights)
  TransformersForCausalLM.log_replacement(self,name,old_module,new_module)
  TransformersForCausalLM.replace_vocab_embed_class(self,module)
  TransformersForCausalLM.tensor_parallel(self,tp_size)
  maybe_prefix(prefix,name)
  replace_linear_class(linear,style,quant_config)
  sglang_flash_attention_forward(module,query,key,value,attention_mask,forward_batch,scaling,attention_instances,**kwargs)
models/vila.py:
  DownSample3x3BlockFix.forward(self,x)
  MultimodalProjector.__init__(self,config,*args,**kwargs)
  MultimodalProjector.device(self)
  MultimodalProjector.dtype(self)
  MultimodalProjector.forward(self,x)
  VILAConfig.__init__(self,text_config,vision_config,hidden_size,image_token_id,mm_hidden_size,mm_projector_type,mm_vision_select_feature,mm_vision_select_layer,video_token_id,**kwargs)
  VILAForConditionalGeneration.__init__(self,config,quant_config,prefix)
  VILAForConditionalGeneration._vision_tower_output_to_mm_projector_input(self,vision_tower_output)
  VILAForConditionalGeneration.dtype(self)
  VILAForConditionalGeneration.forward(self,input_ids,positions,forward_batch,get_embedding)
  VILAForConditionalGeneration.get_image_feature(self,mm_input)
  VILAForConditionalGeneration.load_weights(self,weights)
  VILAForConditionalGeneration.pad_input_ids(self,input_ids,mm_inputs)
models/xverse.py:
  XverseAttention.__init__(self,config,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,rope_is_neox_style,max_position_embeddings,quant_config,prefix)
  XverseAttention.forward(self,positions,hidden_states,forward_batch)
  XverseDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  XverseDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  XverseForCausalLM.__init__(self,config,quant_config,prefix)
  XverseForCausalLM.forward(self,input_ids,positions,forward_batch,input_embeds)
  XverseForCausalLM.load_weights(self,weights,name,loaded_weight)
  XverseForCausalLM.load_weights_per_param(name,loaded_weight)
  XverseMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,prefix)
  XverseMLP.forward(self,x)
  XverseModel.__init__(self,config,quant_config,prefix)
  XverseModel.forward(self,input_ids,positions,forward_batch,input_embeds)
models/xverse_moe.py:
  XverseAttention.__init__(self,hidden_size,num_heads,num_kv_heads,layer_id,rope_theta,rope_scaling,max_position_embeddings,quant_config,prefix)
  XverseAttention.forward(self,positions,hidden_states,forward_batch)
  XverseDecoderLayer.__init__(self,config,layer_id,quant_config,prefix)
  XverseDecoderLayer.forward(self,positions,hidden_states,forward_batch,residual)
  XverseMLP.__init__(self,hidden_size,intermediate_size,hidden_act,quant_config,reduce_results,prefix)
  XverseMLP.forward(self,x)
  XverseMoE.__init__(self,config,quant_config,prefix)
  XverseMoE.forward(self,hidden_states)
  XverseMoE.pack_params(self)
  XverseModel.__init__(self,config,quant_config,prefix)
  XverseModel.forward(self,input_ids,positions,forward_batch)
  XverseMoeForCausalLM.__init__(self,config,quant_config,prefix)
  XverseMoeForCausalLM.forward(self,input_ids,positions,forward_batch)
  XverseMoeForCausalLM.load_weights(self,weights)
models/yivl.py:
  YiVLForCausalLM.__init__(self,config,quant_config,prefix)
  YiVLForCausalLM.load_weights(self,weights)
  YiVLMultiModalProjector.__init__(self,config)
  YiVLMultiModalProjector.forward(self,image_features)
multimodal/mm_utils.py:
  divide_to_patches(image,patch_size)
  expand2square(pil_img,background_color)
  get_anyres_image_grid_shape(image_size,grid_pinpoints,patch_size)
  has_valid_data(data)
  load_image_from_base64(image)
  process_anyres_image(image,processor,grid_pinpoints)
  process_images(images,image_processor,model_cfg)
  resize_and_pad_image(image,target_resolution)
  select_best_resolution(original_size,possible_resolutions)
  unpad_image(tensor,original_size)
  unpad_image_shape(current_height,current_width,original_size)
multimodal/processors/base_processor.py:
  BaseMultiModalProcessorOutput.organize_results(self)
  BaseMultimodalProcessor.__init__(self,hf_config,server_args,_processor,transport_mode,*args,**kwargs)
  BaseMultimodalProcessor._load_single_item(data,modality,frame_count_limit,audio_sample_rate,discard_alpha_channel)
  BaseMultimodalProcessor._process_and_collect_mm_items(self,input_text,images,audios,videos,**kwargs)
  BaseMultimodalProcessor.collect_mm_items_from_processor_output(self,data_dict)
  BaseMultimodalProcessor.get_estimated_frames_list(self,image_data)
  BaseMultimodalProcessor.get_mm_items_offset(input_ids,mm_token_id)
  BaseMultimodalProcessor.get_mm_items_offset_by_pair(input_ids,mm_start_id,mm_end_id)
  BaseMultimodalProcessor.load_mm_data(self,prompt,multimodal_tokens,image_data,video_data,audio_data,return_text,discard_alpha_channel,audio_sample_rate)
  BaseMultimodalProcessor.process_and_combine_mm_data(self,base_output,mm_tokens,**kwargs)
  BaseMultimodalProcessor.process_mm_data(self,input_text,images,videos,audios,**kwargs)
  BaseMultimodalProcessor.submit_data_loading_tasks(self,text_parts,multimodal_tokens,data_iterators,discard_alpha_channel,image_estimated_frames_iter,image_scaling_factor,max_image_frames,audio_sample_rate)
  MultimodalSpecialTokens.build(self,processor)
  MultimodalSpecialTokens.convert_to_str(self,token,processor)
  MultimodalSpecialTokens.convert_to_strs(self,processor)
  MultimodalSpecialTokens.get_combined_regex(self)
  MultimodalSpecialTokens.get_modality_of_token(self,token)
  MultimodalSpecialTokens.get_token_id_by_modality(self,modality)
  MultimodalSpecialTokens.parse_regex(self)
  async BaseMultimodalProcessor.process_mm_data_async(self,image_data,audio_data,input_text,request_obj,**kwargs)
multimodal/processors/clip.py:
  ClipImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async ClipImageProcessor.process_mm_data_async(self,image_data,input_text,*args,**kwargs)
multimodal/processors/deepseek_vl_v2.py:
  DeepseekVL2ImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async DeepseekVL2ImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,max_req_input_len,*args,**kwargs)
multimodal/processors/gemma3.py:
  Gemma3SGLangImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Gemma3SGLangImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/gemma3n.py:
  Gemma3nSGLangProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Gemma3nSGLangProcessor.process_mm_data_async(self,image_data,audio_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/glm4v.py:
  Glm4vImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Glm4vImageProcessor.preprocess_video(self,vr)
  async Glm4vImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/internvl.py:
  InternVLImageProcessor.__init__(self,hf_config,server_args,_image_processor,*args,**kwargs)
  InternVLImageProcessor.build_transform(input_size)
  InternVLImageProcessor.dynamic_preprocess(image,min_num,max_num,image_size,use_thumbnail)
  InternVLImageProcessor.find_closest_aspect_ratio(aspect_ratio,target_ratios,width,height,image_size)
  InternVLImageProcessor.get_index(bound,fps,max_frame,first_idx,num_segments)
  InternVLImageProcessor.load_video(video_path,bound,input_size,max_num,num_segments)
  InternVLImageProcessor.normalize(tensor,mean,std)
  InternVLImageProcessor.process_image_internvl(image,input_size,max_num)
  InternVLImageProcessor.resize_image(img,size)
  InternVLImageProcessor.to_tensor(img)
  InternVLImageProcessor.transform(img)
  async InternVLImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,**kwargs)
multimodal/processors/janus_pro.py:
  JanusProImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async JanusProImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,**kwargs)
multimodal/processors/kimi_vl.py:
  KimiVLImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async KimiVLImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/llava.py:
  LlavaImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  LlavaImageProcessor._process_single_image_task(image_data,image_aspect_ratio,image_grid_pinpoints,processor)
  LlavaMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  LlavaMultimodalProcessor._get_sgl_processor_cls(self,model_type)
  async LlavaImageProcessor._process_single_image(self,image_data,aspect_ratio,grid_pinpoints)
  async LlavaImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
  async LlavaMultimodalProcessor.process_mm_data_async(self,*args,**kwargs)
multimodal/processors/minicpm.py:
  MiniCPMMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async MiniCPMMultimodalProcessor.process_mm_data_async(self,image_data,audio_data,input_text,request_obj,**kwargs)
multimodal/processors/mlama.py:
  MllamaImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async MllamaImageProcessor.process_mm_data_async(self,image_data,input_text,*args,**kwargs)
multimodal/processors/mllama4.py:
  Mllama4ImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Mllama4ImageProcessor.process_mm_data_async(self,image_data,input_text,*args,**kwargs)
multimodal/processors/phi4mm.py:
  Phi4MMMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  Phi4MMProcessorAdapter.__call__(self,**kwargs)
  Phi4MMProcessorAdapter.__init__(self,_processor)
  async Phi4MMMultimodalProcessor.process_mm_data_async(self,image_data,audio_data,input_text,request_obj,**kwargs)
multimodal/processors/pixtral.py:
  PixtralProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  PixtralProcessor.get_patch_grid_size(self,image_width,image_height)
  async PixtralProcessor._resize(self,image)
  async PixtralProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/qwen_audio.py:
  Qwen2AudioMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Qwen2AudioMultimodalProcessor.process_mm_data_async(self,audio_data,input_text,**kwargs)
multimodal/processors/qwen_vl.py:
  Qwen2_5VLImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async Qwen2_5VLImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
  async preprocess_video(vr,image_factor)
  async resize_image_async(image)
  ceil_by_factor(number,factor)
  floor_by_factor(number,factor)
  resize_image(image,size_factor)
  round_by_factor(number,factor)
  smart_nframes(ele,total_frames,video_fps)
  smart_resize(height,width,factor,min_pixels,max_pixels)
multimodal/processors/step3_vl.py:
  GPUToTensor.forward(self,raw_image)
  ImagePatcher.__call__(self,img)
  ImagePatcher.determine_window_size(self,long,short)
  ImagePatcher.get_image_size_for_crop(self,img_width,img_height,window_size)
  ImagePatcher.get_image_size_for_padding(self,img_width,img_height)
  ImagePatcher.get_image_size_for_preprocess(self,img_width,img_height)
  ImagePatcher.get_num_patches(self,img_width,img_height)
  ImagePatcher.patch_crop(self,img,i,j,th,tw)
  ImagePatcher.slide_window(self,width,height,sizes,steps,img_rate_thr)
  ImagePatcher.square_pad(self,img)
  Step3VLImageProcessor.__call__(self,image)
  Step3VLImageProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  Step3VLImageProcessor.preprocess(self,image)
  Step3VLProcessor.__call__(self,text,images,return_tensors,*args,**kwargs)
  Step3VLProcessor.__init__(self,config,tokenizer)
  Step3VLProcessor._convert_images_to_pixel_values(self,images,is_patch)
  Step3VLProcessor._get_image_repl(self,num_images)
  Step3VLProcessor._get_image_repl_features(self,num_images,num_patches,patch_new_line_idx)
  Step3VLProcessor._get_patch_repl(self,num_patches,patch_newline_mask)
  Step3VLProcessor._split_images(self,images)
  Step3VLProcessor.get_num_image_tokens(self,img_width,img_height)
  Step3VLProcessor.image_token_id(self)
  Step3VLProcessor.replace_placeholder(self,text,placeholder,repls)
  Step3VisionProcessor.__call__(self,image,is_patch)
  Step3VisionProcessor.__init__(self,size,interpolation_mode,patch_size)
  async Step3VLImageProcessor.process_mm_data_async(self,image_data,input_text,request_obj,*args,**kwargs)
multimodal/processors/vila.py:
  VILAMultimodalProcessor.__init__(self,hf_config,server_args,_processor,*args,**kwargs)
  async VILAMultimodalProcessor.process_mm_data_async(self,image_data,input_text,request_obj,**kwargs)
offloader.py:
  BaseOffloader.post_init(self)
  BaseOffloader.wrap_modules(self,all_modules_generator,submodule_accessor,whitelist_param_names_creator)
  OffloaderV1.__init__(self,cpu_offload_max_bytes)
  OffloaderV1.forward(*args,**kwargs)
  OffloaderV1.maybe_offload_to_cpu(self,module)
  OffloaderV1.wrap_modules(self,all_modules_generator,submodule_accessor,whitelist_param_names_creator)
  OffloaderV2.__init__(self,group_size,num_in_group,prefetch_step,mode,dp_rank,dp_size)
  OffloaderV2.post_init(self)
  OffloaderV2.wrap_modules(self,all_modules_generator,submodule_accessor,whitelist_param_names_creator)
  _BaseParamOffloader.__init__(self,module,param_name)
  _BaseParamOffloader._param(self)
  _BaseParamOffloader.create(mode,**kwargs)
  _BaseParamOffloader.create_device_tensor(self)
  _BaseParamOffloader.post_init(self)
  _CpuParamOffloader.__init__(self,module,param_name)
  _CpuParamOffloader.create_device_tensor(self)
  _MetaParamOffloader.__init__(self,module,param_name)
  _MetaParamOffloader.create_device_tensor(self)
  _ModuleOffloader.__init__(self,mode,module,alt_stream,whitelist_param_names)
  _ModuleOffloader._create_device_tensors(self)
  _ModuleOffloader.offload(self)
  _ModuleOffloader.post_init(self)
  _ModuleOffloader.start_onload(self)
  _ModuleOffloader.wait_and_get_device_tensors(self)
  _ShardedGpuParamOffloader.__init__(self,module,param_name)
  _ShardedGpuParamOffloader.create_device_tensor(self)
  _ShardedGpuParamOffloader.post_init(self)
  _ShmCpuParamOffloader.__init__(self,module,param_name)
  _ShmCpuParamOffloader.create_device_tensor(self)
  _ShmCpuParamOffloader.post_init(self)
  _create_shared_buffer_tensors(local_tensor)
  _empty_strided_like(x,device,pin_memory)
  _even_chunk(x,chunks)
  _hook_module_forward_for_offloader(index,module,offloaders,prefetch_step)
  _hook_module_forward_raw(module,on_forward_end,get_parameter_and_buffer_dicts)
  _move_param_to_cpu(param,pin_memory)
  _move_param_to_meta(module,param_name)
  _on_forward_end()
  create_offloader_from_server_args(server_args,dp_rank)
  forward(*args,**kwargs)
  get_offloader()
  set_offloader(instance)
operations.py:
  _StageExecutor.__init__(self,debug_name,stages,inputs)
  _StageExecutor.done(self)
  _StageExecutor.next(self)
  _StageExecutor.num_stages(self)
  _StageExecutor.output(self)
  _StateDict.__delattr__(self,item)
  _StateDict.__getattr__(self,item)
  _StateDict.__init__(self)
  _StateDict.__setattr__(self,key,value)
  _StateDict.clear(self,expect_keys)
  _StateDict.get(self,item)
  _StateDict.pop(self,item)
  _StateDict.update(self,values)
  _annotate_region(debug_name)
  _chunk_by_separator(items,is_separator)
  _convert_operations_to_stages(operations)
  _decorate_operation(operation,debug_name_prefix)
  _decorate_operations(operations,debug_name_prefix)
  execute_operations(inputs,operations)
  execute_overlapped_operations(inputs_arr,operations_arr,delta_stages)
operations_strategy.py:
  OperationsStrategy.concat(cls,items)
  OperationsStrategy.init_new_tbo(layers,forward_mode)
  _assert_all_same(items)
  _compute_moe_deepseek_blog_decode(layer)
  _compute_moe_deepseek_blog_prefill(layer)
  _compute_moe_deepseek_layer_operations_strategy_tbo(layer,forward_mode)
  _compute_moe_qwen3_decode(layer)
  _compute_moe_qwen3_layer_operations_strategy_tbo(layer,forward_mode)
  _compute_moe_qwen3_prefill(layer)
patch_torch.py:
  _device_from_maybe_uuid(device_maybe_uuid)
  _device_to_uuid(device)
  _modify_tuple(t,index,modifier)
  _rebuild_cuda_tensor_modified(*args)
  _reduce_tensor_modified(*args,**kwargs)
  monkey_patch_torch_compile()
  monkey_patch_torch_reductions()
poll_based_barrier.py:
  PollBasedBarrier.__init__(self,noop)
  PollBasedBarrier._compute_global_arrived(self)
  PollBasedBarrier.local_arrive(self)
  PollBasedBarrier.poll_global_arrived(self)
reasoning_parser.py:
  BaseReasoningFormatDetector.__init__(self,think_start_token,think_end_token,force_reasoning,stream_reasoning)
  BaseReasoningFormatDetector.detect_and_parse(self,text)
  BaseReasoningFormatDetector.parse_streaming_increment(self,new_text)
  DeepSeekR1Detector.__init__(self,stream_reasoning,force_reasoning)
  GptOssDetector.__init__(self,stream_reasoning,force_reasoning)
  GptOssDetector.detect_and_parse(self,text)
  GptOssDetector.parse_streaming_increment(self,new_text)
  KimiDetector.__init__(self,stream_reasoning,force_reasoning)
  Qwen3Detector.__init__(self,stream_reasoning,force_reasoning)
  ReasoningParser.__init__(self,model_type,stream_reasoning,force_reasoning)
  ReasoningParser.parse_non_stream(self,full_text)
  ReasoningParser.parse_stream_chunk(self,chunk_text)
  StreamingParseResult.__init__(self,normal_text,reasoning_text)
sampling/custom_logit_processor.py:
  CustomLogitProcessor.__call__(self,logits,custom_param_list)
  CustomLogitProcessor.from_str(cls,json_str)
  CustomLogitProcessor.to_str(cls)
  DisallowedTokensLogitsProcessor.__call__(self,logits,custom_param_list)
  _cache_from_str(json_str)
sampling/penaltylib/frequency_penalty.py:
  BatchedFrequencyPenalizer.__init__(self,orchestrator)
  BatchedFrequencyPenalizer._apply(self,logits)
  BatchedFrequencyPenalizer._cumulate_output_tokens(self,output_ids)
  BatchedFrequencyPenalizer._filter(self,keep_indices)
  BatchedFrequencyPenalizer._is_required(self)
  BatchedFrequencyPenalizer._merge(self,their)
  BatchedFrequencyPenalizer._prepare(self)
sampling/penaltylib/min_new_tokens.py:
  BatchedMinNewTokensPenalizer.__init__(self,orchestrator)
  BatchedMinNewTokensPenalizer._apply(self,logits)
  BatchedMinNewTokensPenalizer._cumulate_output_tokens(self,output_ids)
  BatchedMinNewTokensPenalizer._filter(self,keep_indices)
  BatchedMinNewTokensPenalizer._is_required(self)
  BatchedMinNewTokensPenalizer._merge(self,their)
  BatchedMinNewTokensPenalizer._prepare(self)
sampling/penaltylib/orchestrator.py:
  BatchedPenalizerOrchestrator.__init__(self,vocab_size,batch,penalizers)
  BatchedPenalizerOrchestrator.apply(self,logits)
  BatchedPenalizerOrchestrator.batch(self)
  BatchedPenalizerOrchestrator.batch(self,value)
  BatchedPenalizerOrchestrator.cumulate_output_tokens(self,output_ids)
  BatchedPenalizerOrchestrator.filter(self,keep_indices)
  BatchedPenalizerOrchestrator.merge(self,their)
  BatchedPenalizerOrchestrator.reqs(self)
  _BatchedPenalizer._apply(self,logits)
  _BatchedPenalizer._cumulate_output_tokens(self,output_ids)
  _BatchedPenalizer._filter(self,keep_indices)
  _BatchedPenalizer._is_required(self)
  _BatchedPenalizer._merge(self,their)
  _BatchedPenalizer._prepare(self)
  _BatchedPenalizer.apply(self,logits)
  _BatchedPenalizer.cumulate_output_tokens(self,output_ids)
  _BatchedPenalizer.filter(self,keep_indices)
  _BatchedPenalizer.is_prepared(self)
  _BatchedPenalizer.is_required(self)
  _BatchedPenalizer.merge(self,their)
  _BatchedPenalizer.prepare(self)
  _BatchedPenalizer.prepare_if_required(self)
  _BatchedPenalizer.teardown(self)
sampling/penaltylib/presence_penalty.py:
  BatchedPresencePenalizer.__init__(self,orchestrator)
  BatchedPresencePenalizer._apply(self,logits)
  BatchedPresencePenalizer._cumulate_output_tokens(self,output_ids)
  BatchedPresencePenalizer._filter(self,keep_indices)
  BatchedPresencePenalizer._is_required(self)
  BatchedPresencePenalizer._merge(self,their)
  BatchedPresencePenalizer._prepare(self)
sampling/sampling_batch_info.py:
  SamplingBatchInfo.__len__(self)
  SamplingBatchInfo._filter_batch_custom_logit_processor(self,keep_indices,keep_indices_device)
  SamplingBatchInfo.apply_logits_bias(self,logits)
  SamplingBatchInfo.filter_batch(self,keep_indices,keep_indices_device)
  SamplingBatchInfo.from_schedule_batch(cls,batch,vocab_size)
  SamplingBatchInfo.merge_batch(self,other)
  SamplingBatchInfo.merge_custom_logit_processor(lhs,rhs,bs1,bs2,device)
  SamplingBatchInfo.update_penalties(self)
  SamplingBatchInfo.update_regex_vocab_mask(self)
  merge_bias_tensor(lhs,rhs,bs1,bs2,device,default)
sampling/sampling_params.py:
  SamplingParams.__init__(self,max_new_tokens,stop,stop_token_ids,temperature,top_p,top_k,min_p,frequency_penalty,presence_penalty,repetition_penalty,min_new_tokens,n,json_schema,regex,ebnf,structural_tag,ignore_eos,skip_special_tokens,spaces_between_special_tokens,no_stop_trim,custom_params,stream_interval,logit_bias)
  SamplingParams.normalize(self,tokenizer)
  SamplingParams.verify(self,vocab_size)
server_args.py:
  DeprecatedAction.__call__(self,parser,namespace,values,option_string)
  DeprecatedAction.__init__(self,option_strings,dest,nargs,**kwargs)
  LoRAPathAction.__call__(self,parser,namespace,values,option_string)
  PortArgs.init_new(server_args,dp_rank)
  ServerArgs.__post_init__(self)
  ServerArgs.add_cli_args(parser)
  ServerArgs.adjust_mem_fraction_for_vlm(self,model_config)
  ServerArgs.check_lora_server_args(self)
  ServerArgs.check_server_args(self)
  ServerArgs.from_cli_args(cls,args)
  ServerArgs.get_hf_config(self)
  ServerArgs.model_specific_adjustments(self)
  ServerArgs.url(self)
  ServerArgs.validate_disagg_tp_size(self,prefill_tp,decode_tp)
  add_attention_backend_choices(choices)
  add_disagg_transfer_backend_choices(choices)
  add_load_format_choices(choices)
  add_quantization_method_choices(choices)
  auto_choose_speculative_params(self)
  prepare_server_args(argv)
  print_deprecated_warning(message)
speculative/build_eagle_tree.py:
  build_tree_kernel_efficient(verified_id,score_list,token_list,parents_list,seq_lens,seq_lens_sum,topk,spec_steps,num_verify_tokens,tree_mask_mode,tree_mask_buf,position_buf)
  build_tree_kernel_efficient_preprocess(verified_id,score_list,token_list,parents_list,num_verify_tokens)
  test_build_tree_kernel_efficient()
speculative/eagle_draft_cuda_graph_runner.py:
  EAGLEDraftCudaGraphRunner.__init__(self,eagle_worker)
  EAGLEDraftCudaGraphRunner._postprocess_output_to_raw_bs(self,out,raw_bs)
  EAGLEDraftCudaGraphRunner.can_run(self,forward_batch)
  EAGLEDraftCudaGraphRunner.capture(self)
  EAGLEDraftCudaGraphRunner.capture_one_batch_size(self,num_seqs,forward)
  EAGLEDraftCudaGraphRunner.replay(self,forward_batch)
  EAGLEDraftCudaGraphRunner.run_once()
speculative/eagle_draft_extend_cuda_graph_runner.py:
  EAGLEDraftExtendCudaGraphRunner.__init__(self,eagle_worker)
  EAGLEDraftExtendCudaGraphRunner.can_run(self,forward_batch)
  EAGLEDraftExtendCudaGraphRunner.capture(self)
  EAGLEDraftExtendCudaGraphRunner.capture_one_batch_size(self,bs,forward)
  EAGLEDraftExtendCudaGraphRunner.replay(self,forward_batch)
  EAGLEDraftExtendCudaGraphRunner.run_once()
speculative/eagle_utils.py:
  EagleDraftInput.create_idle_input(cls,device,hidden_size,dtype,topk,capture_hidden_mode)
  EagleDraftInput.filter_batch(self,new_indices,has_been_filtered)
  EagleDraftInput.generate_attn_arg_prefill(self,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,req_to_token)
  EagleDraftInput.merge_batch(self,spec_info)
  EagleDraftInput.prepare_extend_after_decode(self,batch,speculative_num_steps)
  EagleDraftInput.prepare_for_extend(self,batch)
  EagleVerifyInput.create_idle_input(cls,topk,spec_steps,num_verify_tokens)
  EagleVerifyInput.generate_attn_arg_prefill(self,req_pool_indices,paged_kernel_lens,paged_kernel_lens_sum,req_to_token)
  EagleVerifyInput.prepare_for_verify(self,batch,page_size)
  EagleVerifyInput.verify(self,batch,logits_output,token_to_kv_pool_allocator,page_size,vocab_mask)
  _generate_simulated_accept_index(accept_index,predict,accept_length,simulate_acc_len,bs,spec_steps)
  align_evict_mask_to_page_size(seq_lens,evict_mask,page_size,num_draft_tokens,BLOCK_SIZE)
  assign_draft_cache_locs(req_pool_indices,req_to_token,seq_lens,extend_lens,num_new_pages_per_topk,out_cache_loc,pool_len,topk,speculative_num_steps,page_size,bs_upper,iter_upper)
  assign_req_to_token_pool(req_pool_indices,req_to_token,start_offset,end_offset,out_cache_loc,pool_len,bs_upper)
  create_accept_length_filter(accept_length,unfinished_index_device,seq_lens)
  create_extend_after_decode_spec_info(verified_id,seq_lens,accept_lens,positions,new_verified_id,bs_upper)
  dfs(curr,retrieve_next_token,retrieve_next_sibling,parent_pos)
  filter_finished_cache_loc_kernel(out_cache_loc,tgt_cache_loc,accept_length,accept_length_filter,bs_upper,num_verify_tokens_upper)
  generate_draft_decode_kv_indices(req_pool_indices,req_to_token,paged_kernel_lens,kv_indices,kv_indptr,positions,pool_len,kv_indices_stride,kv_indptr_stride,bs_upper,iter_upper,num_tokens_upper,page_size)
  generate_token_bitmask(reqs,verify_input,retrieve_next_token_cpu,retrieve_next_sibling_cpu,draft_tokens_cpu,vocab_size)
  get_src_tgt_cache_loc(seq_lens,out_cache_loc,accept_index,accept_length,draft_token_num,page_size)
  get_target_cache_loc(tgt_cache_loc,to_free_slots,accept_length,to_free_num_slots,out_cache_loc,num_verify_tokens,num_verify_tokens_upper,bs_upper)
  select_top_k_tokens(i,topk_p,topk_index,hidden_states,scores,topk)
  traverse_tree(retrieve_next_token,retrieve_next_sibling,draft_tokens,grammar,allocate_token_bitmask)
speculative/eagle_worker.py:
  EAGLEWorker.__init__(self,server_args,gpu_id,tp_rank,dp_rank,moe_ep_rank,nccl_port,target_worker)
  EAGLEWorker._detect_nan_if_needed(self,logits_output)
  EAGLEWorker._draft_preprocess_decode(self,batch)
  EAGLEWorker._draft_preprocess_idle(self,batch)
  EAGLEWorker.add_logprob_values(self,batch,res,logits_output)
  EAGLEWorker.capture_for_decode(self,logits_output,draft_input)
  EAGLEWorker.check_forward_draft_extend_after_decode(self,batch)
  EAGLEWorker.draft(self,batch)
  EAGLEWorker.draft_forward(self,forward_batch)
  EAGLEWorker.draft_model_runner(self)
  EAGLEWorker.forward_batch_speculative_generation(self,batch)
  EAGLEWorker.forward_draft_extend(self,batch,hidden_states,next_token_ids,seq_lens_cpu)
  EAGLEWorker.forward_draft_extend_after_decode(self,batch)
  EAGLEWorker.forward_target_extend(self,batch)
  EAGLEWorker.init_attention_backend(self)
  EAGLEWorker.init_cuda_graphs(self)
  EAGLEWorker.verify(self,batch,spec_info)
  draft_tp_context(tp_group)
  get_last_loc_large_page_size_large_top_k(req_to_token,req_pool_indices,seq_lens,speculative_num_steps,topk,page_size)
  get_last_loc_large_page_size_top_k_1(req_to_token,req_pool_indices,seq_lens,speculative_num_steps)
  load_token_map(token_map_path)
speculative/spec_info.py:
  SpeculativeAlgorithm.from_string(name)
  SpeculativeAlgorithm.is_eagle(self)
  SpeculativeAlgorithm.is_eagle3(self)
  SpeculativeAlgorithm.is_none(self)
tokenizer/tiktoken_tokenizer.py:
  TiktokenProcessor.__init__(self,name)
  TiktokenProcessor.image_processor(self,image)
  TiktokenTokenizer.__call__(self,text,**kwargs)
  TiktokenTokenizer.__init__(self,tokenizer_path)
  TiktokenTokenizer.apply_chat_template(self,messages,tokenize,add_generation_prompt,tools,reasoning_effort)
  TiktokenTokenizer.batch_decode(self,batch,skip_special_tokens,spaces_between_special_tokens)
  TiktokenTokenizer.decode(self,x,*args,**kwargs)
  TiktokenTokenizer.encode(self,x,add_special_tokens)
  TiktokenTokenizer.encode_patched(self,text,allowed_special,disallowed_special)
  TiktokenTokenizer.init_xgrammar(self)
torch_memory_saver_adapter.py:
  TorchMemorySaverAdapter.check_validity(self,caller_name)
  TorchMemorySaverAdapter.configure_subprocess(self)
  TorchMemorySaverAdapter.create(enable)
  TorchMemorySaverAdapter.enabled(self)
  TorchMemorySaverAdapter.pause(self,tag)
  TorchMemorySaverAdapter.region(self,tag)
  TorchMemorySaverAdapter.resume(self,tag)
  _TorchMemorySaverAdapterNoop.configure_subprocess(self)
  _TorchMemorySaverAdapterNoop.enabled(self)
  _TorchMemorySaverAdapterNoop.pause(self,tag)
  _TorchMemorySaverAdapterNoop.region(self,tag)
  _TorchMemorySaverAdapterNoop.resume(self,tag)
  _TorchMemorySaverAdapterReal.configure_subprocess(self)
  _TorchMemorySaverAdapterReal.enabled(self)
  _TorchMemorySaverAdapterReal.pause(self,tag)
  _TorchMemorySaverAdapterReal.region(self,tag)
  _TorchMemorySaverAdapterReal.resume(self,tag)
two_batch_overlap.py:
  MaybeTboDeepEPDispatcher.__init__(self,**kwargs)
  MaybeTboDeepEPDispatcher._execute(self,name,tbo_subbatch_index,**kwargs)
  MaybeTboDeepEPDispatcher.combine(self,**kwargs)
  MaybeTboDeepEPDispatcher.combine_a(self,**kwargs)
  MaybeTboDeepEPDispatcher.combine_b(self,**kwargs)
  MaybeTboDeepEPDispatcher.dispatch(self,**kwargs)
  MaybeTboDeepEPDispatcher.dispatch_a(self,**kwargs)
  MaybeTboDeepEPDispatcher.dispatch_b(self,**kwargs)
  TboCudaGraphRunnerPlugin.__init__(self)
  TboCudaGraphRunnerPlugin.capture_one_batch_size(self,batch,num_tokens)
  TboCudaGraphRunnerPlugin.replay_prepare(self,forward_mode,bs,num_token_non_padded,spec_info)
  TboDPAttentionPreparer._compute_global_forward_mode(forward_modes)
  TboDPAttentionPreparer._compute_local_forward_mode(local_batch)
  TboDPAttentionPreparer._is_all_same(x)
  TboDPAttentionPreparer.compute_output(self,partial_global_info)
  TboDPAttentionPreparer.prepare_all_gather(self,local_batch)
  TboForwardBatchPreparer._compute_split_token_index(cls,batch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded(cls,batch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded_raw(cls,tbo_split_token_index,num_token_non_padded)
  TboForwardBatchPreparer.derive_fields_related_to_seq_len_for_two_chunk(cls,batch,child_a,child_b,tbo_split_seq_index)
  TboForwardBatchPreparer.filter_batch(cls,batch,start_token_index,end_token_index,start_seq_index,end_seq_index,output_attn_backend,out_num_token_non_padded)
  TboForwardBatchPreparer.prepare(cls,batch,is_draft_worker)
  TboForwardBatchPreparer.prepare_raw(cls,batch,tbo_children_num_token_non_padded)
  _compute_extend_num_tokens(input_ids,forward_mode)
  _compute_mask_offset(seq_index,spec_info)
  _handle_key(name)
  _is_two_chunk_split_enabled(extend_lens)
  _model_forward_filter_inputs(hidden_states,residual,positions,output_forward_batch,tbo_subbatch_index)
  _model_forward_non_tbo(inputs,operations_strategy)
  _model_forward_tbo(inputs,operations_strategy,input_data_scatter_mode,layer_input_scatter_mode)
  _model_forward_tbo_merge_outputs(output_a,output_b)
  _model_forward_tbo_split_inputs(hidden_states,residual,positions,forward_batch,zero_allocator,input_data_scatter_mode,layer_input_scatter_mode)
  _model_forward_tbo_split_inputs_raw(hidden_states,residual,positions,forward_batch,zero_allocator)
  _post_transform(hidden_states,residual,forward_batch,**kwargs)
  _split_array_by_balanced_sum(arr)
  _split_array_by_cum_less_than_half(arr)
  _split_extend_seqs(arr)
  _update_device_and_sum_field_from_cpu_field(batch,cpu_field,device_field,sum_field)
  compute_split_indices_for_cuda_graph_replay(forward_mode,cuda_graph_num_tokens,spec_info)
  compute_split_seq_index(forward_mode,num_tokens,extend_lens,token_num_per_seq)
  compute_split_token_index(split_seq_index,forward_mode,extend_seq_lens,token_num_per_seq)
  get_token_num_per_seq(forward_mode,spec_info)
  model_forward_maybe_tbo(layers,enable_tbo,positions,forward_batch,hidden_states,input_data_scatter_mode,residual,zero_allocator)
  split_spec_info(spec_info,start_seq_index,end_seq_index,start_token_index,end_token_index)
utils.py:
  BumpAllocator.__init__(self,buffer_size,dtype,device)
  BumpAllocator.allocate(self,size)
  ConcurrentCounter.__init__(self,initial)
  ConcurrentCounter.__repr__(self)
  ConcurrentCounter.value(self)
  DynamicGradMode.__enter__(self)
  DynamicGradMode.__exit__(self,exc_type,exc_value,traceback)
  DynamicGradMode.__init__(self,mode)
  DynamicGradMode.__new__(cls,mode_or_orig_func)
  DynamicGradMode.clone(self)
  DynamicGradMode.set_inference_mode(mode)
  EmptyContextManager.__enter__(self)
  EmptyContextManager.__exit__(self,exc_type,exc_value,traceback)
  LayerFn.__call__(self,layer_id,prefix)
  LazyValue.__init__(self,creator)
  LazyValue.value(self)
  MultiprocessingSerializer.deserialize(data)
  MultiprocessingSerializer.serialize(obj,output_str)
  PackWeightMethod.__init__(self,weight_names,transpose_dims)
  PackWeightMethod.process_weights_after_loading(self,module)
  TimeInfo.__init__(self,name,interval,color,indent)
  TimeInfo.check(self)
  TimeInfo.pretty_print(self)
  Withable.__init__(self)
  Withable.value(self)
  Withable.with_value(self,new_value)
  _check(cc_major)
  _process_weight_after_loading(module,weight_names,transpose_dims)
  _to_hashable(o)
  add_api_key_middleware(app,api_key)
  add_prefix(name,prefix)
  add_prometheus_middleware(app)
  align(x,y)
  apply_module_patch(target_module,target_function,wrappers)
  assert_pkg_version(pkg,min_version,message)
  async ConcurrentCounter.decrement(self,n,notify_all)
  async ConcurrentCounter.increment(self,n,notify_all)
  async ConcurrentCounter.wait_for(self,condition)
  async ConcurrentCounter.wait_for_zero(self)
  async authentication(request,call_next)
  async health()
  async health_generate()
  bind_or_assign(target,source)
  bind_or_assign(target,source)
  bind_port(port)
  broadcast_pyobj(data,rank,dist_group,src,force_cpu_device)
  calculate_time(show,min_cost_ms)
  ceil_div(x,y)
  check_cuda_result(raw_output)
  configure_gc_logger()
  configure_gc_warning(warn_threshold_secs)
  configure_ipv6(dist_init_addr)
  configure_logger(server_args,prefix)
  cpu_has_amx_support()
  crash_on_warnings()
  create_checksum(directory)
  create_dummy_module(full_path,parent)
  create_placeholder_function(func_name)
  dataclass_to_string_truncated(data,max_length,skip_names)
  debug_timing(func)
  decode_video_base64(video_base64)
  decorator(func)
  decorator(func)
  delete_directory(dirpath)
  dim_is_supported(weight)
  direct_register_custom_op(op_name,op_func,mutates_args,fake_impl,target_lib)
  disable_request_logging()
  dispose_tensor(x)
  dump_to_file(dirpath,name,value)
  dynamic_import(func_path)
  empty_context(*args,**kwargs)
  enable_show_time_cost()
  fast_topk(values,topk,dim)
  find_local_repo_dir(repo_id,revision)
  find_process_using_port(port)
  flatten_nested_list(nested_list)
  format_tcp_address(ip,port)
  freeze_gc(context)
  gc_callback(phase,info)
  gc_callback(phase,info)
  gc_object_counts()
  get_amdgpu_memory_capacity()
  get_available_gpu_memory(device,gpu_id,distributed,empty_cache,cpu_group)
  get_bool_env_var(name,default)
  get_compiler_backend()
  get_cpu_ids_by_node()
  get_cuda_version()
  get_device(device_id)
  get_device_capability(device_id)
  get_device_core_count(device_id)
  get_device_count()
  get_device_memory_capacity(device)
  get_device_name(device_id)
  get_device_sm()
  get_free_port()
  get_hpu_memory_capacity()
  get_int_env_var(name,default)
  get_ip()
  get_local_ip_auto()
  get_local_ip_by_nic(interface)
  get_local_ip_by_remote()
  get_npu_compiler_config()
  get_npu_memory_capacity()
  get_nvgpu_memory_capacity()
  get_open_port()
  get_physical_cpus_by_numa()
  get_quant_method_with_embedding_replaced(self,layer,prefix)
  get_zmq_socket(context,socket_type,endpoint,bind)
  init_custom_process_group(backend,init_method,timeout,world_size,rank,store,group_name,pg_options)
  inner_func(*args,**kwargs)
  is_blackwell()
  is_cpu()
  is_cuda()
  is_cuda_alike()
  is_fa3_default_architecture(hf_config)
  is_flashinfer_available()
  is_habana_available()
  is_hip()
  is_host_cpu_x86()
  is_hpu()
  is_no_spec_infer_or_topk_one(server_args)
  is_non_idle_and_non_empty(forward_mode,hidden_states)
  is_npu()
  is_page_size_one(server_args)
  is_pin_memory_available()
  is_port_available(port)
  is_remote_url(url)
  is_shm_available(dtype,world_size,local_size)
  is_sm100_supported(device)
  is_sm90_supported(device)
  is_triton_3()
  is_triton_kernels_available()
  is_valid_ipv6_address(address)
  is_xpu()
  kill_itself_when_parent_died()
  kill_process_tree(parent_pid,include_parent,skip_pid)
  launch_dummy_health_check_server(host,port,enable_metrics)
  load_audio(audio_file,sr,mono)
  load_image(image_file)
  load_json_config(data)
  load_video(video_file,use_gpu)
  log_info_on_rank0(logger,msg)
  lru_cache_frozenset(maxsize)
  make_layers(num_hidden_layers,layer_fn,pp_rank,pp_size,prefix,return_tuple,offloader_kwargs)
  mark_end(name)
  mark_start(name,interval,color,indent)
  maybe_torch_compile(*args,**kwargs)
  maybe_wrap_ipv6_address(address)
  monkey_patch_p2p_access_check()
  monkey_patch_vllm_gguf_config()
  mxfp_supported()
  next_power_of_2(n)
  nullable_str(val)
  parse_connector_type(url)
  parse_lscpu_topology()
  parse_module_path(module_path,function_name,create_dummy)
  permute_weight(x)
  placeholder(*args,**kwargs)
  point_to_point_pyobj(data,rank,group,src,dst)
  prepack_weight_if_needed(weight)
  prepare_model_and_tokenizer(model_path,tokenizer_path)
  print_info_once(msg)
  print_warning_once(msg)
  pyspy_dump_schedulers()
  pytorch_profile(name,func,*args,data_size)
  random_uuid()
  read_system_prompt_from_file(model_name)
  replace_submodule(model,module_name,new_module)
  require_attn_tp_gather(server_args)
  require_gathered_buffer(server_args)
  require_mlp_sync(server_args)
  require_mlp_tp_gather(server_args)
  retry(fn,max_retry,initial_delay,max_delay,should_retry)
  round_up(x,y)
  set_cuda_arch()
  set_gpu_proc_affinity(tp_size,nnodes,gpu_id)
  set_prometheus_multiproc_dir()
  set_random_seed(seed)
  set_recv_opt()
  set_send_opt()
  set_ulimit(target_soft_limit)
  set_uvicorn_logging_configs()
  set_weight_attrs(weight,weight_attrs)
  support_triton(backend)
  supports_custom_op()
  suppress_other_loggers()
  use_intel_amx_backend(layer)
  wait_port_available(port,port_name,timeout_s,raise_exception)
  wrapper(*args,**kwargs)
  wrapper(*args,**kwargs)
  wrapper(func)
warmup.py:
  async execute_warmups(disaggregation_mode,warmup_names,tokenizer_manager)
  async voice_chat(disaggregation_mode,tokenizer_manager)
  decorator(fn)
  warmup(name)
weight_sync/tensor_bucket.py:
  FlattenedTensorBucket.__init__(self,named_tensors,flattened_tensor,metadata)
  FlattenedTensorBucket.get_flattened_tensor(self)
  FlattenedTensorBucket.get_metadata(self)
  FlattenedTensorBucket.reconstruct_tensors(self)
weight_sync/utils.py:
  _preprocess_tensor_for_update_weights(tensor)
  async update_weights(engine,params_batch,device_mesh_key,device_mesh,load_format)
