entrypoints/EngineBase.py:
  EngineBase.flush_cache(self)
  EngineBase.generate(self,prompt,sampling_params,input_ids,image_data,return_logprob,logprob_start_len,top_logprobs_num,token_ids_logprob,lora_path,custom_logit_processor,return_hidden_states,stream,bootstrap_host,bootstrap_port,bootstrap_room,data_parallel_rank)
  EngineBase.load_lora_adapter(self,lora_name,lora_path)
  EngineBase.release_memory_occupation(self)
  EngineBase.resume_memory_occupation(self)
  EngineBase.shutdown(self)
  EngineBase.unload_lora_adapter(self,lora_name)
  EngineBase.update_weights_from_tensor(self,named_tensors,load_format,flush_cache)
entrypoints/context.py:
  ConversationContext.append_output(self,output)
  ConversationContext.need_builtin_tool_call(self)
  ConversationContext.render_for_completion(self)
  HarmonyContext.__init__(self,messages,tool_sessions)
  HarmonyContext.append_output(self,output)
  HarmonyContext.messages(self)
  HarmonyContext.need_builtin_tool_call(self)
  HarmonyContext.render_for_completion(self)
  SimpleContext.__init__(self)
  SimpleContext.append_output(self,output)
  SimpleContext.need_builtin_tool_call(self)
  SimpleContext.render_for_completion(self)
  StreamingHarmonyContext.__init__(self,*args,**kwargs)
  StreamingHarmonyContext.append_output(self,output)
  StreamingHarmonyContext.is_assistant_action_turn(self)
  StreamingHarmonyContext.is_expecting_start(self)
  StreamingHarmonyContext.messages(self)
  StreamingHarmonyContext.render_for_completion(self)
  async ConversationContext.call_tool(self)
  async HarmonyContext.call_python_tool(self,tool_session,last_msg)
  async HarmonyContext.call_search_tool(self,tool_session,last_msg)
  async HarmonyContext.call_tool(self)
  async SimpleContext.call_tool(self)
entrypoints/engine.py:
  Engine.__enter__(self)
  Engine.__exit__(self,exc_type,exc_value,traceback)
  Engine.__init__(self,**kwargs)
  Engine.collective_rpc(self,method,**kwargs)
  Engine.dump_expert_distribution_record(self)
  Engine.encode(self,prompt,image_data,audio_data,video_data)
  Engine.flush_cache(self)
  Engine.freeze_gc(self)
  Engine.generate(self,prompt,sampling_params,input_ids,image_data,audio_data,video_data,return_logprob,logprob_start_len,top_logprobs_num,token_ids_logprob,lora_path,custom_logit_processor,return_hidden_states,stream,bootstrap_host,bootstrap_port,bootstrap_room,data_parallel_rank)
  Engine.generator_wrapper()
  Engine.get_server_info(self)
  Engine.get_weights_by_name(self,name,truncate_size)
  Engine.init_weights_update_group(self,master_address,master_port,rank_offset,world_size,group_name,backend)
  Engine.load_lora_adapter(self,lora_name,lora_path,pinned)
  Engine.release_memory_occupation(self,tags)
  Engine.rerank(self,prompt)
  Engine.resume_memory_occupation(self,tags)
  Engine.save_remote_model(self,**kwargs)
  Engine.save_sharded_model(self,**kwargs)
  Engine.score(self,query,items,label_token_ids,apply_softmax,item_first)
  Engine.shutdown(self)
  Engine.start_expert_distribution_record(self)
  Engine.start_profile(self)
  Engine.stop_expert_distribution_record(self)
  Engine.stop_profile(self)
  Engine.unload_lora_adapter(self,lora_name)
  Engine.update_weights_from_disk(self,model_path,load_format)
  Engine.update_weights_from_distributed(self,names,dtypes,shapes,group_name,flush_cache)
  Engine.update_weights_from_tensor(self,named_tensors,load_format,flush_cache)
  _launch_subprocesses(server_args,port_args)
  _set_envs_and_config(server_args)
  async Engine.async_encode(self,prompt,image_data,audio_data,video_data)
  async Engine.async_generate(self,prompt,sampling_params,input_ids,image_data,audio_data,video_data,return_logprob,logprob_start_len,top_logprobs_num,token_ids_logprob,lora_path,custom_logit_processor,return_hidden_states,stream,bootstrap_host,bootstrap_port,bootstrap_room,data_parallel_rank)
  async Engine.async_score(self,query,items,label_token_ids,apply_softmax,item_first)
  launch_phase_sigquit_handler(signum,frame)
entrypoints/harmony_utils.py:
  get_developer_message(instructions,tools)
  get_encoding()
  get_stop_tokens_for_assistant_actions()
  get_streamable_parser_for_assistant()
  get_system_message(model_identity,reasoning_effort,start_date,browser_description,python_description)
  get_user_message(content)
  parse_chat_input(chat_msg)
  parse_output_into_messages(token_ids)
  parse_output_message(message)
  parse_remaining_state(parser)
  parse_response_input(response_msg,prev_responses)
  parse_response_output(output)
  render_for_completion(messages)
entrypoints/http_server.py:
  _create_error_response(e)
  _execute_server_warmup(server_args,pipe_finish_writer)
  _update_weight_version_if_provided(weight_version)
  _wait_and_warmup(server_args,pipe_finish_writer,launch_callback)
  async abort_request(obj,request)
  async available_models()
  async classify_request(obj,request)
  async clear_hicache_storage_backend()
  async close_session(obj,request)
  async configure_logging(obj,request)
  async continue_generation(request)
  async dump_expert_distribution_record_async()
  async encode_request(obj,request)
  async flush_cache()
  async freeze_gc_async()
  async gen()
  async generate_from_file_request(file,request)
  async generate_request(obj,request)
  async get_load()
  async get_model_info()
  async get_server_info()
  async get_weight_version()
  async get_weights_by_name(obj,request)
  async health_generate(request)
  async init_weights_update_group(obj,request)
  async lifespan(fast_api_app)
  async load_lora_adapter(obj,request)
  async open_session(obj,request)
  async openai_v1_chat_completions(request,raw_request)
  async openai_v1_completions(request,raw_request)
  async openai_v1_embeddings(request,raw_request)
  async parse_function_call_request(obj,request)
  async pause_generation(request)
  async release_memory_occupation(obj,request)
  async resume_memory_occupation(obj,request)
  async retrieve_model(model)
  async sagemaker_chat_completions(request,raw_request)
  async sagemaker_health()
  async separate_reasoning_request(obj,request)
  async set_internal_state(obj,request)
  async slow_down(obj,request)
  async start_expert_distribution_record_async()
  async start_profile_async(obj)
  async stop_expert_distribution_record_async()
  async stop_profile_async()
  async stream_results()
  async unload_lora_adapter(obj,request)
  async update_weight_version(obj,request)
  async update_weights_from_disk(obj,request)
  async update_weights_from_distributed(obj,request)
  async update_weights_from_tensor(obj,request)
  async v1_cancel_responses(response_id,raw_request)
  async v1_rerank_request(request,raw_request)
  async v1_responses_request(request,raw_request)
  async v1_retrieve_responses(response_id,raw_request)
  async v1_score_request(request,raw_request)
  async validate_json_request(raw_request)
  async validation_exception_handler(request,exc)
  async validation_exception_handler(request,exc)
  async vertex_generate(vertex_req,raw_request)
  launch_server(server_args,pipe_finish_writer,launch_callback)
  set_global_state(global_state)
entrypoints/http_server_engine.py:
  HttpServerEngineAdapter.__init__(self,**kwargs)
  HttpServerEngineAdapter._make_request(self,endpoint,payload)
  HttpServerEngineAdapter.flush_cache(self)
  HttpServerEngineAdapter.generate(self,prompt,sampling_params,input_ids,image_data,return_logprob,logprob_start_len,top_logprobs_num,token_ids_logprob,lora_path,custom_logit_processor)
  HttpServerEngineAdapter.release_memory_occupation(self)
  HttpServerEngineAdapter.resume_memory_occupation(self)
  HttpServerEngineAdapter.shutdown(self)
  HttpServerEngineAdapter.update_weights_from_tensor(self,named_tensors,load_format,flush_cache)
  launch_server_process(server_args)
entrypoints/openai/protocol.py:
  ChatCompletionMessageGenericParam._normalize_role(cls,v)
  ChatCompletionRequest.normalize_reasoning_inputs(cls,values)
  ChatCompletionRequest.set_json_schema(cls,values)
  ChatCompletionRequest.set_tool_choice_default(cls,values)
  ChatCompletionResponseChoice._serialize(self,handler)
  CompletionRequest.validate_max_tokens_positive(cls,v)
  CompletionResponseChoice._serialize(self,handler)
  CompletionResponseStreamChoice._serialize(self,handler)
  DeltaMessage._serialize(self,handler)
  ResponsesRequest.to_sampling_params(self,default_max_tokens,default_params)
  ResponsesResponse.from_request(cls,request,sampling_params,model_name,created_time,output,status,usage)
entrypoints/openai/serving_base.py:
  OpenAIServingBase.__init__(self,tokenizer_manager)
  OpenAIServingBase._convert_to_internal_request(self,request)
  OpenAIServingBase._generate_request_id_base(self,request)
  OpenAIServingBase._request_id_prefix(self)
  OpenAIServingBase._validate_request(self,_)
  OpenAIServingBase.create_error_response(self,message,err_type,status_code,param)
  OpenAIServingBase.create_streaming_error_response(self,message,err_type,status_code)
  async OpenAIServingBase._handle_non_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingBase._handle_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingBase.handle_request(self,request,raw_request)
entrypoints/openai/serving_chat.py:
  OpenAIServingChat.__init__(self,tokenizer_manager,template_manager)
  OpenAIServingChat._apply_conversation_template(self,request,is_multimodal)
  OpenAIServingChat._apply_jinja_template(self,request,tools,is_multimodal)
  OpenAIServingChat._build_chat_response(self,request,ret,created)
  OpenAIServingChat._build_sampling_params(self,request,stop,tool_call_constraint)
  OpenAIServingChat._check_for_unstreamed_tool_args(self,parser,content,request,index)
  OpenAIServingChat._convert_to_internal_request(self,request)
  OpenAIServingChat._get_enable_thinking_from_request(self,request)
  OpenAIServingChat._process_logprobs_tokens(self,logprobs,use_token_index)
  OpenAIServingChat._process_messages(self,request,is_multimodal)
  OpenAIServingChat._process_reasoning_stream(self,index,delta,reasoning_parser_dict,content,request)
  OpenAIServingChat._process_response_logprobs(self,ret_item)
  OpenAIServingChat._process_streaming_logprobs(self,content,n_prev_token)
  OpenAIServingChat._process_tool_calls(self,text,tools,tool_call_parser,finish_reason)
  OpenAIServingChat._request_id_prefix(self)
  OpenAIServingChat._validate_request(self,request)
  async OpenAIServingChat._generate_chat_stream(self,adapted_request,request,raw_request)
  async OpenAIServingChat._handle_non_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingChat._handle_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingChat._process_tool_call_stream(self,index,delta,parser_dict,content,request,has_tool_calls)
entrypoints/openai/serving_completions.py:
  OpenAIServingCompletion.__init__(self,tokenizer_manager,template_manager)
  OpenAIServingCompletion._build_completion_response(self,request,ret,created)
  OpenAIServingCompletion._build_sampling_params(self,request)
  OpenAIServingCompletion._convert_to_internal_request(self,request)
  OpenAIServingCompletion._get_echo_text(self,request,index)
  OpenAIServingCompletion._prepare_echo_prompts(self,request)
  OpenAIServingCompletion._request_id_prefix(self)
  OpenAIServingCompletion._validate_request(self,request)
  async OpenAIServingCompletion._generate_completion_stream(self,adapted_request,request,raw_request)
  async OpenAIServingCompletion._handle_non_streaming_request(self,adapted_request,request,raw_request)
  async OpenAIServingCompletion._handle_streaming_request(self,adapted_request,request,raw_request)
entrypoints/openai/serving_embedding.py:
  OpenAIServingEmbedding.__init__(self,tokenizer_manager,template_manager)
  OpenAIServingEmbedding._build_embedding_response(self,ret)
  OpenAIServingEmbedding._convert_to_internal_request(self,request)
  OpenAIServingEmbedding._request_id_prefix(self)
  OpenAIServingEmbedding._validate_request(self,request)
  async OpenAIServingEmbedding._handle_non_streaming_request(self,adapted_request,request,raw_request)
entrypoints/openai/serving_rerank.py:
  OpenAIServingRerank._build_rerank_response(self,ret,request)
  OpenAIServingRerank._convert_to_internal_request(self,request)
  OpenAIServingRerank._request_id_prefix(self)
  OpenAIServingRerank._validate_request(self,request)
  async OpenAIServingRerank._handle_non_streaming_request(self,adapted_request,request,raw_request)
entrypoints/openai/serving_responses.py:
  OpenAIServingResponses.__init__(self,tokenizer_manager,template_manager,enable_prompt_tokens_details,enable_force_include_usage,tool_server)
  OpenAIServingResponses._construct_input_messages(self,request,prev_response)
  OpenAIServingResponses._construct_input_messages_with_harmony(self,request,prev_response)
  OpenAIServingResponses._make_invalid_id_error(self,response_id)
  OpenAIServingResponses._make_not_found_error(self,response_id)
  OpenAIServingResponses._make_request_with_harmony(self,request,prev_response)
  OpenAIServingResponses._make_response_output_items(self,request,final_output,tokenizer)
  OpenAIServingResponses._make_response_output_items_with_harmony(self,context)
  OpenAIServingResponses._request_id_prefix(self)
  OpenAIServingResponses._send_event(event)
  async OpenAIServingResponses._generate_with_builtin_tools(self,request_id,request_prompt,adapted_request,sampling_params,context,raw_request,priority,**kwargs)
  async OpenAIServingResponses._make_request(self,request,prev_response,tokenizer)
  async OpenAIServingResponses._run_background_request(self,request,sampling_params,result_generator,context,model_name,tokenizer,request_metadata,created_time,*args,**kwargs)
  async OpenAIServingResponses.cancel_responses(self,response_id)
  async OpenAIServingResponses.create_responses(self,request,raw_request)
  async OpenAIServingResponses.empty_async_generator()
  async OpenAIServingResponses.responses_full_generator(self,request,sampling_params,result_generator,context,model_name,tokenizer,request_metadata,created_time)
  async OpenAIServingResponses.responses_stream_generator(self,request,sampling_params,result_generator,context,model_name,tokenizer,request_metadata,created_time)
  async OpenAIServingResponses.retrieve_responses(self,response_id)
entrypoints/openai/serving_score.py:
  OpenAIServingScore._convert_to_internal_request(self,request)
  OpenAIServingScore._request_id_prefix(self)
  async OpenAIServingScore._handle_non_streaming_request(self,adapted_request,request,raw_request)
entrypoints/openai/tool_server.py:
  DemoToolServer.__init__(self)
  DemoToolServer.get_tool_description(self,tool_name)
  DemoToolServer.has_tool(self,tool_name)
  MCPToolServer.__init__(self)
  MCPToolServer.get_tool_description(self,tool_name)
  MCPToolServer.has_tool(self,tool_name)
  ToolServer.get_tool_description(self,tool_name)
  ToolServer.get_tool_session(self,tool_name)
  ToolServer.has_tool(self,tool_name)
  async DemoToolServer.get_tool_session(self,tool_name)
  async MCPToolServer.add_tool_server(self,server_url)
  async MCPToolServer.get_tool_session(self,tool_name)
  async list_server_and_tools(server_url)
  post_process_tools_description(list_tools_result)
  trim_schema(schema)
entrypoints/openai/usage_processor.py:
  UsageProcessor._details_if_cached(count)
  UsageProcessor.calculate_response_usage(responses,n_choices,enable_cache_report)
  UsageProcessor.calculate_streaming_usage(prompt_tokens,completion_tokens,cached_tokens,n_choices,enable_cache_report)
  UsageProcessor.calculate_token_usage(prompt_tokens,completion_tokens,cached_tokens)
entrypoints/openai/utils.py:
  append_token_logprobs(token_logprobs)
  append_top_logprobs(top_logprobs)
  process_hidden_states_from_ret(ret_item,request)
  to_openai_style_logprobs(input_token_logprobs,output_token_logprobs,input_top_logprobs,output_top_logprobs)
entrypoints/tool.py:
  HarmonyBrowserTool.__init__(self)
  HarmonyBrowserTool.tool_config(self)
  HarmonyPythonTool.__init__(self)
  HarmonyPythonTool.tool_config(self)
  async HarmonyBrowserTool.get_result(self,context)
  async HarmonyPythonTool.get_result(self,context)
  async Tool.get_result(self,context)
