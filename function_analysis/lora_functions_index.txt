AST Function Index
Root: /Users/vincentzed/Documents/Github/open_source/sglang/python/sglang/srt
Excluded directories:

File: lora/backend/base_backend.py
  - name: __init__
    signature: (self, name: str, batch_info: LoRABatchInfo = None)
    class: BaseLoRABackend
  - name: run_lora_a_sgemm
    signature: (self, x: torch.Tensor, weights: torch.Tensor, *args, **kwargs)
    return: torch.Tensor
    class: BaseLoRABackend
    doc: Run segment Gemm of lora a modules with current backend.
  - name: run_lora_b_sgemm
    signature: (self, x: torch.Tensor, weights: torch.Tensor, *args, **kwargs)
    return: torch.Tensor
    class: BaseLoRABackend
    doc: Run segment Gemm of lora b modules with current backend.
  - name: run_qkv_lora
    signature: (self, x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]], *args, **kwargs)
    return: torch.Tensor
    class: BaseLoRABackend
    doc: Run the lora pass for QKV Layer.
  - name: run_gate_up_lora
    signature: (self, x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]], *args, **kwargs)
    return: torch.Tensor
    class: BaseLoRABackend
    doc: Run the lora pass for gate_up_proj, usually attached to MergedColumnParallelLayer.
  - name: set_batch_info
    signature: (self, batch_info: LoRABatchInfo)
    class: BaseLoRABackend
  - name: get_backend_from_name
    signature: (name: str)
    return: BaseLoRABackend
    doc: Get corresponding backend class from backend's name

File: lora/backend/triton_backend.py
  - name: __init__
    signature: (self, name: str, batch_info: LoRABatchInfo = None)
    class: TritonLoRABackend
  - name: run_lora_a_sgemm
    signature: (self, x: torch.Tensor, weights: torch.Tensor, *args, **kwargs)
    return: torch.Tensor
    class: TritonLoRABackend
  - name: run_lora_b_sgemm
    signature: (self, x: torch.Tensor, weights: torch.Tensor, base_output: torch.Tensor = None, *args, **kwargs)
    return: torch.Tensor
    class: TritonLoRABackend
  - name: run_qkv_lora
    signature: (self, x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: torch.Tensor, output_offset: torch.Tensor, max_qkv_out_dim: int, base_output: torch.Tensor = None, *args, **kwargs)
    return: torch.Tensor
    class: TritonLoRABackend
  - name: run_gate_up_lora
    signature: (self, x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: torch.Tensor, base_output: torch.Tensor = None, *args, **kwargs)
    return: torch.Tensor
    class: TritonLoRABackend

File: lora/layers.py
  - name: __init__
    signature: (self, base_layer: nn.Module, lora_backend: BaseLoRABackend)
    class: BaseLayerWithLoRA
  - name: forward
    signature: (self, x: torch.Tensor)
    class: BaseLayerWithLoRA
  - name: set_lora_info
    signature: (self, *args)
    class: BaseLayerWithLoRA
  - name: slice_lora_a_weights
    signature: (self, A: torch.Tensor, tp_rank: int)
    class: BaseLayerWithLoRA
  - name: slice_lora_b_weights
    signature: (self, B: torch.Tensor, tp_rank: int)
    class: BaseLayerWithLoRA
  - name: __init__
    signature: (self, base_layer: VocabParallelEmbedding, lora_backend: BaseLoRABackend)
    return: None
    class: VocabParallelEmbeddingWithLoRA
  - name: __init__
    signature: (self, base_layer: ColumnParallelLinear, lora_backend: BaseLoRABackend)
    return: None
    class: ColumnParallelLinearWithLoRA
  - name: set_lora_info
    signature: (self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)
    class: ColumnParallelLinearWithLoRA
  - name: apply_lora
    signature: (self, base_output: torch.Tensor, x: torch.Tensor)
    return: torch.Tensor
    class: ColumnParallelLinearWithLoRA
  - name: forward
    signature: (self, input_: torch.Tensor)
    class: ColumnParallelLinearWithLoRA
  - name: slice_lora_a_weights
    signature: (self, A: torch.Tensor, tp_rank: int)
    class: ColumnParallelLinearWithLoRA
  - name: slice_lora_b_weights
    signature: (self, B: torch.Tensor, tp_rank: int)
    class: ColumnParallelLinearWithLoRA
  - name: __init__
    signature: (self, base_layer: MergedColumnParallelLinear, lora_backend: BaseLoRABackend)
    return: None
    class: MergedColumnParallelLinearWithLoRA
  - name: set_lora_info
    signature: (self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)
    class: MergedColumnParallelLinearWithLoRA
  - name: apply_lora
    signature: (self, base_output: torch.Tensor, x: torch.Tensor)
    return: torch.Tensor
    class: MergedColumnParallelLinearWithLoRA
  - name: slice_lora_a_weights
    signature: (self, A: torch.Tensor, tp_rank: int)
    class: MergedColumnParallelLinearWithLoRA
  - name: slice_lora_b_weights
    signature: (self, B: torch.Tensor, tp_rank: int)
    class: MergedColumnParallelLinearWithLoRA
  - name: __init__
    signature: (self, base_layer: QKVParallelLinear, lora_backend: BaseLoRABackend)
    return: None
    class: QKVParallelLinearWithLoRA
  - name: set_lora_info
    signature: (self, A_buffer_qkv: torch.Tensor, B_buffer_qkv: torch.Tensor)
    class: QKVParallelLinearWithLoRA
  - name: apply_lora
    signature: (self, base_output: torch.Tensor, x: torch.Tensor)
    return: torch.Tensor
    class: QKVParallelLinearWithLoRA
  - name: slice_lora_a_weights
    signature: (self, A: torch.Tensor, tp_rank: int)
    class: QKVParallelLinearWithLoRA
  - name: slice_lora_b_weights
    signature: (self, B: torch.Tensor, tp_rank: int)
    return: torch.Tensor
    class: QKVParallelLinearWithLoRA
  - name: __init__
    signature: (self, base_layer: RowParallelLinear, lora_backend: BaseLoRABackend)
    return: None
    class: RowParallelLinearWithLoRA
  - name: set_lora_info
    signature: (self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)
    class: RowParallelLinearWithLoRA
  - name: apply_lora
    signature: (self, base_output: torch.Tensor, x: torch.Tensor)
    return: torch.Tensor
    class: RowParallelLinearWithLoRA
  - name: forward
    signature: (self, input_: torch.Tensor, skip_all_reduce = False)
    class: RowParallelLinearWithLoRA
  - name: slice_lora_a_weights
    signature: (self, A: torch.Tensor, tp_rank: int)
    class: RowParallelLinearWithLoRA
  - name: slice_lora_b_weights
    signature: (self, B: torch.Tensor, tp_rank: int)
    class: RowParallelLinearWithLoRA
  - name: get_lora_layer
    signature: (layer: nn.Module, lora_backend: BaseLoRABackend)
    return: BaseLayerWithLoRA

File: lora/lora.py
  - name: __init__
    signature: (self, config: LoRAConfig, base_hf_config: AutoConfig)
    class: LoRALayer
  - name: __init__
    signature: (self, uid: str, config: LoRAConfig, base_hf_config: AutoConfig, load_config: LoadConfig, lora_backend: BaseLoRABackend)
    class: LoRAAdapter
  - name: initialize_weights
    signature: (self)
    class: LoRAAdapter
  - name: normalize_qkv_proj
    signature: (self, weight_names: List[str], weights: Dict[str, torch.Tensor])
    class: LoRAAdapter
  - name: normalize_gate_up_proj
    signature: (self, weight_names: List[str], weights: Dict[str, torch.Tensor])
    class: LoRAAdapter

File: lora/lora_config.py
  - name: __init__
    signature: (self, path: str)
    return: None
    class: LoRAConfig
  - name: get_lora_config
    signature: (self, dummy = False)
    class: LoRAConfig

File: lora/lora_manager.py
  - name: __init__
    signature: (self, base_model: torch.nn.Module, base_hf_config: AutoConfig, max_loras_per_batch: int, load_config: LoadConfig, dtype: torch.dtype, lora_backend: str = 'triton', tp_size: int = 1, tp_rank: int = 0, max_lora_rank: Optional[int] = None, target_modules: Optional[Iterable[str]] = None, lora_paths: Optional[List[LoRARef]] = None)
    class: LoRAManager
  - name: init_cuda_graph_batch_info
    signature: (self, max_bs_in_cuda_graph: int)
    class: LoRAManager
  - name: create_lora_update_result
    signature: (self, success: bool, error_message: str = '')
    return: LoRAUpdateResult
    class: LoRAManager
  - name: load_lora_adapter
    signature: (self, lora_ref: LoRARef)
    return: LoRAUpdateResult
    class: LoRAManager
    doc: Load a single LoRA adapter from the specified path.
  - name: validate_new_adapter
    signature: (self, lora_config: LoRAConfig, lora_ref: LoRARef)
    class: LoRAManager
    doc: Validate if an adapter can be loaded into the current LoRA memory pool and generate error if it is incompatible.
  - name: unload_lora_adapter
    signature: (self, lora_ref: LoRARef)
    return: LoRAUpdateResult
    class: LoRAManager
    doc: Unload LoRA adapters by their names. This will remove the adapters from the memory pool and
  - name: validate_lora_batch
    signature: (self, lora_ids: set[str])
    return: bool
    class: LoRAManager
    doc: Validate if the LoRA IDs in the batch can be loaded into the current LoRA memory pool.
  - name: prepare_lora_batch
    signature: (self, forward_batch: ForwardBatch)
    class: LoRAManager
  - name: transfer_adapter_info
    signature: (weight_indices_out: torch.Tensor, lora_ranks_out: torch.Tensor, scalings_out: torch.Tensor)
    class: LoRAManager
    doc: Transfer adapter metadata (weight indices, LoRA rank, scalings) from host
  - name: update_lora_info
    signature: (self)
    class: LoRAManager
    doc: Update all LoRA modules to associate them with the latest memory buffer.
  - name: init_state
    signature: (self, max_lora_rank: Optional[int] = None, target_modules: Optional[Iterable[str]] = None, lora_paths: Optional[List[LoRARef]] = None)
    class: LoRAManager
    doc: Initialize the internal (mutable) state of the LoRAManager.
  - name: init_lora_adapters
    signature: (self, lora_paths: Optional[List[LoRARef]] = None)
    class: LoRAManager
  - name: init_lora_shapes
    signature: (self, max_lora_rank: Optional[int] = None, target_modules: Optional[Iterable[str]] = None)
    class: LoRAManager
    doc: Infer LoRA target modules and max_lora_rank from loaded adapters if not provided.
  - name: load_lora_weights
    signature: (self, lora_ref: LoRARef)
    class: LoRAManager
    doc: Load the weights of a LoRA adapter to CPU memory and conducts post-loading validation.
  - name: init_memory_pool
    signature: (self)
    class: LoRAManager
    doc: (Re)initialize the LoRA memory pool based on the current configurations.
  - name: set_lora_module
    signature: (self, module_name, module)
    class: LoRAManager
  - name: init_lora_modules
    signature: (self)
    class: LoRAManager

File: lora/lora_registry.py
  - name: __post_init__
    signature: (self)
    class: LoRARef
  - name: __str__
    signature: (self)
    return: str
    class: LoRARef
  - name: __init__
    signature: (self, lora_paths: Optional[List[LoRARef]] = None)
    class: LoRARegistry
  - name: register
    signature: (self, lora_ref: LoRARef)
    class: LoRARegistry
    doc: Register a new LoRARef object in the registry.
  - name: unregister
    signature: (self, lora_name: str)
    return: str
    class: LoRARegistry
    doc: Unregister a LoRARef object from the registry and returns the removed LoRA ID.
  - name: acquire
    signature: (self, lora_name: Union[str, List[str]])
    return: Union[str, List[str]]
    class: LoRARegistry
    doc: Queries registry for LoRA IDs based on LoRA names and start tracking the usage of the corresponding LoRA adapters
  - name: _lookup
    signature: (name: str)
    return: str
    class: LoRARegistry
  - name: release
    signature: (self, lora_id: Union[str, List[str]])
    class: LoRARegistry
    doc: Decrements the usage counter for a LoRA adapter, indicating that it is no longer in use.
  - name: wait_for_unload
    signature: (self, lora_id: str)
    class: LoRARegistry
    doc: Waits until the usage counter for a LoRA adapter reaches zero, indicating that it is no longer in use.
  - name: _register_adapter
    signature: (self, lora_ref: LoRARef)
    class: LoRARegistry
    doc: Internal helper method to register a LoRA adapter.
  - name: num_registered_loras
    signature: (self)
    return: int
    class: LoRARegistry
    doc: Returns the total number of LoRA adapters currently registered.

File: lora/mem_pool.py
  - name: __repr__
    signature: (self)
    class: EmptySlot
  - name: __new__
    signature: (cls)
    class: EmptySlot
  - name: __init__
    signature: (self, base_hf_config: AutoConfig, max_loras_per_batch: int, dtype: torch.dtype, tp_size: int, tp_rank: int, max_lora_rank: int, target_modules: Set[str], base_model: torch.nn.Module)
    class: LoRAMemoryPool
  - name: can_support
    signature: (self, config: Union[LoRAConfig, Iterable[LoRAConfig]])
    return: bool
    class: LoRAMemoryPool
    doc: Check if the memory pool can support the given LoRA adapters.
  - name: _can_support
    signature: (config: LoRAConfig)
    return: bool
    class: LoRAMemoryPool
    doc: Check if the memory pool can support a single LoRA adapter.
  - name: get_lora_A_shape
    signature: (self, module_name: str, base_model: torch.nn.Module, max_lora_dim: int)
    return: Tuple[int]
    class: LoRAMemoryPool
    doc: Given a module_name (might be a stacked name), return the hidden dims of modules' input and output.
  - name: get_lora_B_shape
    signature: (self, module_name: str, base_model: torch.nn.Module, max_lora_dim: int)
    return: Tuple[int]
    class: LoRAMemoryPool
    doc: Given a module_name (might be a stacked name), return the hidden dims of modules' input and output.
  - name: init_buffers
    signature: (self, base_model: torch.nn.Module)
    class: LoRAMemoryPool
  - name: init_buffer
    signature: (buffer: Dict[str, List[torch.Tensor]], target_modules: Set[str], get_lora_shape_fn: Callable[[str, torch.nn.Module, int], Tuple[int]])
    class: LoRAMemoryPool
  - name: prepare_lora_batch
    signature: (self, cur_uids: Set[Optional[str]], lora_adapters: Dict[str, LoRAAdapter], lora_modules: List[Dict[str, BaseLayerWithLoRA]], lora_refs: Dict[str, LoRARef])
    class: LoRAMemoryPool
  - name: get_available_buffer_slot
    signature: ()
    class: LoRAMemoryPool
  - name: load_lora_weight_to_buffer
    signature: (self, uid: str, buffer_id: int, lora_adapter: LoRAAdapter, lora_modules: List[Dict[str, BaseLayerWithLoRA]])
    class: LoRAMemoryPool
  - name: load_lora_weight_tensor
    signature: (buffer_view: torch.Tensor, weight: Optional[torch.Tensor])
    class: LoRAMemoryPool
  - name: get_tensor
    signature: (self, target_module: str, layer_id: int, lora_type: LoRAType)
    return: torch.Tensor
    class: LoRAMemoryPool
  - name: get_buffer_id
    signature: (self, lora_uid: str)
    class: LoRAMemoryPool

File: lora/triton_ops/__init__.py
  (no function definitions found)
File: lora/triton_ops/gate_up_lora_b.py
  - name: _gate_up_lora_b_kernel
    signature: (x, weights, output, K, output_dim, x_stride_0, x_stride_1, w_stride_0, w_stride_1, w_stride_2, output_stride_0, output_stride_1, seg_lens, seg_indptr, weight_indices, lora_ranks, BLOCK_S: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, scalings)
    doc: This kernel packs 2 sgemms (gate/up) into a single kernel. The multiplication
  - name: gate_up_lora_b_fwd
    signature: (x: torch.Tensor, gate_up_lora_b: torch.Tensor, batch_info: LoRABatchInfo, output_dim: int, base_output: torch.Tensor = None)
    return: torch.Tensor

File: lora/triton_ops/qkv_lora_b.py
  - name: _qkv_lora_b_kernel
    signature: (x, weights, output, K, max_qkv_out_dim, x_stride_0, x_stride_1, w_stride_0, w_stride_1, w_stride_2, output_stride_0, output_stride_1, seg_lens, seg_indptr, weight_indices, lora_ranks, n_offs, BLOCK_S: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, scalings)
    doc: This kernel packs 3 sgemms (q/k/v) into a single kernel. The multiplication
  - name: qkv_lora_b_fwd
    signature: (x: torch.Tensor, qkv_lora_b: torch.Tensor, batch_info: LoRABatchInfo, output_offset: torch.Tensor, max_qkv_out_dim: int, base_output: torch.Tensor = None)
    return: torch.Tensor

File: lora/triton_ops/sgemm_lora_a.py
  - name: _sgemm_lora_a_kernel
    signature: (x, weights, output, N, K, stack_num, x_stride_0, x_stride_1, w_stride_0, w_stride_1, w_stride_2, output_stride_0, output_stride_1, seg_lens, seg_indptr, weight_indices, lora_ranks, BLOCK_S: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr)
    doc: Computes a segmented batched matrix multiplication for the LoRA A matrix.
  - name: sgemm_lora_a_fwd
    signature: (x: torch.Tensor, weights: torch.Tensor, batch_info: LoRABatchInfo, stack_num: int = 1)
    return: torch.Tensor

File: lora/triton_ops/sgemm_lora_b.py
  - name: _sgemm_lora_b_kernel
    signature: (x, weights, output, N, K, x_stride_0, x_stride_1, w_stride_0, w_stride_1, w_stride_2, output_stride_0, output_stride_1, seg_lens, seg_indptr, weight_indices, lora_ranks, BLOCK_S: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, scalings)
    doc: Computes a segmented batched matrix multiplication for the LoRA B matrix
  - name: sgemm_lora_b_fwd
    signature: (x: torch.Tensor, weights: torch.Tensor, batch_info: LoRABatchInfo, base_output: torch.Tensor = None)
    return: torch.Tensor

File: lora/utils.py
  - name: get_layer_id
    signature: (name: str)
    return: int
    doc: Extract integer id of layer from its name in string.
  - name: get_hidden_dim
    signature: (module_name: str, config: AutoConfig, base_model: torch.nn.Module)
    return: Tuple[int]
    doc: Given a module_name (might be a stacked name), return the hidden dims of modules' input and output.
  - name: get_normalized_target_modules
    signature: (target_modules: Iterable[str])
    return: set[str]
    doc: Mapping a list of target module name to names of the normalized LoRA weights.
  - name: get_stacked_multiply
    signature: (module_name: str)
    return: int
    doc: Mapping a lora module name to its magnification at output dimension
  - name: get_target_module_name
    signature: (full_module_name: str, target_modules: Set[str])
    return: str
    doc: Get the target module name in target_modules that can match full_module_name.
